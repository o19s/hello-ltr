{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics & Prereqs (run once)\n",
    "\n",
    "If you don't already have the downloaded dependencies; if you don't have TheMovieDB data indexed run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client.elastic_client import ElasticClient\n",
    "client = ElasticClient()\n",
    "\n",
    "from ltr import download, index\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "from ltr import download\n",
    "\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "download([corpus], dest='data/');\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Elastic Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client.elastic_client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Task: Optimizing \"Drama\" and \"Science Fiction\" queries\n",
    "\n",
    "In this example we have two user queries\n",
    "\n",
    "- Drama\n",
    "- Science Fiction\n",
    "\n",
    "And we want to train a model to return the best movies for these movies when a user types them into our search bar.\n",
    "\n",
    "We learn through analysis that searchers prefer newer science fiction, but older drama. Like a lot of search relevance problems, two queries need to be optimized in *different* directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Judgment List Generation\n",
    "\n",
    "To setup this example, we'll generate a judgment list that rewards new science fiction movies as more relevant; and old drama movies as relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.date_genre_judgments import synthesize\n",
    "judgments = synthesize(client, judgmentsOutFile='data/genre_by_date_judgments.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection should be *easy!*\n",
    "\n",
    "Notice we have 4 proposed features, that seem like they should work! This should be a piece of cake...\n",
    "\n",
    "1. Release Year of a movie `release_year` - feature ID 1\n",
    "2. Is the movie Science Fiction `is_scifi` - feature ID 2\n",
    "3. Is the movie Drama `is_drama` - feature ID 3\n",
    "4. Does the search term match the genre field `is_genre_match` - feature ID 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\n",
    "    \"featureset\": {\n",
    "            \"features\": [\n",
    "            {\n",
    "                \"name\": \"release_year\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"function_score\": {\n",
    "                        \"field_value_factor\": {\n",
    "                        \"field\": \"release_year\",\n",
    "                        \"missing\": 2000\n",
    "                    },\n",
    "                    \"query\": { \"match_all\": {} }\n",
    "                }\n",
    "            }\n",
    "            },\n",
    "             {\n",
    "                \"name\": \"is_sci_fi\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match_phrase\": {\"genres\": \"Science Fiction\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0                    }\n",
    "            }\n",
    "            },\n",
    "             {\n",
    "                \"name\": \"is_drama\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match_phrase\": {\"genres\": \"Drama\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0                    }\n",
    "                }\n",
    "            },\n",
    "             {\n",
    "                \"name\": \"is_genre_match\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                        \"filter\": {\n",
    "                            \"match_phrase\": {\"genres\": \"{{keywords}}\"}\n",
    "                        },\n",
    "                        \"boost\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    ]\n",
    "    },\n",
    "    \"validation\": {\n",
    "       \"params\": {\n",
    "           \"keywords\": \"Science Fiction\"\n",
    "       },\n",
    "       \"index\": \"tmdb\"\n",
    "    }\n",
    "}\n",
    "\n",
    "client.create_featureset(index='tmdb', name='genre', ftr_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log from search engine -> to training set\n",
    "\n",
    "Each feature is a query to be scored against the judgment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.judgments import judgments_open\n",
    "from ltr.log import FeatureLogger\n",
    "from itertools import groupby\n",
    "\n",
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='genre')\n",
    "with judgments_open('data/genre_by_date_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Guaraneed Perfect Search Results!\n",
    "\n",
    "We'll train a LambdaMART model against this training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainResponse = train(client,\n",
    "                 training_set=ftr_logger.logged,\n",
    "                 metric2t='NDCG@10',\n",
    "                 index='tmdb',\n",
    "                 featureSet='genre',\n",
    "                 modelName='genre')\n",
    "\n",
    "trainLog = trainResponse.trainingLogs[0]\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(client.get_feature_name(config, ftrId), impact))\n",
    "    \n",
    "print(\"Perfect NDCG! {}\".format(trainLog.rounds[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But this search sucks!\n",
    "Try searches for \"Science Fiction\" and \"Drama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.search import search\n",
    "search(client, keywords=\"drama\", modelName=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why didn't it work!?!? Training data\n",
    "\n",
    "1. Examine the training data, do we cover every example of a BAD result\n",
    "2. Examine the feature impacts, do any of the features the model uses even USE the keywords?\n",
    "\n",
    "### Ranklib only sees the data you give it, we don't have good enough coverage\n",
    "\n",
    "You need to have feature coverage, especially over negative examples. Most documents in the index are negative! \n",
    "\n",
    "One trick commonly used is to treat other queries positive results as this queries negative results. Indeed what we're missing here are negative examples for \"Science Fiction\" that are not science fiction movies. A glaring omission, we'll handle now... With the `autoNegate` flag, we'll add additional negative examples to the judgment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import date_genre_judgments\n",
    "date_genre_judgments.synthesize(client,\n",
    "                                judgmentsOutFile='data/genre_by_date_judgments.txt',\n",
    "                                autoNegate=True)\n",
    "\n",
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='genre')\n",
    "with judgments_open('data/genre_by_date_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "        \n",
    "from ltr.ranklib import train\n",
    "trainResponse = train(client,\n",
    "                 training_set=ftr_logger.logged,\n",
    "                 metric2t='NDCG@10',\n",
    "                 index='tmdb',\n",
    "                 featureSet='genre',\n",
    "                 modelName='genre')\n",
    "\n",
    "trainLog = trainResponse.trainingLogs[0]\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(client.get_feature_name(config, ftrId), impact))\n",
    "    \n",
    "print(\"NDCG {}\".format(trainLog.rounds[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try those queries...\n",
    "\n",
    "Replace keywords below with 'science fiction' or 'drama' and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.search import search\n",
    "search(client, keywords=\"drama\", modelName=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.search import search\n",
    "search(client, keywords=\"science fiction\", modelName=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next problem\n",
    "\n",
    "- Overfit to these two examples\n",
    "- We need many more queries, covering more use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
