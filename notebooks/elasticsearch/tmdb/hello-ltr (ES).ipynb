{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello LTR!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fire up an elastic server with the LTR plugin installed and run thru the cells below to get started with Learning-to-Rank. These notebooks we'll use in this training have something of an ltr client library, and a starting point for demonstrating several important learning to rank capabilities.\n",
    "\n",
    "This notebook will document many of the important pieces so you can reuse them in future training sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download some requirements\n",
    "\n",
    "Several requirements/datasets are stored in online, these include various training data sets, the data sets, and tools. You'll only need to do this once. But if you lose the data, you can repeat this command if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "\n",
    "download([corpus], dest='data/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Elastic client\n",
    "\n",
    "Two LTR clients exist in this code, an ElasticClient and a SolrClient. The workflow for doing Learning to Rank is the same in both search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Movies\n",
    "\n",
    "In these demos, we'll use [TheMovieDB](http://themoviedb.org) alongside some supporting assets from places like movielens.\n",
    "\n",
    "When we reindex, we'll use `rebuild_tmdb` which deletes and recreates the index, with a few hooks to help us enrich the underlying data or modify the search engine configuration for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Learning to Rank\n",
    "\n",
    "We'll discuss the feature sets a bit more. You can think of them as a series of queries that will be stored and executed before we need to train a model. \n",
    "\n",
    "`setup` is our function for preparing learning to rank to optimize search using a set of features. In this stock demo, we just have one feature, the year of the movie's release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\n",
    "    \"featureset\": {\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"name\": \"release_year\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"function_score\": {\n",
    "                        \"field_value_factor\": {\n",
    "                            \"field\": \"release_year\",\n",
    "                            \"missing\": 2000\n",
    "                        },\n",
    "                        \"query\": { \"match_all\": {} }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "client.create_featureset(index='tmdb', name='release', ftr_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this thing on?\n",
    "\n",
    "Before we dive into all the pieces, with a real training set, we'll try out two examples of models. One that always prefers newer movies. And another that always prefers older movies. If you're curious you can opet `classic-training.txt` and `latest-training.txt` after running this to see what the training set looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import years_as_ratings\n",
    "from ltr.judgments import judgments_from_file\n",
    "\n",
    "years_as_ratings.synthesize(client, \n",
    "                            featureSet='release',\n",
    "                            classicTrainingSetOut='data/classic-training.txt',\n",
    "                            latestTrainingSetOut='data/latest-training.txt')\n",
    "\n",
    "# Load into training set \n",
    "classic_training_set = [j for j in judgments_from_file(open('data/classic-training.txt'))]\n",
    "latest_training_set = [j for j in judgments_from_file(open('data/latest-training.txt'))]\n",
    "\n",
    "classic_training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Submit\n",
    "\n",
    "We'll train a lot of models in this class! Our ltr library has a `train` method that wraps a tool called `Ranklib` (more on Ranklib later), allows you to pass the most common commands to Ranklib, stores a model in the search engine, and then returns diagnostic output that's worth inspecting. \n",
    "\n",
    "For now we'll just train using the generated training set, and store two models `latest` and `classic`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "train(client, training_set=latest_training_set, \n",
    "      index='tmdb', featureSet='release', modelName='latest')\n",
    "train(client, training_set=classic_training_set, \n",
    "      index='tmdb', featureSet='release', modelName='classic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ben Affleck vs Adam West\n",
    "If we search for `batman`, how do the results compare?  Since the `classic` model prefered old movies it has old movies in the top position, and the opposite is true for the `latest` model.  To continue learning LTR, brainstorm more features and generate some real judgments for real queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.release_date_plot import plot\n",
    "plot(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
