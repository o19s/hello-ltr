{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log Elasticsearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with Elasticsearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
    "1. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doug/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the Elasticsearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries...\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>427.587115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>9.463946</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>219.800566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>62.204093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>3.014736</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>43.694734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>1.160558</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>5.542298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.598524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.715586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.826142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.930783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-21.030027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
       "    2       1     1_1370    1      rambo    1370      3   \n",
       "    3       2     1_1369    1      rambo    1369      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount       gain        dcg  \\\n",
       "qid                                                                            \n",
       "1   0   [11.657399, 10.083591]             0  1.000000  15.000000  33.734341   \n",
       "    1         [0.0, 11.113943]             1  0.630930   9.463946  33.734341   \n",
       "    2    [9.456276, 13.265001]             2  0.500000   3.500000  33.734341   \n",
       "    3    [6.036743, 11.113943]             3  0.430677   3.014736  33.734341   \n",
       "    4          [0.0, 6.869545]             4  0.386853   1.160558  33.734341   \n",
       "...                        ...           ...       ...        ...        ...   \n",
       "40  25              [0.0, 0.0]            25  0.210310   0.000000  31.225149   \n",
       "    26              [0.0, 0.0]            26  0.208015   0.000000  31.225149   \n",
       "    27              [0.0, 0.0]            27  0.205847   0.000000  31.225149   \n",
       "    28              [0.0, 0.0]            28  0.203795   0.000000  31.225149   \n",
       "    29              [0.0, 0.0]             8  0.301030   0.000000  31.225149   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   427.587115  \n",
       "    1   219.800566  \n",
       "    2    62.204093  \n",
       "    3    43.694734  \n",
       "    4     5.542298  \n",
       "...            ...  \n",
       "40  25  -20.598524  \n",
       "    26  -20.715586  \n",
       "    27  -20.826142  \n",
       "    28  -20.930783  \n",
       "    29  -21.030027  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
    "    ranked_list['discount'] = 1 / np.log2(2 + ranked_list['display_rank'])\n",
    "    ranked_list['gain'] = (2**ranked_list['grade'] - 1) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('grade', kind='stable', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
       "1     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "2     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "6     154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "7     155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "8     156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "9     157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "10    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "11    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "12    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "13    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "14    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "15    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "16    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "17    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "18    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "19    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "20    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "21    169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "22    170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "23    171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "24    173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "25    174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "26    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "27    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "28    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "29    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "30    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "31    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>427.587115</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219.800566</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.204093</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.694734</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.542298</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>-20.598524</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-20.715586</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-20.826142</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-20.930783</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-21.030027</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
       "1   0   427.587115  [11.657399, 10.083591]\n",
       "    1   219.800566        [0.0, 11.113943]\n",
       "    2    62.204093   [9.456276, 13.265001]\n",
       "    3    43.694734   [6.036743, 11.113943]\n",
       "    4     5.542298         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -20.598524              [0.0, 0.0]\n",
       "    26  -20.715586              [0.0, 0.0]\n",
       "    27  -20.826142              [0.0, 0.0]\n",
       "    28  -20.930783              [0.0, 0.0]\n",
       "    29  -21.030027              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([345.6500854])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[11.1, 10.08]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15.3987952])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(248.0, 308.0, 'X[0] <= 10.666\\nmse = 4161.543\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(124.0, 184.79999999999998, 'X[0] <= 9.182\\nmse = 1038.341\\nsamples = 1329\\nvalue = -9.745'),\n",
       " Text(62.0, 61.599999999999966, 'mse = 421.042\\nsamples = 1301\\nvalue = -11.724'),\n",
       " Text(186.0, 61.599999999999966, 'mse = 21086.54\\nsamples = 28\\nvalue = 82.191'),\n",
       " Text(372.0, 184.79999999999998, 'X[0] <= 18.186\\nmse = 25061.546\\nsamples = 61\\nvalue = 212.311'),\n",
       " Text(310.0, 61.599999999999966, 'mse = 26150.423\\nsamples = 51\\nvalue = 188.147'),\n",
       " Text(434.0, 61.599999999999966, 'mse = 1343.167\\nsamples = 10\\nvalue = 335.547')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>427.553645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>97.974272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>63.670676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>13.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>83.896012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>-19.692155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>-19.806921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>-19.915309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>-20.017899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>-20.115197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       3    1_13258    1      rambo   13258      2   \n",
       "    4       4     1_1368    1      rambo    1368      4   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]              0.0             0  1.000000   \n",
       "    1    [9.456276, 13.265001]              0.0             1  0.630930   \n",
       "    2    [6.036743, 11.113943]              0.0             2  0.500000   \n",
       "    3          [0.0, 6.869545]              0.0             3  0.430677   \n",
       "    4         [0.0, 11.113943]              0.0             4  0.386853   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]              0.0            25  0.210310   \n",
       "    26              [0.0, 0.0]              0.0            26  0.208015   \n",
       "    27              [0.0, 0.0]              0.0            27  0.205847   \n",
       "    28              [0.0, 0.0]              0.0            28  0.203795   \n",
       "    29              [0.0, 0.0]              0.0             9  0.289065   \n",
       "\n",
       "             gain        dcg      lambda       delta  \n",
       "qid                                                   \n",
       "1   0   15.000000  30.700871  213.776822  427.553645  \n",
       "    1    4.416508  30.700871   48.987136   97.974272  \n",
       "    2    3.500000  30.700871   31.835338   63.670676  \n",
       "    3    1.292030  30.700871    6.723500   13.447000  \n",
       "    4    5.802792  30.700871   41.948006   83.896012  \n",
       "...           ...        ...         ...         ...  \n",
       "40  25   0.000000  30.207651   -9.846078  -19.692155  \n",
       "    26   0.000000  30.207651   -9.903461  -19.806921  \n",
       "    27   0.000000  30.207651   -9.957655  -19.915309  \n",
       "    28   0.000000  30.207651  -10.008949  -20.017899  \n",
       "    29   0.000000  30.207651  -10.057598  -20.115197  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Important - stable sort. Otherwise DCG swaps get kind of wonky due to position discounts\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['delta'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                swap_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                dcg_after_swap = metric(swap_judgments, at=at)\n",
    "\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
    "                    query_judgments.loc[better, 'delta'] += delta\n",
    "                    \n",
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "                    query_judgments.loc[worse, 'delta'] -= delta\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0.0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "#\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.553645</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.868699</td>\n",
       "      <td>-9.934349</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.149295</td>\n",
       "      <td>-10.074647</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.276314</td>\n",
       "      <td>-10.138157</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.395685</td>\n",
       "      <td>-10.197842</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.508155</td>\n",
       "      <td>-10.254078</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.336529</td>\n",
       "      <td>-10.168264</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.432963</td>\n",
       "      <td>-10.216481</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.524423</td>\n",
       "      <td>-10.262211</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.611330</td>\n",
       "      <td>-10.305665</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.694056</td>\n",
       "      <td>-10.347028</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.069351</td>\n",
       "      <td>-10.534675</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.147879</td>\n",
       "      <td>-10.573939</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.222977</td>\n",
       "      <td>-10.611488</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.294893</td>\n",
       "      <td>-10.647447</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.363851</td>\n",
       "      <td>-10.681926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.430053</td>\n",
       "      <td>-10.715026</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.493681</td>\n",
       "      <td>-10.746841</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.554902</td>\n",
       "      <td>-10.777451</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.013760</td>\n",
       "      <td>-10.006880</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.712923</td>\n",
       "      <td>-9.856462</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.974272</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.545028</td>\n",
       "      <td>-9.772514</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.670676</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.447000</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.896012</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.400912</td>\n",
       "      <td>-4.200456</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.024956</td>\n",
       "      <td>-5.012478</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.243092</td>\n",
       "      <td>-7.621546</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.950401</td>\n",
       "      <td>-7.975200</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.536694</td>\n",
       "      <td>-8.268347</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.032666</td>\n",
       "      <td>-8.516333</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.459201</td>\n",
       "      <td>-8.729601</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.831043</td>\n",
       "      <td>-8.915522</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.158927</td>\n",
       "      <td>-9.079464</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.450871</td>\n",
       "      <td>-9.225435</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.712994</td>\n",
       "      <td>-9.356497</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.950060</td>\n",
       "      <td>-9.475030</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.165834</td>\n",
       "      <td>-9.582917</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.363338</td>\n",
       "      <td>-9.681669</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.613868</td>\n",
       "      <td>-10.806934</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keywords  display_rank  grade  last_prediction       delta      lambda  \\\n",
       "0     rambo             0      4              0.0  427.553645  213.776822   \n",
       "21    rambo            21      0              0.0  -19.868699   -9.934349   \n",
       "23    rambo            23      0              0.0  -20.149295  -10.074647   \n",
       "24    rambo            24      0              0.0  -20.276314  -10.138157   \n",
       "25    rambo            25      0              0.0  -20.395685  -10.197842   \n",
       "26    rambo            26      0              0.0  -20.508155  -10.254078   \n",
       "27    rambo            27      1              0.0  -20.336529  -10.168264   \n",
       "28    rambo            28      1              0.0  -20.432963  -10.216481   \n",
       "29    rambo            29      1              0.0  -20.524423  -10.262211   \n",
       "30    rambo            30      1              0.0  -20.611330  -10.305665   \n",
       "31    rambo            31      1              0.0  -20.694056  -10.347028   \n",
       "32    rambo            32      0              0.0  -21.069351  -10.534675   \n",
       "33    rambo            33      0              0.0  -21.147879  -10.573939   \n",
       "34    rambo            34      0              0.0  -21.222977  -10.611488   \n",
       "35    rambo            35      0              0.0  -21.294893  -10.647447   \n",
       "36    rambo            36      0              0.0  -21.363851  -10.681926   \n",
       "37    rambo            37      0              0.0  -21.430053  -10.715026   \n",
       "38    rambo            38      0              0.0  -21.493681  -10.746841   \n",
       "39    rambo            39      0              0.0  -21.554902  -10.777451   \n",
       "22    rambo            22      0              0.0  -20.013760  -10.006880   \n",
       "20    rambo            20      0              0.0  -19.712923   -9.856462   \n",
       "1     rambo             1      3              0.0   97.974272   48.987136   \n",
       "19    rambo            19      0              0.0  -19.545028   -9.772514   \n",
       "2     rambo             2      3              0.0   63.670676   31.835338   \n",
       "3     rambo             3      2              0.0   13.447000    6.723500   \n",
       "4     rambo             4      4              0.0   83.896012   41.948006   \n",
       "5     rambo             5      1              0.0   -8.400912   -4.200456   \n",
       "6     rambo            40      1              0.0  -10.024956   -5.012478   \n",
       "7     rambo             7      0              0.0  -15.243092   -7.621546   \n",
       "8     rambo             8      0              0.0  -15.950401   -7.975200   \n",
       "9     rambo             9      0              0.0  -16.536694   -8.268347   \n",
       "10    rambo            10      0              0.0  -17.032666   -8.516333   \n",
       "11    rambo            11      0              0.0  -17.459201   -8.729601   \n",
       "12    rambo            12      0              0.0  -17.831043   -8.915522   \n",
       "13    rambo            13      0              0.0  -18.158927   -9.079464   \n",
       "14    rambo            14      0              0.0  -18.450871   -9.225435   \n",
       "15    rambo            15      0              0.0  -18.712994   -9.356497   \n",
       "16    rambo            16      0              0.0  -18.950060   -9.475030   \n",
       "17    rambo            17      0              0.0  -19.165834   -9.582917   \n",
       "18    rambo            18      0              0.0  -19.363338   -9.681669   \n",
       "40    rambo             6      0              0.0  -21.613868  -10.806934   \n",
       "\n",
       "                  features  \n",
       "0   [11.657399, 10.083591]  \n",
       "21              [0.0, 0.0]  \n",
       "23              [0.0, 0.0]  \n",
       "24              [0.0, 0.0]  \n",
       "25              [0.0, 0.0]  \n",
       "26              [0.0, 0.0]  \n",
       "27              [0.0, 0.0]  \n",
       "28              [0.0, 0.0]  \n",
       "29              [0.0, 0.0]  \n",
       "30              [0.0, 0.0]  \n",
       "31              [0.0, 0.0]  \n",
       "32              [0.0, 0.0]  \n",
       "33              [0.0, 0.0]  \n",
       "34              [0.0, 0.0]  \n",
       "35              [0.0, 0.0]  \n",
       "36              [0.0, 0.0]  \n",
       "37              [0.0, 0.0]  \n",
       "38              [0.0, 0.0]  \n",
       "39              [0.0, 0.0]  \n",
       "22              [0.0, 0.0]  \n",
       "20              [0.0, 0.0]  \n",
       "1    [9.456276, 13.265001]  \n",
       "19              [0.0, 0.0]  \n",
       "2    [6.036743, 11.113943]  \n",
       "3          [0.0, 6.869545]  \n",
       "4         [0.0, 11.113943]  \n",
       "5         [0.0, 7.8627386]  \n",
       "6          [0.0, 4.563677]  \n",
       "7               [0.0, 0.0]  \n",
       "8               [0.0, 0.0]  \n",
       "9               [0.0, 0.0]  \n",
       "10              [0.0, 0.0]  \n",
       "11              [0.0, 0.0]  \n",
       "12              [0.0, 0.0]  \n",
       "13              [0.0, 0.0]  \n",
       "14              [0.0, 0.0]  \n",
       "15              [0.0, 0.0]  \n",
       "16              [0.0, 0.0]  \n",
       "17              [0.0, 0.0]  \n",
       "18              [0.0, 0.0]  \n",
       "40              [0.0, 0.0]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.loc[1, :][['keywords', 'display_rank',  'grade', 'last_prediction', 'delta', 'lambda', 'features']].sort_values('last_prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173.48027244])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.72350002])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>uid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>106.888411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>48.010828</td>\n",
       "      <td>26.458003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>31.382749</td>\n",
       "      <td>18.143963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>6.460558</td>\n",
       "      <td>7.448315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>43.639845</td>\n",
       "      <td>21.819923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>4.923039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>4.951730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>4.978827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>5.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>5.028799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid        uid   keywords   docId  grade                features  \\\n",
       "0       1     1_7555      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1       1     1_1370      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2       1     1_1369      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1    1_13258      rambo   13258      2         [0.0, 6.869545]   \n",
       "4       1     1_1368      rambo    1368      4        [0.0, 11.113943]   \n",
       "...   ...        ...        ...     ...    ...                     ...   \n",
       "1385   40   40_37079  star wars   37079      0              [0.0, 0.0]   \n",
       "1386   40  40_126757  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40   40_39797  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40   40_18112  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40   40_43052  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount       gain  train_dcg  \\\n",
       "0                   0             0  1.000000  15.000000  30.700871   \n",
       "1                   0             1  0.630930   4.416508  30.700871   \n",
       "2                   0             2  0.500000   3.500000  30.700871   \n",
       "3                   0             3  0.430677   1.292030  30.700871   \n",
       "4                   0             4  0.386853   5.802792  30.700871   \n",
       "...               ...           ...       ...        ...        ...   \n",
       "1385                0            25  0.210310   0.000000  30.207651   \n",
       "1386                0            26  0.208015   0.000000  30.207651   \n",
       "1387                0            27  0.205847   0.000000  30.207651   \n",
       "1388                0            28  0.203795   0.000000  30.207651   \n",
       "1389                0             9  0.289065   0.000000  30.207651   \n",
       "\n",
       "            dcg      lambda      weight  \n",
       "0     30.552986  213.776822  106.888411  \n",
       "1     30.552986   48.010828   26.458003  \n",
       "2     30.552986   31.382749   18.143963  \n",
       "3     30.552986    6.460558    7.448315  \n",
       "4     30.552986   43.639845   21.819923  \n",
       "...         ...         ...         ...  \n",
       "1385  30.120435   -9.846078    4.923039  \n",
       "1386  30.120435   -9.903461    4.951730  \n",
       "1387  30.120435   -9.957655    4.978827  \n",
       "1388  30.120435  -10.008949    5.004475  \n",
       "1389  30.120435  -10.057598    5.028799  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
    " \n",
    "    qid = query_judgments.loc[0, 'qid']\n",
    "    keywords = query_judgments.loc[0, 'keywords']\n",
    "\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "\n",
    "    for better in range(0,len(query_judgments)):\n",
    "         for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                return query_judgments\n",
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(train_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta != 0.0:\n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff)) \n",
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label each row with its unique prediction (ie tree path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "lambdas_per_query['path'] = tree_paths(tree3, train_set['features'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    1673.720249\n",
       "1010010    1467.050422\n",
       "1100100     538.598514\n",
       "1101000    4655.114588\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    3334.073882\n",
       "1010010    2787.629391\n",
       "1100100     893.120752\n",
       "1101000   -7014.824025\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1010001': 1.9920138295288718,\n",
       " '1010010': 1.9001592234365976,\n",
       " '1100100': 1.6582309999465077,\n",
       " '1101000': -1.5069068425436145}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 181.2, 'X[0] <= 10.328\\nmse = 883.211\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 179.235\\nsamples = 1324\\nvalue = -4.624'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 62.9\\nsamples = 1301\\nvalue = -5.392'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 4838.036\\nsamples = 23\\nvalue = 38.831'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 13.782\\nmse = 5973.405\\nsamples = 66\\nvalue = 92.753'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 5828.45\\nsamples = 39\\nvalue = 71.478'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 4584.565\\nsamples = 27\\nvalue = 123.484')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the original tree, and provide new value\n",
    "\n",
    "Instead of directly using the provided decision tree, we want to use our prediction for each leaf. This `OverridenDecisionTree` takes the original tree, looks up the prediction path in the original tree, but uses our values for the predicted variable instead of what's here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.50690684,  1.658231  ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X, use_original=False):\n",
    "        if use_original:\n",
    "            return self.predict(X)\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
    "* While DCG converges, it does sometimes wander a tad, so there might be more room for improvement\n",
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "Train DCGs\n",
      "mean    20.03521601393222\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    20.558839639948378\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 5\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 6\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 7\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 8\n",
      "Train DCGs\n",
      "mean    20.53155110954963\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 9\n",
      "Train DCGs\n",
      "mean    20.52107778379093\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 10\n",
      "Train DCGs\n",
      "mean    20.621960320672194\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.1, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query.iloc[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
