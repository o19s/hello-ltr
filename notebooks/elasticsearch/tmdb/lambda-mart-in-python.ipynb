{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log Elasticsearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with Elasticsearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
    "---\n",
    "\n",
    "6. [A Pandas version!](#5.-Pure-Pandas-Implementation?) -- walking through a faster version computing the per-tree training data using Pandas - a much for useful toy example.\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
    "1. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the Elasticsearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries...\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>427.587115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>9.463946</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>219.800566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>62.204093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>3.014736</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>43.694734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>1.160558</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>5.542298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.598524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.715586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.826142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.930783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-21.030027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
       "    2       1     1_1370    1      rambo    1370      3   \n",
       "    3       2     1_1369    1      rambo    1369      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount       gain        dcg  \\\n",
       "qid                                                                            \n",
       "1   0   [11.657399, 10.083591]             0  1.000000  15.000000  33.734341   \n",
       "    1         [0.0, 11.113943]             1  0.630930   9.463946  33.734341   \n",
       "    2    [9.456276, 13.265001]             2  0.500000   3.500000  33.734341   \n",
       "    3    [6.036743, 11.113943]             3  0.430677   3.014736  33.734341   \n",
       "    4          [0.0, 6.869545]             4  0.386853   1.160558  33.734341   \n",
       "...                        ...           ...       ...        ...        ...   \n",
       "40  25              [0.0, 0.0]            25  0.210310   0.000000  31.225149   \n",
       "    26              [0.0, 0.0]            26  0.208015   0.000000  31.225149   \n",
       "    27              [0.0, 0.0]            27  0.205847   0.000000  31.225149   \n",
       "    28              [0.0, 0.0]            28  0.203795   0.000000  31.225149   \n",
       "    29              [0.0, 0.0]             8  0.301030   0.000000  31.225149   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   427.587115  \n",
       "    1   219.800566  \n",
       "    2    62.204093  \n",
       "    3    43.694734  \n",
       "    4     5.542298  \n",
       "...            ...  \n",
       "40  25  -20.598524  \n",
       "    26  -20.715586  \n",
       "    27  -20.826142  \n",
       "    28  -20.930783  \n",
       "    29  -21.030027  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
    "    ranked_list['discount'] = 1 / np.log2(2 + ranked_list['display_rank'])\n",
    "    ranked_list['gain'] = (2**ranked_list['grade'] - 1) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('grade', kind='stable', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
       "1     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "2     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "6     154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "7     155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "8     156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "9     157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "10    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "11    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "12    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "13    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "14    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "15    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "16    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "17    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "18    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "19    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "20    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "21    169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "22    170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "23    171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "24    173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "25    174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "26    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "27    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "28    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "29    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "30    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "31    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>427.587115</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219.800566</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.204093</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.694734</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.542298</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>-20.598524</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-20.715586</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-20.826142</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-20.930783</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-21.030027</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
       "1   0   427.587115  [11.657399, 10.083591]\n",
       "    1   219.800566        [0.0, 11.113943]\n",
       "    2    62.204093   [9.456276, 13.265001]\n",
       "    3    43.694734   [6.036743, 11.113943]\n",
       "    4     5.542298         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -20.598524              [0.0, 0.0]\n",
       "    26  -20.715586              [0.0, 0.0]\n",
       "    27  -20.826142              [0.0, 0.0]\n",
       "    28  -20.930783              [0.0, 0.0]\n",
       "    29  -21.030027              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([445.85277429])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[11.1, 10.08]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15.3987952])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 181.2, 'X[0] <= 10.666\\nmse = 4161.543\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 1038.341\\nsamples = 1329\\nvalue = -9.745'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 421.042\\nsamples = 1301\\nvalue = -11.724'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 21086.54\\nsamples = 28\\nvalue = 82.191'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 18.186\\nmse = 25061.546\\nsamples = 61\\nvalue = 212.311'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 26150.423\\nsamples = 51\\nvalue = 188.147'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 1343.167\\nsamples = 10\\nvalue = 335.547')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>427.553645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>97.974272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>63.670676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>13.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>83.896012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>-19.692155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>-19.806921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>-19.915309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>-20.017899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>-20.115197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       3    1_13258    1      rambo   13258      2   \n",
       "    4       4     1_1368    1      rambo    1368      4   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]              0.0             0  1.000000   \n",
       "    1    [9.456276, 13.265001]              0.0             1  0.630930   \n",
       "    2    [6.036743, 11.113943]              0.0             2  0.500000   \n",
       "    3          [0.0, 6.869545]              0.0             3  0.430677   \n",
       "    4         [0.0, 11.113943]              0.0             4  0.386853   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]              0.0            25  0.210310   \n",
       "    26              [0.0, 0.0]              0.0            26  0.208015   \n",
       "    27              [0.0, 0.0]              0.0            27  0.205847   \n",
       "    28              [0.0, 0.0]              0.0            28  0.203795   \n",
       "    29              [0.0, 0.0]              0.0             9  0.289065   \n",
       "\n",
       "             gain        dcg      lambda       delta  \n",
       "qid                                                   \n",
       "1   0   15.000000  30.700871  213.776822  427.553645  \n",
       "    1    4.416508  30.700871   48.987136   97.974272  \n",
       "    2    3.500000  30.700871   31.835338   63.670676  \n",
       "    3    1.292030  30.700871    6.723500   13.447000  \n",
       "    4    5.802792  30.700871   41.948006   83.896012  \n",
       "...           ...        ...         ...         ...  \n",
       "40  25   0.000000  30.207651   -9.846078  -19.692155  \n",
       "    26   0.000000  30.207651   -9.903461  -19.806921  \n",
       "    27   0.000000  30.207651   -9.957655  -19.915309  \n",
       "    28   0.000000  30.207651  -10.008949  -20.017899  \n",
       "    29   0.000000  30.207651  -10.057598  -20.115197  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Important - stable sort. Otherwise DCG swaps get kind of wonky due to position discounts\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['delta'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                swap_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                dcg_after_swap = metric(swap_judgments, at=at)\n",
    "\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
    "                    query_judgments.loc[better, 'delta'] += delta\n",
    "                    \n",
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "                    query_judgments.loc[worse, 'delta'] -= delta\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0.0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "#\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.553645</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.868699</td>\n",
       "      <td>-9.934349</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.149295</td>\n",
       "      <td>-10.074647</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.276314</td>\n",
       "      <td>-10.138157</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.395685</td>\n",
       "      <td>-10.197842</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.508155</td>\n",
       "      <td>-10.254078</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.336529</td>\n",
       "      <td>-10.168264</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.432963</td>\n",
       "      <td>-10.216481</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.524423</td>\n",
       "      <td>-10.262211</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.611330</td>\n",
       "      <td>-10.305665</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.694056</td>\n",
       "      <td>-10.347028</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.069351</td>\n",
       "      <td>-10.534675</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.147879</td>\n",
       "      <td>-10.573939</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.222977</td>\n",
       "      <td>-10.611488</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.294893</td>\n",
       "      <td>-10.647447</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.363851</td>\n",
       "      <td>-10.681926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.430053</td>\n",
       "      <td>-10.715026</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.493681</td>\n",
       "      <td>-10.746841</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.554902</td>\n",
       "      <td>-10.777451</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.013760</td>\n",
       "      <td>-10.006880</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.712923</td>\n",
       "      <td>-9.856462</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.974272</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.545028</td>\n",
       "      <td>-9.772514</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.670676</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.447000</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.896012</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.400912</td>\n",
       "      <td>-4.200456</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.024956</td>\n",
       "      <td>-5.012478</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.243092</td>\n",
       "      <td>-7.621546</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.950401</td>\n",
       "      <td>-7.975200</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.536694</td>\n",
       "      <td>-8.268347</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.032666</td>\n",
       "      <td>-8.516333</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.459201</td>\n",
       "      <td>-8.729601</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.831043</td>\n",
       "      <td>-8.915522</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.158927</td>\n",
       "      <td>-9.079464</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.450871</td>\n",
       "      <td>-9.225435</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.712994</td>\n",
       "      <td>-9.356497</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.950060</td>\n",
       "      <td>-9.475030</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.165834</td>\n",
       "      <td>-9.582917</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.363338</td>\n",
       "      <td>-9.681669</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.613868</td>\n",
       "      <td>-10.806934</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keywords  display_rank  grade  last_prediction       delta      lambda  \\\n",
       "0     rambo             0      4              0.0  427.553645  213.776822   \n",
       "21    rambo            21      0              0.0  -19.868699   -9.934349   \n",
       "23    rambo            23      0              0.0  -20.149295  -10.074647   \n",
       "24    rambo            24      0              0.0  -20.276314  -10.138157   \n",
       "25    rambo            25      0              0.0  -20.395685  -10.197842   \n",
       "26    rambo            26      0              0.0  -20.508155  -10.254078   \n",
       "27    rambo            27      1              0.0  -20.336529  -10.168264   \n",
       "28    rambo            28      1              0.0  -20.432963  -10.216481   \n",
       "29    rambo            29      1              0.0  -20.524423  -10.262211   \n",
       "30    rambo            30      1              0.0  -20.611330  -10.305665   \n",
       "31    rambo            31      1              0.0  -20.694056  -10.347028   \n",
       "32    rambo            32      0              0.0  -21.069351  -10.534675   \n",
       "33    rambo            33      0              0.0  -21.147879  -10.573939   \n",
       "34    rambo            34      0              0.0  -21.222977  -10.611488   \n",
       "35    rambo            35      0              0.0  -21.294893  -10.647447   \n",
       "36    rambo            36      0              0.0  -21.363851  -10.681926   \n",
       "37    rambo            37      0              0.0  -21.430053  -10.715026   \n",
       "38    rambo            38      0              0.0  -21.493681  -10.746841   \n",
       "39    rambo            39      0              0.0  -21.554902  -10.777451   \n",
       "22    rambo            22      0              0.0  -20.013760  -10.006880   \n",
       "20    rambo            20      0              0.0  -19.712923   -9.856462   \n",
       "1     rambo             1      3              0.0   97.974272   48.987136   \n",
       "19    rambo            19      0              0.0  -19.545028   -9.772514   \n",
       "2     rambo             2      3              0.0   63.670676   31.835338   \n",
       "3     rambo             3      2              0.0   13.447000    6.723500   \n",
       "4     rambo             4      4              0.0   83.896012   41.948006   \n",
       "5     rambo             5      1              0.0   -8.400912   -4.200456   \n",
       "6     rambo            40      1              0.0  -10.024956   -5.012478   \n",
       "7     rambo             7      0              0.0  -15.243092   -7.621546   \n",
       "8     rambo             8      0              0.0  -15.950401   -7.975200   \n",
       "9     rambo             9      0              0.0  -16.536694   -8.268347   \n",
       "10    rambo            10      0              0.0  -17.032666   -8.516333   \n",
       "11    rambo            11      0              0.0  -17.459201   -8.729601   \n",
       "12    rambo            12      0              0.0  -17.831043   -8.915522   \n",
       "13    rambo            13      0              0.0  -18.158927   -9.079464   \n",
       "14    rambo            14      0              0.0  -18.450871   -9.225435   \n",
       "15    rambo            15      0              0.0  -18.712994   -9.356497   \n",
       "16    rambo            16      0              0.0  -18.950060   -9.475030   \n",
       "17    rambo            17      0              0.0  -19.165834   -9.582917   \n",
       "18    rambo            18      0              0.0  -19.363338   -9.681669   \n",
       "40    rambo             6      0              0.0  -21.613868  -10.806934   \n",
       "\n",
       "                  features  \n",
       "0   [11.657399, 10.083591]  \n",
       "21              [0.0, 0.0]  \n",
       "23              [0.0, 0.0]  \n",
       "24              [0.0, 0.0]  \n",
       "25              [0.0, 0.0]  \n",
       "26              [0.0, 0.0]  \n",
       "27              [0.0, 0.0]  \n",
       "28              [0.0, 0.0]  \n",
       "29              [0.0, 0.0]  \n",
       "30              [0.0, 0.0]  \n",
       "31              [0.0, 0.0]  \n",
       "32              [0.0, 0.0]  \n",
       "33              [0.0, 0.0]  \n",
       "34              [0.0, 0.0]  \n",
       "35              [0.0, 0.0]  \n",
       "36              [0.0, 0.0]  \n",
       "37              [0.0, 0.0]  \n",
       "38              [0.0, 0.0]  \n",
       "39              [0.0, 0.0]  \n",
       "22              [0.0, 0.0]  \n",
       "20              [0.0, 0.0]  \n",
       "1    [9.456276, 13.265001]  \n",
       "19              [0.0, 0.0]  \n",
       "2    [6.036743, 11.113943]  \n",
       "3          [0.0, 6.869545]  \n",
       "4         [0.0, 11.113943]  \n",
       "5         [0.0, 7.8627386]  \n",
       "6          [0.0, 4.563677]  \n",
       "7               [0.0, 0.0]  \n",
       "8               [0.0, 0.0]  \n",
       "9               [0.0, 0.0]  \n",
       "10              [0.0, 0.0]  \n",
       "11              [0.0, 0.0]  \n",
       "12              [0.0, 0.0]  \n",
       "13              [0.0, 0.0]  \n",
       "14              [0.0, 0.0]  \n",
       "15              [0.0, 0.0]  \n",
       "16              [0.0, 0.0]  \n",
       "17              [0.0, 0.0]  \n",
       "18              [0.0, 0.0]  \n",
       "40              [0.0, 0.0]  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.loc[1, :][['keywords', 'display_rank',  'grade', 'last_prediction', 'delta', 'lambda', 'features']].sort_values('last_prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173.48027244])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.72350002])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>uid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>106.888411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>48.010828</td>\n",
       "      <td>26.458003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>31.382749</td>\n",
       "      <td>18.143963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>6.460558</td>\n",
       "      <td>7.448315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>43.639845</td>\n",
       "      <td>21.819923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>4.923039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>4.951730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>4.978827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>5.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>5.028799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid        uid   keywords   docId  grade                features  \\\n",
       "0       1     1_7555      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1       1     1_1370      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2       1     1_1369      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1    1_13258      rambo   13258      2         [0.0, 6.869545]   \n",
       "4       1     1_1368      rambo    1368      4        [0.0, 11.113943]   \n",
       "...   ...        ...        ...     ...    ...                     ...   \n",
       "1385   40   40_37079  star wars   37079      0              [0.0, 0.0]   \n",
       "1386   40  40_126757  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40   40_39797  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40   40_18112  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40   40_43052  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount       gain  train_dcg  \\\n",
       "0                   0             0  1.000000  15.000000  30.700871   \n",
       "1                   0             1  0.630930   4.416508  30.700871   \n",
       "2                   0             2  0.500000   3.500000  30.700871   \n",
       "3                   0             3  0.430677   1.292030  30.700871   \n",
       "4                   0             4  0.386853   5.802792  30.700871   \n",
       "...               ...           ...       ...        ...        ...   \n",
       "1385                0            25  0.210310   0.000000  30.207651   \n",
       "1386                0            26  0.208015   0.000000  30.207651   \n",
       "1387                0            27  0.205847   0.000000  30.207651   \n",
       "1388                0            28  0.203795   0.000000  30.207651   \n",
       "1389                0             9  0.289065   0.000000  30.207651   \n",
       "\n",
       "            dcg      lambda      weight  \n",
       "0     30.552986  213.776822  106.888411  \n",
       "1     30.552986   48.010828   26.458003  \n",
       "2     30.552986   31.382749   18.143963  \n",
       "3     30.552986    6.460558    7.448315  \n",
       "4     30.552986   43.639845   21.819923  \n",
       "...         ...         ...         ...  \n",
       "1385  30.120435   -9.846078    4.923039  \n",
       "1386  30.120435   -9.903461    4.951730  \n",
       "1387  30.120435   -9.957655    4.978827  \n",
       "1388  30.120435  -10.008949    5.004475  \n",
       "1389  30.120435  -10.057598    5.028799  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
    " \n",
    "    qid = query_judgments.loc[0, 'qid']\n",
    "    keywords = query_judgments.loc[0, 'keywords']\n",
    "\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "\n",
    "    for better in range(0,len(query_judgments)):\n",
    "         for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                return query_judgments\n",
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(train_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta != 0.0:\n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff)) \n",
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=4)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label each row with its unique prediction (ie tree path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "lambdas_per_query['path'] = tree_paths(tree3, train_set['features'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    1673.720249\n",
       "1010010    1467.050422\n",
       "1100100     538.598514\n",
       "1101000    4655.114588\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    3334.073882\n",
       "1010010    2787.629391\n",
       "1100100     893.120752\n",
       "1101000   -7014.824025\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1010001': 1.9920138295288714,\n",
       " '1010010': 1.9001592234365985,\n",
       " '1100100': 1.658230999946508,\n",
       " '1101000': -1.5069068425436136}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 181.2, 'X[0] <= 10.328\\nmse = 883.211\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 179.235\\nsamples = 1324\\nvalue = -4.624'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 62.9\\nsamples = 1301\\nvalue = -5.392'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 4838.036\\nsamples = 23\\nvalue = 38.831'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 13.782\\nmse = 5973.405\\nsamples = 66\\nvalue = 92.753'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 5828.45\\nsamples = 39\\nvalue = 71.478'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 4584.565\\nsamples = 27\\nvalue = 123.484')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the original tree, and provide new value\n",
    "\n",
    "Instead of directly using the provided decision tree, we want to use our prediction for each leaf. This `OverridenDecisionTree` takes the original tree, looks up the prediction path in the original tree, but uses our values for the predicted variable instead of what's here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.50690684,  1.658231  ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X, use_original=False):\n",
    "        if use_original:\n",
    "            return self.predict(X)\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
    "* While DCG converges, it does sometimes wander a tad, so there might be more room for improvement\n",
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "Train DCGs\n",
      "mean    20.03521601393222\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    20.558839639948378\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-285d00895833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mjudgments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftr_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mlambdas_per_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_mart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudgments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjudgments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-285d00895833>\u001b[0m in \u001b[0;36mlambda_mart\u001b[0;34m(judgments, rounds, learning_rate, max_leaf_nodes, metric)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# ------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#1. Build pair-wise predictors for this round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n\u001b[0m\u001b[1;32m     32\u001b[0m                                                                    axis=1, metric=dcg)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \"\"\"\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                 \u001b[0msdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                 \u001b[0mresult_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mfast_apply\u001b[0;34m(self, f, sdata, names)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;31m# must return keys::list, values::list, mutated::bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_frame_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.apply_frame_axis0\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nan\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-10840cc48db9>\u001b[0m in \u001b[0;36mcompute_swaps_scaled_with_weights\u001b[0;34m(query_judgments, axis, metric, at)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0mquery_judgments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0mquery_judgments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0;31m# We have to operate column-wise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1815\u001b[0m             \u001b[0;31m# scalar value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0milocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_with_indexer_2d_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m             \u001b[0;31m# set the item, possibly having a dtype change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1919\u001b[0;31m             \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1920\u001b[0m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5931\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5932\u001b[0m         \"\"\"\n\u001b[0;32m-> 5933\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5934\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5935\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;31m# ---------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block_same_class\u001b[0;34m(self, values, placement)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# We assume maybe_coerce_values has already been called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.1, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query.iloc[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://es-learn-to-rank.labs.o19s.com/RankyMcRankFace.jar\n",
      "Running java -jar /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t DCG@10 -tree 10 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/training.txt -save data/title_model.txt \n",
      "Delete model title: 404\n",
      "Created Model title [Status: 201]\n",
      "Model saved\n",
      "Every N rounds of ranklib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.692,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.73,\n",
       " 20.7506,\n",
       " 20.6507,\n",
       " 20.7237,\n",
       " 20.7237]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pure Pandas Implementation?\n",
    "\n",
    "Can we make it faster by vectorizing with pandas?\n",
    "\n",
    "Turns out Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "lambdas_per_query['last_prediction'] = 0.0\n",
    "lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "#TBD - How do generalize this?\n",
    "lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1) # * lambdas_per_query['discount']\n",
    "\n",
    "lambdas_per_query[['join_key', 'qid', 'display_rank', 'discount', 'grade', 'gain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise deltas\n",
    "\n",
    "Delta captures pair-wise difference of the ranking metric (ie DCG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each group paired with each other group\n",
    "swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "# changes[j][i] = changes[i][j] = (discount(i) - discount(j)) * (gain(rel[i]) - gain(rel[j]));\n",
    "swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise rhos\n",
    "\n",
    "Rho captures pair-wise difference of the current model's prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute lambdas\n",
    "\n",
    "For every row where grade_x > grade_y,  compute `delta*rho`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps['lambda'] = 0\n",
    "slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho', 'lambda']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get per-key lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid  display_rank_x\n",
       "1    0                 211.781688\n",
       "     1                  46.938369\n",
       "     2                  30.637615\n",
       "     3                   5.888732\n",
       "     4                  43.177578\n",
       "                          ...    \n",
       "40   25                 -9.853045\n",
       "     26                 -9.911575\n",
       "     27                 -9.966853\n",
       "     28                -10.019174\n",
       "     29                -10.068796\n",
       "Name: lambda, Length: 1390, dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better minus worse\n",
    "lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "lambdas = lambdas_x - lambdas_y\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>uid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "      <th>path</th>\n",
       "      <th>lambdas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>106.888411</td>\n",
       "      <td>1010010</td>\n",
       "      <td>211.781688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>48.010828</td>\n",
       "      <td>26.458003</td>\n",
       "      <td>1100100</td>\n",
       "      <td>49.390958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>31.382749</td>\n",
       "      <td>18.143963</td>\n",
       "      <td>1101000</td>\n",
       "      <td>33.090203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>6.460558</td>\n",
       "      <td>7.448315</td>\n",
       "      <td>1101000</td>\n",
       "      <td>10.106768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>43.639845</td>\n",
       "      <td>21.819923</td>\n",
       "      <td>1101000</td>\n",
       "      <td>43.177578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>4.923039</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>4.951730</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>4.978827</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>5.004475</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>5.028799</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.568410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid        uid   keywords   docId  grade                features  \\\n",
       "0       1     1_7555      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1       1     1_1370      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2       1     1_1369      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1    1_13258      rambo   13258      2         [0.0, 6.869545]   \n",
       "4       1     1_1368      rambo    1368      4        [0.0, 11.113943]   \n",
       "...   ...        ...        ...     ...    ...                     ...   \n",
       "1385   40   40_37079  star wars   37079      0              [0.0, 0.0]   \n",
       "1386   40  40_126757  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40   40_39797  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40   40_18112  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40   40_43052  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount       gain  train_dcg  \\\n",
       "0                   0             0  1.000000  15.000000  30.700871   \n",
       "1                   0             1  0.630930   4.416508  30.700871   \n",
       "2                   0             2  0.500000   3.500000  30.700871   \n",
       "3                   0             3  0.430677   1.292030  30.700871   \n",
       "4                   0             4  0.386853   5.802792  30.700871   \n",
       "...               ...           ...       ...        ...        ...   \n",
       "1385                0            25  0.210310   0.000000  30.207651   \n",
       "1386                0            26  0.208015   0.000000  30.207651   \n",
       "1387                0            27  0.205847   0.000000  30.207651   \n",
       "1388                0            28  0.203795   0.000000  30.207651   \n",
       "1389                0             9  0.289065   0.000000  30.207651   \n",
       "\n",
       "            dcg      lambda      weight     path     lambdas  \n",
       "0     30.552986  213.776822  106.888411  1010010  211.781688  \n",
       "1     30.552986   48.010828   26.458003  1100100   49.390958  \n",
       "2     30.552986   31.382749   18.143963  1101000   33.090203  \n",
       "3     30.552986    6.460558    7.448315  1101000   10.106768  \n",
       "4     30.552986   43.639845   21.819923  1101000   43.177578  \n",
       "...         ...         ...         ...      ...         ...  \n",
       "1385  30.120435   -9.846078    4.923039  1101000    0.000000  \n",
       "1386  30.120435   -9.903461    4.951730  1101000    0.000000  \n",
       "1387  30.120435   -9.957655    4.978827  1101000    0.000000  \n",
       "1388  30.120435  -10.008949    5.004475  1101000    0.000000  \n",
       "1389  30.120435  -10.057598    5.028799  1101000    0.568410  \n",
       "\n",
       "[1390 rows x 16 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.merge(lambdas, left_on=['qid', 'display_rank'], right_on=['qid', 'display_rank_x'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambdas(lambdas_per_query):\n",
    "    lambdas_per_query = lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')\n",
    "    lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "    #TBD - How do generalize this to any metric?\n",
    "    lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "    lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1)\n",
    "\n",
    "    # swaps dataframe holds each pair-wise swap computed (shrink columns for memory?)   \n",
    "    # Optimization of swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "    # to limit to just needed columns\n",
    "    to_swap = lambdas_per_query[['qid', 'display_rank', 'grade', 'last_prediction', 'discount', 'gain']]\n",
    "    #to_swap = lambdas_per_query\n",
    "    swaps = to_swap.merge(to_swap, on='qid', how='outer')\n",
    "\n",
    "    # delta - delta in DCG due to swap\n",
    "    swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "    \n",
    "    # rho - based on current model prediction delta\n",
    "    swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "    swaps['weight'] = swaps['rho'] * (1.0 - swaps['rho']) * swaps['delta']\n",
    "\n",
    "    # Compute lambdas (the next model in ensemble's predictors) when grade_x > grade_y\n",
    "    swaps['lambda'] = 0\n",
    "    slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "    swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "    \n",
    "    # accumulate lambdas and add back to model\n",
    "    lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "    lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "\n",
    "    weights_x = swaps.groupby(['qid', 'display_rank_x'])['weight'].sum().rename('weight')\n",
    "    weights_y = swaps.groupby(['qid', 'display_rank_y'])['weight'].sum().rename('weight')\n",
    "    \n",
    "    weights = weights_x + weights_y\n",
    "    lambdas = lambdas_x - lambdas_y\n",
    "\n",
    "    lambdas_per_query = lambdas_per_query.merge(lambdas, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "    lambdas_per_query = lambdas_per_query.merge(weights, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "\n",
    "    return lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "grade                 4\n",
      "last_prediction    0.01\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.193092700772873\n",
      "----------\n",
      "round 1\n",
      "grade                     4\n",
      "last_prediction    0.019915\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 2\n",
      "grade                     4\n",
      "last_prediction    0.029745\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 3\n",
      "grade                     4\n",
      "last_prediction    0.039492\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 4\n",
      "grade                     4\n",
      "last_prediction    0.049159\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 5\n",
      "grade                     4\n",
      "last_prediction    0.058748\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 6\n",
      "grade                    4\n",
      "last_prediction    0.06826\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 7\n",
      "grade                     4\n",
      "last_prediction    0.077698\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 8\n",
      "grade                     4\n",
      "last_prediction    0.087063\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 9\n",
      "grade                     4\n",
      "last_prediction    0.096357\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 10\n",
      "grade                     4\n",
      "last_prediction    0.105582\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 11\n",
      "grade                     4\n",
      "last_prediction    0.114739\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 12\n",
      "grade                    4\n",
      "last_prediction    0.12383\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 13\n",
      "grade                     4\n",
      "last_prediction    0.132857\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 14\n",
      "grade                    4\n",
      "last_prediction    0.14182\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 15\n",
      "grade                     4\n",
      "last_prediction    0.150722\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 16\n",
      "grade                     4\n",
      "last_prediction    0.159564\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 17\n",
      "grade                     4\n",
      "last_prediction    0.168347\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 18\n",
      "grade                     4\n",
      "last_prediction    0.177072\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 19\n",
      "grade                     4\n",
      "last_prediction    0.185742\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 20\n",
      "grade                     4\n",
      "last_prediction    0.194356\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 21\n",
      "grade                     4\n",
      "last_prediction    0.202916\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 22\n",
      "grade                     4\n",
      "last_prediction    0.211424\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 23\n",
      "grade                    4\n",
      "last_prediction    0.21988\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 24\n",
      "grade                     4\n",
      "last_prediction    0.228285\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 25\n",
      "grade                     4\n",
      "last_prediction    0.236641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 26\n",
      "grade                     4\n",
      "last_prediction    0.244949\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 27\n",
      "grade                     4\n",
      "last_prediction    0.253209\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 28\n",
      "grade                     4\n",
      "last_prediction    0.261423\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 29\n",
      "grade                     4\n",
      "last_prediction    0.269591\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 30\n",
      "grade                     4\n",
      "last_prediction    0.277715\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 31\n",
      "grade                     4\n",
      "last_prediction    0.285795\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 32\n",
      "grade                     4\n",
      "last_prediction    0.293832\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 33\n",
      "grade                     4\n",
      "last_prediction    0.301828\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 34\n",
      "grade                     4\n",
      "last_prediction    0.309782\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 35\n",
      "grade                     4\n",
      "last_prediction    0.317695\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 36\n",
      "grade                     4\n",
      "last_prediction    0.325569\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 37\n",
      "grade                     4\n",
      "last_prediction    0.333405\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 38\n",
      "grade                     4\n",
      "last_prediction    0.341202\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 39\n",
      "grade                     4\n",
      "last_prediction    0.348962\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 40\n",
      "grade                     4\n",
      "last_prediction    0.356685\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 41\n",
      "grade                     4\n",
      "last_prediction    0.364372\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 42\n",
      "grade                     4\n",
      "last_prediction    0.372024\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 43\n",
      "grade                     4\n",
      "last_prediction    0.379641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 44\n",
      "grade                     4\n",
      "last_prediction    0.387224\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 45\n",
      "grade                     4\n",
      "last_prediction    0.394774\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 46\n",
      "grade                     4\n",
      "last_prediction    0.402291\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 47\n",
      "grade                     4\n",
      "last_prediction    0.409776\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 48\n",
      "grade                     4\n",
      "last_prediction    0.417229\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 49\n",
      "grade                     4\n",
      "last_prediction    0.424651\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart_pure(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = compute_lambdas(lambdas_per_query)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(lambdas_per_query.loc[0, ['grade', 'last_prediction']])\n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        lambdas_per_query['discounted_gain'] = lambdas_per_query['gain'] * lambdas_per_query['discount'] \n",
    "        dcg = lambdas_per_query[lambdas_per_query['display_rank'] < 10].groupby('qid')['discounted_gain'].sum().mean()\n",
    "        print(\"mean   \", dcg)\n",
    "        print(\"----------\")\n",
    "        \n",
    "        lambdas_per_query = lambdas_per_query.drop(['lambda', 'weight'], axis=1)\n",
    "    return lambdas_per_query\n",
    "\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "grade                 4\n",
      "last_prediction    0.01\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.193092700772873\n",
      "----------\n",
      "round 1\n",
      "grade                     4\n",
      "last_prediction    0.019915\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 2\n",
      "grade                     4\n",
      "last_prediction    0.029745\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 3\n",
      "grade                     4\n",
      "last_prediction    0.039492\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 4\n",
      "grade                     4\n",
      "last_prediction    0.049159\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 5\n",
      "grade                     4\n",
      "last_prediction    0.058748\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 6\n",
      "grade                    4\n",
      "last_prediction    0.06826\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 7\n",
      "grade                     4\n",
      "last_prediction    0.077698\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 8\n",
      "grade                     4\n",
      "last_prediction    0.087063\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 9\n",
      "grade                     4\n",
      "last_prediction    0.096357\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 10\n",
      "grade                     4\n",
      "last_prediction    0.105582\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 11\n",
      "grade                     4\n",
      "last_prediction    0.114739\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 12\n",
      "grade                    4\n",
      "last_prediction    0.12383\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 13\n",
      "grade                     4\n",
      "last_prediction    0.132857\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 14\n",
      "grade                    4\n",
      "last_prediction    0.14182\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 15\n",
      "grade                     4\n",
      "last_prediction    0.150722\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 16\n",
      "grade                     4\n",
      "last_prediction    0.159564\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 17\n",
      "grade                     4\n",
      "last_prediction    0.168347\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 18\n",
      "grade                     4\n",
      "last_prediction    0.177072\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 19\n",
      "grade                     4\n",
      "last_prediction    0.185742\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 20\n",
      "grade                     4\n",
      "last_prediction    0.194356\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 21\n",
      "grade                     4\n",
      "last_prediction    0.202916\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 22\n",
      "grade                     4\n",
      "last_prediction    0.211424\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 23\n",
      "grade                    4\n",
      "last_prediction    0.21988\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 24\n",
      "grade                     4\n",
      "last_prediction    0.228285\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 25\n",
      "grade                     4\n",
      "last_prediction    0.236641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 26\n",
      "grade                     4\n",
      "last_prediction    0.244949\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 27\n",
      "grade                     4\n",
      "last_prediction    0.253209\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 28\n",
      "grade                     4\n",
      "last_prediction    0.261423\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 29\n",
      "grade                     4\n",
      "last_prediction    0.269591\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 30\n",
      "grade                     4\n",
      "last_prediction    0.277715\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 31\n",
      "grade                     4\n",
      "last_prediction    0.285795\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 32\n",
      "grade                     4\n",
      "last_prediction    0.293832\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 33\n",
      "grade                     4\n",
      "last_prediction    0.301828\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 34\n",
      "grade                     4\n",
      "last_prediction    0.309782\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 35\n",
      "grade                     4\n",
      "last_prediction    0.317695\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 36\n",
      "grade                     4\n",
      "last_prediction    0.325569\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 37\n",
      "grade                     4\n",
      "last_prediction    0.333405\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 38\n",
      "grade                     4\n",
      "last_prediction    0.341202\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 39\n",
      "grade                     4\n",
      "last_prediction    0.348962\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 40\n",
      "grade                     4\n",
      "last_prediction    0.356685\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 41\n",
      "grade                     4\n",
      "last_prediction    0.364372\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 42\n",
      "grade                     4\n",
      "last_prediction    0.372024\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 43\n",
      "grade                     4\n",
      "last_prediction    0.379641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 44\n",
      "grade                     4\n",
      "last_prediction    0.387224\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 45\n",
      "grade                     4\n",
      "last_prediction    0.394774\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 46\n",
      "grade                     4\n",
      "last_prediction    0.402291\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 47\n",
      "grade                     4\n",
      "last_prediction    0.409776\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 48\n",
      "grade                     4\n",
      "last_prediction    0.417229\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 49\n",
      "grade                     4\n",
      "last_prediction    0.424651\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      " "
     ]
    }
   ],
   "source": [
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "%prun -s cumtime lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/RankyMcRankFace.jar already exists\n",
      "Running java -jar /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t DCG@10 -tree 10 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/training.txt -save data/title_model.txt \n",
      "Delete model title: 200\n",
      "Created Model title [Status: 201]\n",
      "Model saved\n",
      " Every N rounds of ranklib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.692,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.73,\n",
       " 20.7506,\n",
       " 20.6507,\n",
       " 20.7237,\n",
       " 20.7237]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltr.ranklib import train\n",
    "%prun -s cumtime trainLog  = train(client, training_set=ftr_logger.logged, index='tmdb', trees=10, featureSet='movies', modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
