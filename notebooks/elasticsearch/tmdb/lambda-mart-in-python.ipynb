{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log Elasticsearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with Elasticsearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
    "1. **Likely bug** - When using DCG, the model here seems to wander around more than Ranklib, instead of converging. I think likely this points at an underlying bug that's actively being investigated.\n",
    "2. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the Elasticsearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries...\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>183.343798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>6.189645</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>116.134366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>39.713833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>2.408240</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>30.087439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>1.115772</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>8.628473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1371</td>\n",
       "      <td>40_81899</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>81899</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-10.968332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1370</td>\n",
       "      <td>40_209276</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>209276</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.062526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1368</td>\n",
       "      <td>40_52959</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>52959</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.151815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1367</td>\n",
       "      <td>40_85783</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>85783</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.236624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.317325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       1     1_1370    1      rambo    1370      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1371   40_81899   40  star wars   81899      0   \n",
       "    26   1370  40_209276   40  star wars  209276      0   \n",
       "    27   1368   40_52959   40  star wars   52959      0   \n",
       "    28   1367   40_85783   40  star wars   85783      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount      gain        dcg  \\\n",
       "qid                                                                           \n",
       "1   0   [11.657399, 10.083591]             0  0.500000  8.000000  22.796491   \n",
       "    1         [0.0, 11.113943]             1  0.386853  6.189645  22.796491   \n",
       "    2    [6.036743, 11.113943]             2  0.333333  2.666667  22.796491   \n",
       "    3    [9.456276, 13.265001]             3  0.301030  2.408240  22.796491   \n",
       "    4          [0.0, 6.869545]             4  0.278943  1.115772  22.796491   \n",
       "...                        ...           ...       ...       ...        ...   \n",
       "40  25         [0.0, 6.868508]            25  0.173765  0.173765  21.491637   \n",
       "    26        [5.8994045, 0.0]            26  0.172195  0.172195  21.491637   \n",
       "    27           [7.2726, 0.0]            27  0.170707  0.170707  21.491637   \n",
       "    28           [7.2726, 0.0]            28  0.169294  0.169294  21.491637   \n",
       "    29              [0.0, 0.0]             8  0.231378  0.231378  21.491637   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   183.343798  \n",
       "    1   116.134366  \n",
       "    2    39.713833  \n",
       "    3    30.087439  \n",
       "    4     8.628473  \n",
       "...            ...  \n",
       "40  25  -10.968332  \n",
       "    26  -11.062526  \n",
       "    27  -11.151815  \n",
       "    28  -11.236624  \n",
       "    29  -11.317325  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
    "    ranked_list['discount'] = 1 / (1 + (np.log2(2 + ranked_list['display_rank'])))\n",
    "    ranked_list['gain'] = (2**ranked_list['grade']) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('grade', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = best_dcg - dcg_after_swap\n",
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
       "1     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "2     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "6     169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "7     170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "8     171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "9     173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "10    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "11    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "12    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "13    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "14    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "15    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "16    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "17    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "18    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "19    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "20    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "21    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "22    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "23    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "24    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "25    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "26    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "27    157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "28    156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "29    155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "30    154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "31    153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>183.343798</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.134366</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.713833</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.087439</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.628473</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>-10.968332</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-11.062526</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-11.151815</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-11.236624</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-11.317325</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
       "1   0   183.343798  [11.657399, 10.083591]\n",
       "    1   116.134366        [0.0, 11.113943]\n",
       "    2    39.713833   [6.036743, 11.113943]\n",
       "    3    30.087439   [9.456276, 13.265001]\n",
       "    4     8.628473         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -10.968332         [0.0, 6.868508]\n",
       "    26  -11.062526        [5.8994045, 0.0]\n",
       "    27  -11.151815           [7.2726, 0.0]\n",
       "    28  -11.236624           [7.2726, 0.0]\n",
       "    29  -11.317325              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([183.34379848])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.80365819])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 181.2, 'X[0] <= 10.666\\nmse = 793.283\\nsamples = 1390\\nvalue = 0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 222.998\\nsamples = 1329\\nvalue = -4.264'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 107.23\\nsamples = 1301\\nvalue = -5.173'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 3778.904\\nsamples = 28\\nvalue = 37.982'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 13.782\\nmse = 4190.964\\nsamples = 61\\nvalue = 92.903'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 4938.44\\nsamples = 34\\nvalue = 72.93'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 2114.76\\nsamples = 27\\nvalue = 118.054')]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>7.292951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.100504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>3.094822</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.697540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>1_32221</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>32221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>1_801</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>801</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>2.992312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1_70</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.022346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1366</td>\n",
       "      <td>40_1891</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1891</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1365</td>\n",
       "      <td>40_1892</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1892</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 2.4547963]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1364</td>\n",
       "      <td>40_1895</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.487482, 2.062405]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.682829</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1363</td>\n",
       "      <td>40_330459</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>330459</td>\n",
       "      <td>3</td>\n",
       "      <td>[7.2694716, 4.237955]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>1.354350</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.013697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2      39    1_32221    1      rambo   32221      0   \n",
       "    3      29      1_801    1      rambo     801      1   \n",
       "    4      22       1_70    1      rambo      70      0   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1366    40_1891   40  star wars    1891      3   \n",
       "    26   1365    40_1892   40  star wars    1892      3   \n",
       "    27   1364    40_1895   40  star wars    1895      2   \n",
       "    28   1363  40_330459   40  star wars  330459      3   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]         7.292951             0  0.500000   \n",
       "    1    [9.456276, 13.265001]         3.798236             1  0.386853   \n",
       "    2               [0.0, 0.0]        -0.517338             2  0.333333   \n",
       "    3               [0.0, 0.0]        -0.517338             3  0.301030   \n",
       "    4               [0.0, 0.0]        -0.517338             4  0.278943   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]        -0.517338            25  0.173765   \n",
       "    26        [0.0, 2.4547963]        -0.517338            26  0.172195   \n",
       "    27    [6.487482, 2.062405]        -0.517338            27  0.170707   \n",
       "    28   [7.2694716, 4.237955]        -0.517338            28  0.169294   \n",
       "    29              [0.0, 0.0]        -0.517338             1  0.386853   \n",
       "\n",
       "            gain        dcg    lambda  \n",
       "qid                                    \n",
       "1   0   8.000000  13.741487  0.100504  \n",
       "    1   3.094822  13.741487  0.697540  \n",
       "    2   0.333333  13.741487 -0.005952  \n",
       "    3   0.602060  13.741487  2.992312  \n",
       "    4   0.278943  13.741487 -0.022346  \n",
       "...          ...        ...       ...  \n",
       "40  25  1.390123  10.793185 -0.002116  \n",
       "    26  1.377563  10.793185 -0.002126  \n",
       "    27  0.682829  10.793185 -0.002136  \n",
       "    28  1.354350  10.793185 -0.002145  \n",
       "    29  0.386853  10.793185 -0.013697  \n",
       "\n",
       "[1390 rows x 13 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = best_dcg - dcg_after_swap\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
    "\n",
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>7.292951</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.002105</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>0.697540</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.024589</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.019625</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.025089</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.024931</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.020225</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.023536</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1.098537</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2.992312</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.022346</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.237120</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.235248</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.233307</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.231291</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.229197</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.227018</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.224748</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.222381</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.219909</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.217324</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.214616</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.032095</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.050806</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.039664</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.195112</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.191166</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.186963</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.177656</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.172476</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.166880</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.160809</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.154187</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.146926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.138911</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.129999</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.238927</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keywords  grade  last_prediction    lambda                features\n",
       "0     rambo      4         7.292951  0.100504  [11.657399, 10.083591]\n",
       "26    rambo      4        -0.517338  0.000000        [0.0, 11.113943]\n",
       "24    rambo      3        -0.517338 -0.002105   [6.036743, 11.113943]\n",
       "1     rambo      3         3.798236  0.697540   [9.456276, 13.265001]\n",
       "25    rambo      2        -0.517338 -0.024589         [0.0, 6.869545]\n",
       "10    rambo      1        -0.517338 -0.019625              [0.0, 0.0]\n",
       "28    rambo      1        -0.517338 -0.025089         [0.0, 4.563677]\n",
       "27    rambo      1        -0.517338 -0.024931        [0.0, 7.8627386]\n",
       "11    rambo      1        -0.517338 -0.020225              [0.0, 0.0]\n",
       "20    rambo      1        -0.517338 -0.023536              [0.0, 0.0]\n",
       "9     rambo      1        -0.517338  1.098537              [0.0, 0.0]\n",
       "3     rambo      1        -0.517338  2.992312              [0.0, 0.0]\n",
       "4     rambo      0        -0.517338 -0.022346              [0.0, 0.0]\n",
       "39    rambo      0        -0.517338 -0.237120              [0.0, 0.0]\n",
       "38    rambo      0        -0.517338 -0.235248              [0.0, 0.0]\n",
       "37    rambo      0        -0.517338 -0.233307              [0.0, 0.0]\n",
       "36    rambo      0        -0.517338 -0.231291              [0.0, 0.0]\n",
       "35    rambo      0        -0.517338 -0.229197              [0.0, 0.0]\n",
       "34    rambo      0        -0.517338 -0.227018              [0.0, 0.0]\n",
       "33    rambo      0        -0.517338 -0.224748              [0.0, 0.0]\n",
       "32    rambo      0        -0.517338 -0.222381              [0.0, 0.0]\n",
       "31    rambo      0        -0.517338 -0.219909              [0.0, 0.0]\n",
       "30    rambo      0        -0.517338 -0.217324              [0.0, 0.0]\n",
       "29    rambo      0        -0.517338 -0.214616              [0.0, 0.0]\n",
       "2     rambo      0        -0.517338 -0.005952              [0.0, 0.0]\n",
       "5     rambo      0        -0.517338 -0.032095              [0.0, 0.0]\n",
       "8     rambo      0        -0.517338 -0.050806              [0.0, 0.0]\n",
       "6     rambo      0        -0.517338 -0.039664              [0.0, 0.0]\n",
       "23    rambo      0        -0.517338 -0.195112              [0.0, 0.0]\n",
       "22    rambo      0        -0.517338 -0.191166              [0.0, 0.0]\n",
       "21    rambo      0        -0.517338 -0.186963              [0.0, 0.0]\n",
       "7     rambo      0        -0.517338 -0.045760              [0.0, 0.0]\n",
       "19    rambo      0        -0.517338 -0.177656              [0.0, 0.0]\n",
       "18    rambo      0        -0.517338 -0.172476              [0.0, 0.0]\n",
       "17    rambo      0        -0.517338 -0.166880              [0.0, 0.0]\n",
       "16    rambo      0        -0.517338 -0.160809              [0.0, 0.0]\n",
       "15    rambo      0        -0.517338 -0.154187              [0.0, 0.0]\n",
       "14    rambo      0        -0.517338 -0.146926              [0.0, 0.0]\n",
       "13    rambo      0        -0.517338 -0.138911              [0.0, 0.0]\n",
       "12    rambo      0        -0.517338 -0.129999              [0.0, 0.0]\n",
       "40    rambo      0        -0.517338 -0.238927              [0.0, 0.0]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.loc[1, :][['keywords', 'grade', 'last_prediction', 'lambda', 'features']].sort_values('grade', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10050401])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02458865])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.75718641e-16]],\n",
       "\n",
       "       [[-4.26415296e+00]],\n",
       "\n",
       "       [[ 9.29026112e+01]],\n",
       "\n",
       "       [[-5.17337837e+00]],\n",
       "\n",
       "       [[ 3.79823562e+01]],\n",
       "\n",
       "       [[ 7.29295071e+01]],\n",
       "\n",
       "       [[ 1.18053928e+02]]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree_.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'apply',\n",
       " 'capacity',\n",
       " 'children_left',\n",
       " 'children_right',\n",
       " 'compute_feature_importances',\n",
       " 'compute_partial_dependence',\n",
       " 'decision_path',\n",
       " 'feature',\n",
       " 'impurity',\n",
       " 'max_depth',\n",
       " 'max_n_classes',\n",
       " 'n_classes',\n",
       " 'n_features',\n",
       " 'n_leaves',\n",
       " 'n_node_samples',\n",
       " 'n_outputs',\n",
       " 'node_count',\n",
       " 'predict',\n",
       " 'threshold',\n",
       " 'value',\n",
       " 'weighted_n_node_samples']"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree.tree_.decision_path(np.array([[0.0, 6.869545]], dtype='float32')).toarray()\n",
    "dir(tree.tree_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>7.292951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>0.099688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>3.094822</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.697540</td>\n",
       "      <td>0.740546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>1_32221</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>32221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>0.005887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>1_801</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>801</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>2.992312</td>\n",
       "      <td>1.507942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1_70</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.022346</td>\n",
       "      <td>0.016692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1366</td>\n",
       "      <td>40_1891</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1891</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1365</td>\n",
       "      <td>40_1892</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1892</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 2.4547963]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002126</td>\n",
       "      <td>0.002125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1364</td>\n",
       "      <td>40_1895</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.487482, 2.062405]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.682829</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002136</td>\n",
       "      <td>0.002135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1363</td>\n",
       "      <td>40_330459</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>330459</td>\n",
       "      <td>3</td>\n",
       "      <td>[7.2694716, 4.237955]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>1.354350</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002145</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>0.013544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2      39    1_32221    1      rambo   32221      0   \n",
       "    3      29      1_801    1      rambo     801      1   \n",
       "    4      22       1_70    1      rambo      70      0   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1366    40_1891   40  star wars    1891      3   \n",
       "    26   1365    40_1892   40  star wars    1892      3   \n",
       "    27   1364    40_1895   40  star wars    1895      2   \n",
       "    28   1363  40_330459   40  star wars  330459      3   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]         7.292951             0  0.500000   \n",
       "    1    [9.456276, 13.265001]         3.798236             1  0.386853   \n",
       "    2               [0.0, 0.0]        -0.517338             2  0.333333   \n",
       "    3               [0.0, 0.0]        -0.517338             3  0.301030   \n",
       "    4               [0.0, 0.0]        -0.517338             4  0.278943   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]        -0.517338            25  0.173765   \n",
       "    26        [0.0, 2.4547963]        -0.517338            26  0.172195   \n",
       "    27    [6.487482, 2.062405]        -0.517338            27  0.170707   \n",
       "    28   [7.2694716, 4.237955]        -0.517338            28  0.169294   \n",
       "    29              [0.0, 0.0]        -0.517338             1  0.386853   \n",
       "\n",
       "            gain  train_dcg        dcg    lambda    weight  \n",
       "qid                                                         \n",
       "1   0   8.000000  13.741487  13.741487  0.100504  0.099688  \n",
       "    1   3.094822  13.741487  13.741487  0.697540  0.740546  \n",
       "    2   0.333333  13.741487  13.741487 -0.005952  0.005887  \n",
       "    3   0.602060  13.741487  13.741487  2.992312  1.507942  \n",
       "    4   0.278943  13.741487  13.741487 -0.022346  0.016692  \n",
       "...          ...        ...        ...       ...       ...  \n",
       "40  25  1.390123  11.668802  10.793185 -0.002116  0.002115  \n",
       "    26  1.377563  11.668802  10.793185 -0.002126  0.002125  \n",
       "    27  0.682829  11.668802  10.793185 -0.002136  0.002135  \n",
       "    28  1.354350  11.668802  10.793185 -0.002145  0.002144  \n",
       "    29  0.386853  11.668802  10.793185 -0.013697  0.013544  \n",
       "\n",
       "[1390 rows x 15 columns]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = train_dcg - dcg_after_swap\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    assert(not math.isnan(query_judgments.loc[better, 'last_prediction']))\n",
    "                    assert(not math.isnan(query_judgments.loc[worse, 'last_prediction']))\n",
    "                    \n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff))   \n",
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=4)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-474-421c2af06928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambdas_per_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'groupby'"
     ]
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010101     21.084755\n",
       "1010110     26.392483\n",
       "1011000     78.211815\n",
       "1100000   -125.689053\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1010101': 0.7250098367343205,\n",
       " '1010110': 1.9965848868529685,\n",
       " '1011000': 2.0,\n",
       " '1100000': -0.8559887899743589}"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(111.60000000000001, 190.26, 'X[0] <= 8.205\\nmse = 5.779\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(55.800000000000004, 135.9, 'mse = 0.912\\nsamples = 1246\\nvalue = -0.101'),\n",
       " Text(167.4, 135.9, 'X[0] <= 8.242\\nmse = 47.043\\nsamples = 144\\nvalue = 0.873'),\n",
       " Text(111.60000000000001, 81.53999999999999, 'mse = 0.0\\nsamples = 1\\nvalue = 78.212'),\n",
       " Text(223.20000000000002, 81.53999999999999, 'X[0] <= 8.4\\nmse = 5.252\\nsamples = 143\\nvalue = 0.332'),\n",
       " Text(167.4, 27.180000000000007, 'mse = 53.276\\nsamples = 12\\nvalue = 2.199'),\n",
       " Text(279.0, 27.180000000000007, 'mse = 0.504\\nsamples = 131\\nvalue = 0.161')]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOy9eVyU1fv//5rBYFxxIRdAQGHEYZkZZDFwQQNFFCGzEkVETdF6K/VLElPLpXIJzSBNswIpFUuFNE1IRXOlEGQxRTIZEReUXQEZluv7Bx/uH+MMOCAMi+f5eJzHY+77XOec655bLs+cc53r4hERGAwGg6EZ+K2tAIPBYLxIMKPLYDAYGoQZXQaDwdAgzOgyGAyGBmFGl8FgMDQIM7oMBoOhQTq1tgLtgc6dO99/8uRJv9bW40VBIBDklJWV9W9tPRiMloDH/HSfDY/HI/Y9aQ4ejwci4rW2HgxGS8CWFxgMBkODMKPLYDAYGoQZXQaDwdAgzOgyGAyGBmFGt5m4e/cuBg4ciLt37wIA5HI5rKyscO7cOchkMggEAkilUpSVlQEATp48CZFIBDMzMyxZsoTrJzg4GEZGRli4cKFG9E5OToajoyNsbGwgkUhw9OhRJZmMjAyMHj0aFhYWsLKyQmhoKFdXWFgId3d3CIVCjBgxAllZWQCg8MxSqRRTpkzRyPMwGG0eImLlGaXma3o2oaGhNHXqVCIiWrt2Lc2bN4+IiDIzM8nc3JyTq6ysJFNTU8rIyKCqqioaO3YsxcbGcvXh4eG0YMECtcYkIqqoqKDi4mK15esyduxYOnr0KBERpaWlkaGhoZKMTCajtLQ0IiIqLi4moVBIqampRES0bNkyWrNmDRERhYWF0fTp04lI+Zkbw/99363+3llhpSUKm+k2I//73/+QlZWFLVu24Pvvv8cXX3yhUi4hIQHGxsYQCoXg8/nw8/NDVFRUo8f7559/EBgYCKFQiMuXLzdJZx6Ph6KiIgBAUVERBgwYoCRjbGwMKysrAED37t0xdOhQZGdnAwCio6MxZ84cAMCMGTMQExMDIuZex2DUBzsc0Yzw+XyEhITAyckJERER6NWrl0q57OxsDBw4kLs2MjLCgQMH1BqjsLAQkZGR2LVrF3R0dDBr1iykpKSgR48eAIDQ0FCEhYUptdPX18fvv/+udD80NBTu7u4ICgpCSUkJTp482eD4N2/eRGJiIhwdHQHULKsYGBgAAHR0dNC9e3fk5+cDALKysjBs2DAIBAIsX74cHh4eaj0jg9GRYUa3mTl27BgGDBiAtLS0Zu/77t27MDU1xZgxYxAZGYnBgwcryQQEBCAgIEDtPr/55hts2LABM2bMwMmTJ+Hr61uv7sXFxXj99dfx1VdfoWfPng32O2DAAGRlZUFPTw/Xr1/HuHHjYGFhoVJnBuNFgi0vNCPp6enYvXs3EhMTERUVVa/xMjQ0xO3bt7nrrKwsbrbYEP369cPevXuhra0NT09PfPbZZ7h165aCTGhoKLd5VbdMnDhRZZ8RERGYNm0aAMDFxQU5OTkoLi5WkisvL4eXlxdmz56NN998k7uvr6+PO3fucDLFxcXo3bs3dHR0oKenBwAwNzfH6NGjkZSU9MxnZDA6PK29qNweCtTcSHN2dqYDBw4QEVF0dDQ5OTlRdXW1yo20wYMHK2ykHTt2jKtXZyMtJyeHNm/eTBKJhJydnenatWtq6fg0IpGIYmJiiIgoKSlJ5UZaVVUVvf766xQUFKRUFxQUpLCRNm3aNCIievDgAVVUVBAR0f3792nQoEF09epVtXQC20hjpQOXVlegPRR1jG54eDh5eHgo3PPw8KCdO3eq3MmPjY0lc3NzGjx4ML3//vtKfTXGeyExMZFu3Lihtnxdzp8/T7a2tiQWi8nGxobi4uKIiOjOnTvk7u5ORERHjhwhHo9HEomEK1FRUURElJ+fT25ubmRmZkavvPIKyWQyIiI6ePAgWVhYkFgsJrFYTOHh4WrrxIwuKx25sIA3avC8AW9kMhkmTJiA9PR0teR37dqF+Ph47Nixo8ljtmdYwBtGR4at6WoALS0tPH78WOFwRH0EBwdj/fr10NXV1ZB2DAZDk7CZrhqw0I6ahc10GR0ZNtN9weDz+ZxHw/Dhw1XKbNmyRcHzgcfjISUlBWVlZQr3+/fvzx3vLSwsxBtvvAGxWAypVIpz585p8rEYjHYDm+mqQUea6QoEAjx58kRt+b/++gu+vr7IyMhQqnN3d8fMmTPh4+ODpUuXolOnTli3bh2ys7MxefJkJCYmgs9v/P/rbKbL6MiwmW4LIZPJIBQKMX/+fFhYWGD8+PFISkrCq6++isGDB+Pbb78FAJSUlMDT0xNisVghmEx+fj68vb1hb28PqVSK6OjoVnmOvXv3wsfHR+n+w4cPcfHiRbz22msAao4kjxs3DkCNH3Lnzp1x6dIljerKYLQLWtt9oj0UqOmnW5fMzEzi8/l06dIlIiKaPHkyjRkzhp48eUL379+nPn36UHV1NR08eFDBPaygoICIiHx9fenEiRNEVOOWZWpqqjKojYuLi4IrV23Zvn27Sr34fD7Z2dmRnZ0d/fDDDw0+Q2VlJfXr14/+/fdfpbqvv/6afHx8uOvly5fTu+++S9XV1XTt2jXq2rUr57PcWMBcxljpwIUdA25BjIyMYGtrCwCQSqXQ0tKCjo4O+vXrhy5duiAvLw9isRiBgYEIDAzEhAkT4OLiAgCIiYlBamoq15dcLodMJoO1tbXCGCdOnGiUTrdu3YKhoSHu3r2LcePGQSgUYtSoUSplT5w4ARMTE5iZmSnV7dmzB6tWreKuP/roI7z//vuwsbGBmZkZRo4ciU6d2D8vBuNp2F9FC6Kjo8N95vP5SteVlZUwMzNDUlISYmJiEBISgp9//hnfffcdqqqqcO7cOXTr1q3BMVxdXZGbm6t0f+HChSpj8hoaGgKoOb47ZcoU/PXXX/Ua3b1792LmzJlK92/evInMzExuOQEAunXrhu+//567tre3h7m5eYO6MxgvIszotjJ37txB79694e3tDXNzc8ydOxdAzSbVli1b8PHHHwMAEhMTuVlzXRoz0y0oKEDnzp0hEAjw6NEjxMbG4rPPPlMpW1ZWht9++w2bNm1Sqtu7dy+mTZsGLS0t7l5hYSG6dOkCbW1tHD58GLq6uhg6dKjaujEYLwrM6LYyqampCAoKAp/PB4/Hw4YNGwDUBK5ZvHgxrK2tUV1dDWNjY5WhGRtDeno6/P39wefzUVVVBV9fX7i5uQEAd/qtdnZ8+PBhODo64uWXX1bqZ+/evYiIiFC4d+3aNcyaNQtaWloYNGiQUj2DwaiBuYypQUdyGWsPMJcxRkeGuYwxGAyGBmFGl8FgMDQIM7rtBIFAoPEx79y5A2dnZ3Tt2lXJE+Ldd9+FRCKBRCKBm5sblwW5oqIC8+fPh7W1NaysrBRyv2VlZWHEiBEQCoVwd3fncrMxGC8SzOgy6qVbt25Yt24dNm/erFS3YcMGpKSkICUlBZMmTcInn3wCAPjuu+9QWlqK1NRUnD9/HmvWrMGjR48AAEFBQViwYAH+/fdfODg4YOPGjRp9HgajLcCMbhOo7+hueHg4HBwcIJVK4ebmhgcPHgAAVq9eDT8/P4wZMwbGxsbYtm0btm3bBjs7O4jFYvz333+c3MyZM+Hk5AShUIh169apHH///v0YPnw4bGxs8MYbb3DpdVasWAELCwuIxWKV/rWNRVdXFyNGjFA5y65NhAkAjx49Ao9Xs+/1zz//wNXVFTweD7q6urC0tMSxY8dARIiNjYW3tzcAYO7cuU3KgMxgtHta+0hceyh46hhwfUd3c3NzuXvbtm2jJUuWEBHRqlWryMHBgTsC3L17dwoJCSEios2bN9OiRYs4OZFIRI8fP6ZHjx6RSCSixMREIiLS0dEhIqL09HSaMGEClZeXExHRunXr6OOPP6a8vDwSiURUVVWloFNdysvLVR4ZlkgkdOjQISX5WurLZLF48WLS19cnCwsLysnJISKinTt3kqenJ5WXl9Pdu3fJwMCANm3aRA8fPiRjY2OubWVlJfXo0UPleGDHgFnpwIX56TaB+o7uXrt2DStWrEB+fj7Ky8sVMt9OnDiROwLcs2dPeHl5Aag5HhwXF8fJvfbaa+jatSv3+ezZsxg2bBhXf/z4caSkpMDBwQFAzfFge3t76OrqQiAQYO7cuZgwYQI8PT2V9NbW1kZycnKzfQ+hoaEICQnBmjVrsG3bNqxZswZz587F9evX4eDggP79+8PZ2ZkdB2Yw6sCWF5pA7dFdOzs7hISEwN/fHwDg6+uLTZs2IS0tDdu2bVMIoVjfkeDa48DqQkSYMWMGkpOTkZycjKtXryIiIgJaWlqIj4/H9OnTceHCBdjb2yv1K5fLVWYKlkqlOHz4cJO+Cx6PB19fXxw8eBBATZaMTZs2ITk5GTExMZDL5TA3N0efPn1QXFwMuVwOAMjOzsaAAQOaNCaD0Z5hU5AmUN/R3eLiYhgYGICIEB4e3qS+Dx06hJUrV4KIcOjQIfz0008K9a6urpg0aRI++OAD6Ovro6SkBNnZ2dDX10dpaSnc3NwwZswYGBkZ4fHjx+jZsyfXtjlnuhkZGRgyZAgAIDo6GiKRCABQWlqK6upqdOvWDQkJCbh+/TrGjRsHHo+H8ePHY9++fZg1axbCwsK4sJAMxosEM7pNoL6juxs2bICTkxP09PTg6urKuVE1hmHDhsHV1RUPHz7EnDlzFJYWAEAkEiE4OBgeHh7cTHbt2rXo2rUrpk6dirKyMlRXVyMwMFDB4DaF8vJymJqaorS0FHK5HEeOHMH+/fvh6OgIf39/5OXlgcfjYfDgwfjmm28AAA8ePICbmxu0tLTQu3dv7Nu3j4vRsHHjRnh7e+PTTz+Fqakp9u3b91z6MRjtEXYMWA00dQx49erVEAgEWLZsWYuP1ZZhx4AZHRm2pstgMBgahM101YAFvNEsbKbL6MiwmS6DwWBoEGZ02xizZ89uMxtMJ0+ehEgkgpmZGZYsWVKv3JIlS2BmZgaRSISTJ09qUEMGo/3BjC5DJVVVVViwYAEOHz6MjIwMXL58GX/88YeSXGxsLFJSUpCRkYFff/0V/v7+qKqqagWNGYz2ATO6LcjHH3+skO7mu+++wzvvvAMAWLx4Mezt7WFlZYV3330XqtaMTUxMcP/+fQA1Kd3rpr/5+uuv4eDgAIlEAn9//0YdsFCHhIQEGBsbQygUgs/nw8/PT2WshOjoaPj5+YHP58Pc3BxGRkZISEhoVl0YjI4EM7otyPTp0xEZGcldR0ZGYsaMGQBq3MMSEhKQlpaG/Px8HD16VO1+4+LikJiYiPj4eKSkpIDP5+PHH39UkgsNDVV5+mzixInPHCM7OxsDBw7kro2MjHDnzp0myzEYjBrY4YgWxMLCAlVVVcjIyED37t2RmZmJkSNHAgCioqKwY8cOVFRUIDc3F1KpFB4eHmr1+/vvv+P06dPcwYmysjLo6ekpyQUEBCAgIKD5HojBYDw3zOi2MLWzXV1dXbz11lvg8XjIzMzEunXrkJCQAD09PaxYsUIhTkMtnTp1QnV1NQAo1BMRAgMDsWjRogbHDg0NRVhYmNJ9fX19pSSXly5dwrx58wAAH3zwAYYMGYLbt29z9VlZWTAwMFDqy9DQUC05BoPxf7R2mLP2UPBUaMfGIJPJaOjQoeTg4EDJyclERJSSkkIWFhZUWVlJhYWFNGTIEFq1ahUREfn5+VFkZCQREbm6utLhw4eJiOjTTz8lc3NzIiI6fvw42djYUGFhIRER5eXlUWZmZpN1VEVlZSUNHjyYMjIyqKqqisaOHUvHjh1Tkjt27Bi5urpSVVUVpaen06BBg6iysvK5xgYL7chKBy5sTbeFMTY2Ru/evVFSUgKJRAKgJjSko6Mjhg4dismTJ8PJyUll2zVr1iAwMBB2dnYKM11XV1fMnz8fo0aNglgsxrhx45oU56EhtLS0sH37dkyePBlCoRASiQQTJkwAUJOuvTZlu5ubGywtLSEUCuHl5YVvv/2Wi7XAYDCUYSfS1ICdSNMs7EQaoyPDZroMBoOhQZjRZTAYDA3CjC6DwWBoEGZ0GQwGQ4MwP101EAgEOTwer19r6/GiIBAIclpbBwajpWDeCy0Ij8dbDmAgAG0AQwBMIqLi1tWqYTp37nz/yZMn7D+YZkQgEOSUlZX1b209GG0DZnRbEB6PlwbgLoCXAAQA6ENEf7auVg3D3OOaH+YCx6gLW9NtIXg8nhSACIAZgD4ATgIY26pKMRiMVoet6bYc8wCUAzgKYD+AC0TEAs0yGC84bHmBoQBbXmh+2PICoy5seYHBYDA0SLMY3c6dO9/n8XjEimZK586d7zfHe1OXu3fvYuDAgVxQHblcDisrK5w7dw4ymQwCgQBSqRRlZWUA6s+tFhwcDCMjIyxcuFAjeicnJ8PR0RE2NjaQSCQqA8VnZGRg9OjRsLCwgJWVFUJDQ5Vk9u/fDx6Ph/j4eADAhQsX4ODgACsrK4jFYvz8888t/iyMDkRzhCrDc4Q+ZDQetGDow/reZWhoKE2dOpWIiNauXUvz5s0jIqLMzEwu5CRRTUhIU1NThZCQsbGxXH14eDgtWLBA7WetqKig4uJiteXrMnbsWDp69CgREaWlpZGhoaGSjEwmo7S0NCIiKi4uJqFQSKmpqVx9YWEhjRw5koYPH04XL14kIqIrV65woTTv3LlD/fr1o7y8vHr1aMn3xUr7K2x5gaEW//vf/5CVlYUtW7bg+++/xxdffKFSTt3cas/in3/+QWBgIIRCIS5fvtwknXk8HoqKigAARUVFGDBggJKMsbExrKysAADdu3fH0KFDkZ2dzdUHBQVh5cqVEAgE3D1LS0uYmJgAqAkI37dvX+TksPMcDPVg3gsMteDz+QgJCYGTkxMiIiLQq1cvlXKqcqYdOHBArTEKCwsRGRmJXbt2QUdHB7NmzUJKSgp69OgBoHGZMGrl3d3dERQUhJKSkmemh7958yYSExPh6OgIADh//jyKiorg5uaG9evXq2xz4cIFlJWVYciQIWo9I4PBjC5DbY4dO4YBAwYgLS2t2fu+e/cuTE1NMWbMGERGRmLw4MFKMo3N+fbNN99gw4YNmDFjBk6ePAlfX996dS8uLsbrr7+Or776Cj179kRFRQUCAwNx8ODBevvPzs7GrFmzEBERwQK3M9SGLS88xZ49eyAUCmFmZobNmzerlLl9+zbGjx8PsViMV155BdeuXePqpk+fjr59+yqkSweAtWvXQiQSQSKRwNXVFbdu3WrR52hu0tPTsXv3biQmJiIqKqpe49XUnGn9+vXD3r17oa2tDU9PT3z22WdK31FjsxtHRERg2rRpAAAXFxfk5OSguFj5FHZ5eTm8vLwwe/ZsvPnmmwCAe/fu4caNG3BycoKJiQni4+MxZcoU/PlnzYHCgoICTJw4ERs3bsSIESOe+XwMBkdzLAyjg2ykFRQUkImJCT148IBKS0tJJBJRenq6ktybb75J3377LRERXb58mcaOHcvVnTp1ihITExU2l4hq8po9efKEiIi++eYbblOqKaAVNtKcnZ3pwIEDREQUHR1NTk5OVF1drXIjraHcaupspOXk5NDmzZtJIpGQs7MzXbt2rTFfD4dIJKKYmBgiIkpKSlK5kVZVVUWvv/46BQUFNdiXs7Mzt5FWUlJCTk5OtH37drX0aMn3xUr7K83TSROMbmZmJpmZmdG8efNIJBLRuHHjKDExkcaOHUuDBg2iHTt2EBHR48ePafLkyWRtbU2WlpYUEhJCRDXJGKdNm0Z2dnYkkUgoKiqq0To8TWRkJM2dO5e7Xr16Na1fv15JzsLCgm7evMld6+vrU05OjsKzPW1065KYmEgODg5N1lPTRjc8PJw8PDwU7nl4eNDOnTtVPmtsbCyZm5vT4MGD6f3331fqqzHeC4mJiXTjxg215ety/vx5srW1JbFYTDY2NhQXF0dENR4H7u7uRER05MgR4vF4JJFIuKLq31Jdo7t161bS1tZWaBMfH1+vHszoslK3NE8nTTS6fD6fLl26REREkydPpjFjxtCTJ0/o/v371KdPH6qurqaDBw8q/JEWFBQQEZGvry+dOHGCiIjy8/PJ1NRUpWuRi4uLwh9HbVE1SwkODuay8hIRhYWF0aJFi5TkZsyYQRs2bCAiotOnTxOPx+Oeo/bZGjK6CxYsUBinsbTGTLc+nvWsT9NYo9sRYEaXlbqlVTfSjIyMYGtrCwCQSqXQ0tKCjo4O+vXrhy5duiAvLw9isRiBgYEIDAzEhAkT4OLiAgCIiYlBamoq15dcLodMJoO1tbXCGCdOnGh2vTdv3oyAgABIpVLY2trCxsYGnTqp91WGh4cjKSkJZ86caXa9WgMtLS08fvwYUqkUFy9eROfOneuVDQ4Oxvfff4/XXntNgxoyGG2LVjW6Ojo63Gc+n690XVlZCTMzMyQlJSEmJgYhISH4+eef8d1336Gqqgrnzp1Dt27dGhzD1dUVubm5SvcXLlyodDLK0NAQx48f567r2wTq378/fvnlFwBAVVUVTExMVO62P83Ro0cRHByMP//8U8Hvsz0zcOBABb/Whvjwww/x4YcftrBGDEYbpzmmy2ji8kLdn6WrVq1SWD81Njame/fuUXZ2NpWWlhJRzWaIVColIiIfHx9au3YtJ1/3531Tyc/PJ2NjY4WNNFWbOA8fPqSqqioiIgoJCaG33367wWcjqllfNDU1VVgLbipoQ8sLbYGKigqSSCTk5ubG3ZsxYwa3lGRqako9e/ZU2TY5OZmT8fHxIblcrlAfHx9PfD6fIiMjiahm42348OEkkUjIwsKCFixYQBUVFQ3q15Lvi5X2V9q8y1hqaiqGDx8OqVSKuXPnYsOGDQBq3IfS09NhbW0NS0tLfPzxx889Vq9evfDpp5/C0dERVlZWmDNnDuf69cknn+Dw4cMAgD///BPm5uYwNzfHhQsX8OWXX3J9eHp6wtHREf/99x8MDQ25uvfeew8lJSWYMmUKpFIp3NzcnltfRg2bN2/mTpXVsmfPHiQnJyM5ORnz58/HG2+8obLtwoULERoaihs3bkBLSwvh4eFcXWVlJYKCghTeFZ/Pxx9//IHk5GRcuXIFDx8+xL59+1rmwRgdk+aw3GiHs6P2DFphptsWvU2IiP777z9ydXWluLg4hZluXcRiMZ0+fVrp/r1798jU1JS7Pn36tEIf69atox07dpCfnx83061LeXk5TZw4kXbv3t2gji35vlhpf6V5OmFGV6O0ltFta94mREQTJ06k1NRUOnXqlEqjm5aWRgMHDqTq6mqluoSEBHJ2duaub968SVZWVkREdOPGDRo7dixVV1erNLqOjo6kq6tL3t7eVFlZqVK3WpjRZaVuYceAGWrT1rxN9uzZA5FIBGtra5w+fbpemRkzZoDHa1wM8UWLFuHLL7+st92FCxdQWloKb29vxMXFYdy4cY3qn/HiwowuQ23amrfJ+fPn8dtvv+HAgQN48uQJioqKMGXKFERHRwOo+RUXGRmJI0eOqBzL0NBQwfOirrfK33//zbm25ebm4ujRo6iqqoKPjw8n36VLF3h5eeHQoUPM6DLUpzmmy2gDyws6OjoaHzM7O5tGjx5NXbp0UfhJXVVVRV5eXjRkyBCysrKiOXPmUHl5uULbzMxM6tq1q4LHhlwup3fffZeEQiGZm5vTtm3bVI6LVlpeaGveJnVRtbxw9uxZEovFDbYbPnw4/fnnn0RENGvWLJXLGHWXF3Jzcyk3N5eIat7XlClTaOvWrQ2O0ZLvi5X2V9q890Jbplu3bli3bp3KwDj+/v64fv06UlNTUVZWhp07dyrUv/fee0qBWtavX4+XXnoJGRkZSE9Pr3fHvS2jSW+TZ7Fnzx7MnDlT6b5UKuU+b9++HYsXL4aZmRkqKiowd+7cBvt8+PAhxo0bB7FYDKlUCiMjIyxYsKDZdWd0XJolMeXTyQxLSkowffp0yGQyVFdXw9/fHwEBAQgPD8f27dshl8vRr18//PTTT+jbty9Wr16NzMxM3Lp1C5mZmVi6dCmAmtNbcrkc0dHRMDU1xerVq3Hjxg3cvHkTDx8+xJw5c7B8+XIAgEAgwJMnTwDUpFfZtGkT5HI5TE1NERYWhh49emDFihWIjo5Gp06dIBaLsXv37ud+dgDYtWsX4uPjsWPHDpX1X375JXJycrBx40YAQGRkJFJTU6GjowOBQIBly5YBAAwMDHD16lXo6uo2OF5LJjpkiSmbH5aYklGXFpnpxsbGQl9fH6mpqbhy5QpmzZoFoMaH9e+//0ZycjK8vLwUsg+kp6cjNjYWf//9Nz766CNUVVXh0qVLmD17Nr766itOLikpCcePH8fly5exe/duJCUlKYx9/fp1hIWF4ezZs7h8+TJsbW2xadMm5OfnIzo6GleuXEFqaiq2bt2qpLdcLlcZOlAqlXI+uo1FLpcjIiIC7u7uAGpCAoaEhCjN9AoLC0FEWLduHWxtbeHh4YGbN282aUwGg9F2aZGNtPp2sK9du4YVK1YgPz8f5eXlCkdnJ06cyO2E9+zZE15eXgBqfgrGxcVxcq+99hq6du3KfT579iyGDRvG1R8/fhwpKSlwcHAAUGP07O3toaurC4FAgLlz52LChAnw9PRU0ltbWxvJycnN+l0sWLAAo0ePxpgxYwAAS5cuxcqVK9GlSxcFucrKSty7dw+WlpbYuHEjfv75Z8yePbvDxGhgMBg1tIjRrW8H29fXF7/88gvs7e1x/PhxfP7551yb+nbGa3fF1YWIMGPGDGzatEmpLj4+HqdOncLRo0fx6aefIiUlRSFQjVwu54z106xdu1aloW6Ijz76CEVFRfjhhx+4e3///TeOHz+ORYsWobCwEDweDzweD0uXLkXnzp25INpvvPEG5s2b16jxGAxG26dFlhfu3LkDHR0deHt7Y+3atbh06RKAmpQoBgYGICKF45aN4dChQygtLUVJSQkOHTqEUaNGKdS7uroiKiqKSxdeUlKC69ev49GjRygoKICbmxuCg4ORm5uLx48fK7StnemqKo01uF999RUuXryIvXv3gs///7/mlFIT6k0AACAASURBVJQUyGQyyGQyvP/++wgKCkJQUBB4PB6mTJnC5fE6deoURCJRU76idk9rBAO6c+cOnJ2d0bVr13pTxAcHB4PH4+H+/fsK94uKimBgYKCx1PKM9k2LzHRTU1MRFBQEPp8PHo/H7WBv2LABTk5O0NPTg6urK2cYG8OwYcPg6urKbaTVXVoAAJFIhODgYHh4eHAz5LVr16Jr166YOnUqysrKUF1djcDAQPTs2fO5nrO8vBympqYoLS2FXC7HkSNHsH//flhZWeGDDz6AqakpXnnlFQDApEmTFGb2qti4cSN8fX2xfPlydO/eXWGGzGhZaj1R0tLSVC4xZWZm4uTJkzAyMlKq++ijjzB27FhNqMnoCDSH3xk05Kf7tG/oiwo06KdbXyyFsLAwsre3J4lEQuPHj+cyZ6xatYpmzZpFzs7OZGRkRFu3bqWtW7eSra0tWVtbc1kgVq1aRT4+PuTo6EhmZmb0+eefc2PW9bn+5ZdfyMHBgaRSKU2dOpWKioqIiGj58uUkEonI2tqafHx8mueLpfqDrHt4eFBaWhrnj1zL+fPnacaMGQ0GZ2/J98VK+yvMT5fRIMwTBdi7dy8sLCyUIplVVFTgww8/rDeBKYOhinZ1DHj16tWtrcILx4vuiVJQUICvvvoKp06dUqr74osvMH36dPTv3/+5x2G8OLTZme7s2bPbTJzSkydPQiQSwczMDEuWLFEpk5ycjFdeeQU6OjrcGnYtKSkpkEqlMDMzw8yZM1FRUQEA+OOPP7hUP23lWZ+m1hPFzs4OISEh8Pf3BwD4+vpi06ZNSEtLw7Zt27iDKUDze6LUbmZevXoVERER0NLSQnx8PKZPn44LFy7A3t5eqd/mmun+888/uHXrFiwtLWFiYoLs7GzY29vjv//+w8WLFxEcHAwTExMEBgZiz549WLx4sdp9M15M2tVMtzWoqqrCggULcOzYMZiamsLV1RV//PEHxo8fryDXt29fbN26lQu2UpfaQNmjR4+Gn58fwsPD4e/vD1NTU/z4448IDg7W1OM0mjt37qB3797w9vaGubk5d0y2uTxRVq5cCSLCoUOH8NNPPynUu7q6YtKkSfjggw+gr6+PkpISZGdnQ19fH6WlpXBzc8OYMWNgZGSEx48fK2yMNtdMd+TIkcjJyeGuTUxMEB8fj/79+ysE0qk9lfj1118/95iMjo1GZroff/yxgt/sd999h3feeQcAsHjxYtjb28PKygrvvvsuiJSPoJqYmHBuOjKZjMvmAABff/01HBwcIJFI4O/v36iZlDokJCTA2NgYQqEQfD4ffn5+iIqKUpLT19eHnZ0dXnrpJYX79+/fx8OHDzF69GgAwNy5c7n2pqamsLa2VnApa2vUF0uh1hPF3t5e5Y6+OtR6okilUvj4+DToiSIWi+Ho6Ihr166hqKgInp6eEIvFsLW1bTZPFENDQ3zwwQfYvXs3DA0NcfHixefqk8FQSXPsxuEZ3gv//PMPDRs2jLseO3YsnTlzhoiIi9hUXV1N06ZNo99++42IFCM71d0xrhvt6uTJk+Tn58flK1uwYAH98MMPSuOHhISoDIzt7u7eoN5ERPv37yc/Pz/uOi4ujjw8POqVf9rDoqFA2bXUl5mgPtABcqS9SJ4oLfm+WGl/RSPLCxYWFqiqqkJGRga6d++OzMxMjBw5EgAQFRWFHTt2oKKiArm5uZBKpfDw8FCr399//x2nT5/mZkhlZWXQ09NTkgsICEBAQEDzPRCDwWA0EY2t6U6fPh2RkZHQ1dXFW2+9BR6Ph8zMTKxbtw4JCQnQ09PDihUrFDZkOCU7dUJ1dTUAKNQTEQIDA7Fo0aIGxw4NDUVYWJjSfX19ffz+++8K9y5dusQdv/3ggw8wZMgQ3L59m6uvLy17fTQUKPtFhnmiMF5UNLaY6O3tjX379iEyMhIzZswAADx69AhdunRBr169UFRUhAMHDqhsO2jQICQmJgKAgoy7uzvCwsJQVFQEAMjPz4dMJlNqHxAQoPJo79MGFwDs7Oy4+lmzZsHe3h4ymQz//vsvqqurERERwWUUUIf+/ftDT0+PC1wTFhbWqPbtmdb0QMnKysKIESMgFArh7u7O/Rt5mi1btsDS0hJisRju7u5c1gofHx/O28HMzAy9evUCANy7dw92dnaQSqWwsLDAqlWrNPZMjA5Cc6xRQM11QCcnJ7K0tFS49/bbb5OZmRmNGjWKZs+eTatWrSIixXXO8+fP05AhQ8jW1pZWrFihkMHgm2++IWtra7K2tqZhw4bR+fPn1dKlMcTGxpK5uTkNHjyY3n//fe7+9u3buUwDN27cIAMDA+revTv16NGDDAwM6Pbt20RUk0FBLBaTqakpTZ8+ncsicerUKTIwMKAuXbpQ7969ycDAQC190E7WdBu7Vt2ceHt7U0REBBERffLJJ/TRRx8pydy6dYtMTEyopKSEiIiWLFlCK1euVJLbsGEDzZs3j4hqskXUZsaQy+Xk4OBAZ8+ebVCXlnxfrLS/0jydtIF0PS8SrWF0V65cScHBwdz1zp07aeHChUREtGjRIrKzsyNLS0t65513uMy76myGEhGFhoaSvb09icVimj9/PlVUVDzX91NdXU29evXi/nOTyWQK49Uik8nIwMCAcnNzqbq6mvz9/VWmSKovhfvjx49JKpXSuXPnGtSHGV1W6pa266vEaFPUrsnXUneZaPXq1UhISEBaWhry8/Nx9OhRtfuNi4tDYmIi4uPjkZKSAj6fjx9//FFJLjQ0VOVBh6dTHgFAXl4eevToAW1tbQA16+r37t1TkjM2NkZgYCCMjIzQv39/3LhxQylS2JUrV1BQUMC5/AE1AeclEgn69u0LV1dXjBgxQu3nZTDY4QiGWnRED5S8vDwcOHAA//33H15++WXMmzcPX375JQIDAzkZVSnce/bsiZSUFOTn58PLywtXrlxRisvAYNQHM7oMtWkvHih9+vRBcXEx5HI5tLW1kZ2djQEDBii1jYuLg6mpKRc74c0338T333+voFtDKdx79+6NMWPGICYmhhldhtqw5QWG2rQXDxQej4fx48dznhP1eYwYGxsjPj4ejx49AlATYKdu4Pjz589DV1dXwaDevXsXJSUlAGoC5D/dhsF4FszoMtTG2NgYvXv3RklJCSQSCQBwx3OHDh2KyZMnw8nJSWXbNWvWIDAwEHZ2dgozXVdXV8yfPx+jRo2CWCzGuHHjmhTc/mk2btyI7du3QygU4q+//uIyLtf1w3ZwcMDMmTNhb28Pa2tr3Lp1Cx9++CHXh6oU7v/++y8cHR0hkUgwfPhweHp6YtKkSc+tL+PFoVlSsHfu3Pn+kydP+jWDPgw1EAgEOWVlZS0ST5ClYG9+WAp2Rl2axegyOg7M6DY/zOgy6sKWFxgMBkODMKPLYDAYGoQZXQaDwdAgzE+XoYBAIMjh8XhsU7QZEQgEOc+WYrwosI00RpPh8XibAZQCKALwDoBXiehW62rFYLRtmNFlNAkej8cHkAXgAICJAF4FcKetuz4w98bmpyVdGDsizOgymgSPx3MG8DNqZrqHAYwHcJiIlrWqYs+AucQ1P8wlrnGwNV1GU1kLoB+ATADlAOYASGhVjRiMdgCb6TKaBI/H8wCQD+Bie5o6splu88Nmuo2DGV3GCwUzus0PM7qNg/npMhgMhgZhRrcRdO7c+T6PxyNWmr907tz5fmu/X1XcvXsXAwcO5CKfyeVyWFlZ4dy5c5DJZBAIBJBKpSgrKwMAnDx5EiKRCGZmZliyZAnXT3BwMIyMjJQyU7QUycnJcHR0hI2NDSQSyTOzeXh4eGDo0KEa0e2Fp7XzBbWnApYLrsWAhvKINeUdhoaG0tSpU4mIaO3atVySyqdzvVVWVpKpqSllZGRQVVUVjR07lmJjY7n68PBwWrBggdrjVlRUUHFxcaP1JSIaO3YsHT16lIiI0tLSyNDQsF7Zffv2kY+Pj8o8cuqgqXfXUQqb6TIYz+B///sfsrKysGXLFnz//ff44osvVMolJCTA2NgYQqEQfD4ffn5+iIqKavR4//zzDwIDAyEUCnH58uUm6czj8bjA8EVFRSozZwA1+d5CQ0OxYsWKJo3DaDzMZYzBeAZ8Ph8hISFwcnJCREQEevXqpVIuOzsbAwcO5K6NjIzqzaTxNIWFhYiMjMSuXbugo6ODWbNmISUlBT169ADQuHRFtfLu7u4ICgpCSUkJTp48qXLcpUuXYuXKlejcubNaejKeHzbTZQCoyZIgFAphZmaGzZs3q5SpqKjAzJkzYWZmBqlUipSUFA1r2XocO3YMAwYMQFpaWrP3fffuXQwYMACHDx9GZGQkzpw5g3nz5nEGF2hcuiIA+Oabb7BhwwZkZWXhl19+ga+vr5LMuXPnUFhYCHd392Z/Jkb9MKPLQGFhIVauXIkLFy4gLS0NP/zwA65fv64kFxYWBm1tbdy4cQNbtmzBO++80wraap709HTs3r0biYmJiIqKqtfwGhoa4vbt29x1VlYWDAwMntl/v379sHfvXmhra8PT0xOfffYZbt1SDGHRmBT0ABAREYFp06YBAFxcXJCTk4Pi4mIFmXPnzuH8+fMwMTHByJEj8d9//8He3v6Z+jKek9ZeVG5PBc2wkZaZmUlmZmY0b948EolENG7cOEpMTKSxY8fSoEGDaMeOHURE9PjxY5o8eTJZW1uTpaUlhYSEEBFRXl4eTZs2jezs7EgikVBUVNRz6xQZGUlz587lrlevXk3r169XknNzc6MzZ85w14MGDaJ79+499/hE1KY30pydnenAgQNERBQdHU1OTk5UXV2tciNt8ODBChtpx44d4+rV2UjLycmhzZs3k0QiIWdnZ7p27Vqj9SUiEolEFBMTQ0RESUlJDW6kESlvCjYGTb27jlJaXYH2VJrL6PL5fLp06RIREU2ePJnGjBlDT548ofv371OfPn2ourqaDh48qPAHWlBQQEREvr6+dOLECSIiys/PJ1NTU5U73C4uLiSRSJTK9u3blWSDg4Np1apV3HVYWBgtWrRISc7S0pIyMzO569GjR3PP8by0VaMbHh5OHh4eCvc8PDxo586dKg1VbGwsmZub0+DBg+n9999X6qsx3guJiYl048aNRulby/nz58nW1pbEYjHZ2NhQXFwcERHduXOH3N3dleSZ0dVcYRtprYCRkRFsbW0BAFKpFFpaWtDR0UG/fv3QpUsX5OXlQSwWIzAwEIGBgZgwYQJcXFwAADExMUhNTeX6ksvlkMlksLa2VhjjxIkTmnugDszs2bMxe/ZshXu//fYbAKhMFT9+/Hikp6c3y9jDhg1rclsnJydcunRJ6X59G28mJibNpjejYdiabiugo6PDfebz+UrXlZWVMDMzQ1JSEuzs7BASEgJ/f38AQFVVFc6dO8dtpGRlZSkZXKAmtbmqNcAdO3Yoyaq7Fvm03O3bt9Vas+yoaGlp4fHjxwqHI+ojODgY69evh66uroa0Y7RZWnuq3Z4Kmml5oe7PuFWrVimsnxobG9O9e/coOzubSktLiahmTU4qlRIRkY+PD61du5aTb46f9/n5+WRsbEwPHjyg0tJSEolEKtcSt2/fTnPmzCEiori4OBo+fPhzj10L2ujyQluCx+Nxy0QODg4qZXbs2EGWlpZkbW1Njo6OlJyczNUZGxuTpaUl10dubi4REa1Zs4aGDh1KYrGYXFxcSCaTNUovTb27jlLY8kIbJTU1FUFBQeDz+eDxeNiwYQOAml3sxYsXw9raGtXV1TA2Nq7XbUhdevXqhU8//RSOjo4gIixcuJA7EvrJJ5/Azs4Onp6emDt3Ls6cOQMzMzN07doVu3btet7HZDQCbW1tJCcnNygjEolw4cIF9OjRA8eOHcPbb7+tsMxw4sQJ9O+vGG/cyckJQUFB0NHRwfbt27FkyRK1/YsZTaC1rX57KmjHs6S2DtrQTLctepgQEeno6DRKPi8vj/r27ctd1/6KaojExMR6Z9H1oal311FKqyvQngozui1HWzO6bc3DhIiIz+eTnZ0d2dnZ0Q8//PDM51i/fj35+flx1yYmJmRjY0NSqVSlSyAR0YIFCxQ8WdSBGd3GFba8wGCooC16mNy6dQuGhoa4e/cuxo0bB6FQiFGjRqmUPX78OMLDw3Hu3Dnu3tmzZ2FoaIjCwkJMmTIFhoaGmDlzJlcfHh6OpKQknDlzplF6MRoHM7oMhgoa42ESExODkJAQ/Pzzz/juu+84D5Nu3bo1OIarqytyc3OV7i9cuFBlCEhDQ0MANW5fU6ZMwV9//aXS6F66dAkLFixAbGwsXn75ZaX2PXv2hI+PD+Lj4zmje/ToUQQHB+PPP/+EQCBoUG/G88FcxjoYrfEHc+fOHTg7O6Nr164aixfbFrhz5w50dHTg7e2NtWvXchtW7u7u2LJlCyeXmJiosv2JEydUxlNQ9R0WFBTgyZMnAIBHjx4hNjZWpavgv//+i2nTpuGXX36BUCjk7peUlHDHgOVyOX777Teu/YULF/Dee+/h6NGjCkaa0TKwmS7juenWrRvWrVuHtLS0Z+6udyQ06WGSnp4Of39/8Pl8VFVVwdfXF25ubgDA+V4vXLgQy5cvR2FhIebNm8e1TUxMRE5ODl5//XVUV1ejqqoKEyZM4GTee+89lJSUYMqUKQBqYkHExsY+l76M+mE50hpBY/NrlZSUYPr06ZDJZKiuroa/vz8CAgIQHh6O7du3Qy6Xo1+/fvjpp5/Qt29frF69GpmZmbh16xYyMzOxdOlSADVrbXK5HNHR0TA1NcXq1atx48YN3Lx5Ew8fPsScOXOwfPlyADUz3doZ0f79+7Fp0ybI5XKYmpoiLCwMPXr0wIoVKxAdHY1OnTpBLBZj9+7dzfL97Nq1C/Hx8SoPYDwLTeXZYjnSmh+WI61xsOWFFiQ2Nhb6+vpITU3FlStXMGvWLACAp6cn/v77byQnJ8PLy0shKHZ6ejpiY2Px999/46OPPkJVVRUuXbqE2bNn46uvvuLkkpKScPz4cVy+fBm7d+9GUlKSwtjXr19HWFgYzp49i8uXL8PW1habNm1Cfn4+oqOjceXKFaSmpmLr1q1KesvlcpWn2aRSKQ4fPtxC3xaD8WLAlhdakPp2t69du4YVK1YgPz8f5eXlGDx4MNdm4sSJ3C55z5494eXlBaBmBz0uLo6Te+2119C1a1fu89mzZxXO6h8/fhwpKSlwcHAAUGNI7e3toaurC4FAgLlz52LChAnw9PRU0lsdJ3wGg9E02Ey3BakvfoKvry82bdqEtLQ0bNu2jVsOAOrfNa/dMVcXIsKMGTO4zZmrV68iIiICWlpaiI+Px/Tp03HhwgXY29sr9ctmugxGy8GMbgtS3+52cXExDAwMQEQIDw9vUt+HDh1CaWkpSkpKcOjQISXXIVdXV0RFRXFZbEtKSnD9+nU8evQIBQUFcHNzQ3BwMHJzc/H48WOFtrUzXVVF1cyY0TjaqodJcHAweDwe7t+vScycnJwMGxsbSKVSWFlZNWmtnqEMW15oQerb3d6wYQOcnJygp6cHV1dXzjA2hmHDhsHV1ZXbSHs6DKBIJEJwcDA8PDy4mezatWvRtWtXTJ06FWVlZaiurkZgYCB69uz5XM9ZXl4OU1NTlJaWQi6X48iRI9i/fz8cHR2fq19G8/EsD5PMzEycPHkSRkZG3D1zc3P8/fffeOmll/Do0SNYW1tj0qRJCnngGE2gtY/EtaeCNnIM+OnIZB0BtNIx4PriJ4SFhZG9vT1JJBIaP3485eTkEFHNdz9r1ixydnYmIyMj2rp1K23dupVsbW3J2tqaCzq+atUq8vHxIUdHRzIzM6PPP/+cG7NuDIVffvmFHBwcSCqV0tSpU6moqIiIiJYvX04ikYisra3Jx8enGb7hGuoLpO7h4UFpaWn1xmd48OABGRoaUlZWllKdpt5dRylseYHxQsM8TIC9e/fCwsICVlZWSnVXr16FtbU1jIyMsHTpUjbLbQbY8kI7ZPXq1a2tQofhRfcwKSgowFdffYVTp06prLewsEBaWhpu376N1157DW+99Rb69ev33OO+yLCZLuOF5kX3MPnnn39w69YtWFpawsTEBNnZ2bC3t8d///2nIDdw4EBYWFjg7NmzavfNUA0zuhpm9uzZ2LdvX6uMPXbsWO4P08jICDY2Nlzd0qVLYWlpCUtLS8yYMUPByNSydu1aiEQiSCQSuLq6cmnCy8vLMX78ePTs2RMTJkxQaPPOO+/A3NwcYrEYr7/+OgoKClr2IRvJi+5hMnLkSOTk5EAmk0Emk8HQ0BAJCQkwNTWFTCaDXC4HAOTm5uLChQtccHtG02FG9wXi1KlT3B/mpEmT8MYbbwAAzpw5g7Nnz3LrmhUVFdizZ49SeycnJyQnJyMlJQVTp07FkiVLANTkClu2bJnK48Senp64evUqUlNTIRQK8fnnn7fsQzaS1NRUDB8+HFKpFHPnzlXyMLG3t1fY0W8MtR4mUqkUPj4+DXqYiMViODo64tq1aygqKoKnpyfEYjFsbW2bzcPE0NAQH3zwAXbv3g1DQ0NcvHixwTZ//fUXhg0bBolEAhcXFyxfvlzlui+jkbT2Tl57Knhq53vlypUUHBzMXe/cuZMWLlxIRESLFi0iOzs7srS0pHfeeYeqq6uJiMjPz48iIyOJSDGS/9O500JDQ8ne3p7EYjHNnz+fKioqqLmQy+Wkp6fHpVM/c+YMSaVSKikpIblcTu7u7nT06NEG+1CVYeDUqVPk5uZWb5uDBw/SW2+9pbIObSiIeXPQET1M6kNT766jFDbTfQ6mT5+OyMhI7joyMhIzZswAULPZlZCQgLS0NOTn5+Po0aNq9xsXF4fExETEx8cjJSUFfD4fP/74o5JcaGioyjW9iRMnNth/bGwszM3NYWJiAgAYNWoUXn31VfTv3x/9+/dH3759n9nHzp074e7urvYzERG+++67RrVhMDoizHvhObCwsEBVVRUyMjLQvXt3ZGZmYuTIkQCAqKgo7NixAxUVFcjNzYVUKoWHh4da/f7+++84ffo093O0rKwMenp6SnIBAQEICAhotN579uxRyBiQkZGB1NRUZGdnQ1tbG15eXjhw4AC3/PA0TckwsGbNGmhra8PPz6/R+rZHmIcJoz6Y0X1Oame7urq6eOutt8Dj8ZCZmYl169YhISEBenp6WLFihcqNqU6dOqG6uhoAFOqJCIGBgVi0aFGDY4eGhiIsLEzpvr6+fr3xWx8/foyYmBhs27aNu3fo0CE4OTmhR48eAAAvLy9cuHBBpdFtSoaBb7/9Fn/88QdOnDgBHo9FAGS82LDlhefE29sb+/btU1haePToEbp06YJevXqhqKio3nTWgwYN4rIK1JVxd3dHWFgYioqKAAD5+fmQyWRK7QMCAlTuXjcUMPvXX3/F6NGj0bt3b+6esbExTp8+DblcjurqasTFxUEkEim1bUqGgYMHD2Lr1q04cuQIunTpolab9kJreqJkZWVhxIgREAqFcHd35/6t1OXevXuws7ODVCqFhYUFVq1axdV9/vnnEIvFkEqlGDlyJK5evQqg5h07ODjAysoKYrEYP//8s8ae6YWhtReV21NBPZswTk5OZGlpqXDv7bffJjMzMxo1ahTNnj2by7BadyPt/PnzNGTIELK1taUVK1YobKR98803ZG1tTdbW1jRs2DA6f/68yrEby4QJE+iXX35RuFdVVUXvvvsumZubk4WFBc2dO5fkcjkREX388cd06NAhIiKys7Oj/v37c1lrx48fz/VhY2NDenp6pKOjQwYGBtwYenp6ZGRkxLWZPXu2Sr3QDjfS6r5LTePt7U0RERFERPTJJ5/QRx99pCQjl8uptLSU++zg4EBnz54lIqLCwkJO7tChQ+Ti4kJERFeuXOE2WO/cuUP9+vWjvLy8BnXR1LvrKKXVFWhPpTn/YBmKtLbRbU+eKNXV1dSrVy8qLy8nIiKZTKYwnioeP35MUqmUzp07p1S3e/ducnV1VdnO2tqarl692mDfzOg2rrDlBQYD7csTJS8vDz169IC2tjaAmiy/9+7dUzl+YWEhJBIJ+vbtC1dXV4wYMYKr27BhAwYNGoSgoCCV8R0uXLiAsrIyDBkyRO3nZTwbtpHGYKD9eqI8i549eyIlJQX5+fnw8vLClStXuAMOy5Ytw7JlyxAeHo5169YhIiKCa5ednY1Zs2Zxx5IZzQczugzG/9FePFH69OmD4uJiyOVyaGtrIzs7GwMGDGiw/969e2PMmDGIiYlROlXm6+uLgIAAzugWFBRg4sSJ2Lhxo8LMmNE8sOUFBuP/aC+eKDweD+PHj+c8J8LCwvDaa68pyd29exclJSUAauI6HD9+nPNKycjI4OR+/fVXLqZCaWkpPDw88O6772Lq1KkNf2GMptHai8rtqQgEgvsAiJXmLwKB4L4m3iGesRnaXjxRZDIZvfLKK2RmZkZubm5UUFBAREQJCQn09ttvExHR6dOnydramsRiMVlaWioEUp8+fTpZWFiQWCwmFxcXbrNs69atpK2tzXmbSCQSio+Pb1AXsI20RhVezXfGYLwY8Hg8Yv/mmxcejwciYqde1IQtLzAYDIYGYUaXwWAwNAgzugwGg6FBmNFlMBgMDcL8dBkvFAKBIIfH47HMis2IQCDIaW0d2hNspst4oSgrK+tPRLxnFQCdARQCWAAgB4CdOu3aawEwA8B9AIsBXGhM27Kysv6t9kLbIcxljMFQAY/Hex3ApwB6A1gKwArAESLqcOlweTzeLACGAB4DWAFAB4CEiG61qmIdFDbTZTBUswzAEABaAIIAyAH826oatRxJAAag5j+XcgDdAXzYqhp1YNhMl8FQAY/HOwvgLwA7iSjjWfIdAR6PxwdgD+D/A/CEiGa3rkYdE2Z0GQwGQ4Ow5QUGg8HQIMxljKGSzp0733/y5AlzrXoOBAJBjiZ39tk7axma+z2y9pdftgAAEKZJREFU5QWGSlhgmOdH04Fg2DtrGZr7PbLlBQaDwdAgzOgyGAyGBmFGl8FgMDQIM7qMdg2fz+cy5w4fPpy7//nnn0MsFkMqlWLkyJG4evWqUtu8vDyMHz8eQ4cOhaWlJZYtW8bVffjhh1y/FhYW0NLSQn5+PgCguLgY06dPh7m5OczNzfHrr7+2/IO+AOzZswdCoRBmZmbYvHmzSpldu3ZBT0+Pezeff/45V3fy5EmIRCKYmZlhyZIlSm33798PHo+H+Pj4FnsGtWjt1BWstM2CZ6S1aSvo6OiovF9YWMh9PnToELm4uCjJ5Ofnc6lzysvLaeTIkfTbb78pye3bt49cXV256zlz5tCWLVuIiKiyspIePnyoUgdoOI1Ne3lnqigoKCATExN68OABlZaWkkgkovT0dCW58PBwWrBggdL9yspKMjU1pYyMDKqqqqKxY8dSbGwsV19YWEgjR46k4cOH08WLFxulW3O/RzbTZaiFTCaDUCjE/PnzYWFhgfHjxyMpKQmvvvoqBg8ejG+//RZATQJET09PiMViWFlZITQ0FEBNQkZvb2/Y29tDKpUiOjq6RfXV1dXlPj969Ag8nvLmc69eveDk5AQA0NbWho2NDW7fvq0kt3fvXsycORNAzSz3+PHjWLx4MQBAS0tLZUr1tkxbfJcxMTF49dVX8fLLL6Nz586YNm1ao/pNSEiAsbExhEIh+Hw+/Pz8EBUVxdUHBQVh5cqVEAgEz63rc9OcFpyVjlPw1KwpMzOT+Hw+Xbp0iYiIJk+eTGPGjKEnT57Q/fv3qU+fPlRdXU0HDx5UmInUJkz09fWlEydOEFHNDNPU1JSKi4vpaVxcXBSSItaW7du3K8kSEfH5fLKzsyM7Ozv64YcfFOrWr19PJiYmZGBgoHLWVJf8/HwyMjKijIwMhft5eXmkq6vL6Xr58mUaNmwYvf322ySVSsnb25sePHigsk+00ZluW3yXwcHBXMJPIqKwsDBatGiRklx4eDj179+frK2tadKkSVxCzf3795Ofnx8nFxcXRx4eHkREdO7cOfL29iYiImdn51af6bLDEQy1MTIygq2tLQBAKpVCS0sLOjo66NevH7p06YK8vDyIxWIEBgYiMDAQEyZMgIuLC4CamUxqairXl1wuh0wmg7W1tcIYJ06caJROt27dgqGhIe7evYtx48ZBKBRi1KhRAIBly5Zh2bJlCA8Px7p16xAREaGyj4qKCrz11lsICAiAUChUqNu/fz8mTJiA7t27AwAqKytx+fJlbN68Gd9//z2++OILLFmyBD/++GOj9G5t2uK7VIfJkyfD29sbAoEA0dHR8PLyUkgn/zQVFRUIDAzEwYMHm12XpsKWFxhqo6Ojw33m8/lK15WVlTAzM0NSUhLs7OwQEhICf39/AEBVVRXOnTuH5ORkJCcnIysrS+mPFABcXV25TZK6ZceOHSp1MjQ0BADo6+tjypQp+Ouvv5RkfH19FX5q1oWIMHv2bFhaWqrcfKm7tFA73ssvv4wxY8YAAN58800kJiaq7Lst09bepaHh/2vv/mOirv84gD/vOJEFCibOgq9h/PQODw4C5IchS/JAjDCzgUUSFBoFNrORkSK2EU7bBHfkcsFcrjNpNVgBzWU/DGMxGD9GCEVag4ji2AC7i1Pu9f2D8cmTOz0QjoNej+2z3d3n/f58PncveO1zn3u/3p//GV3a+e233+Du7j6p3fLly4VLBFu3boVWq8XAwIDZ/n19ffj5558RGRmJ1atXo76+Hlu3bsU333wzjU9thszkaTMvC2eBicsLfn5+wvP8/Hx6++23heceHh7U19dHPT09pNVqiYioqamJFAoFERE9/fTTdPjwYaH9xFfbuzE4OEg6nY6IiIaHhykkJIRqa2uJiKizs1NoV1FRQSEhISa3sWfPHkpJSSGDwTBp3a+//korVqwgvV5v9Pr69eupubmZiIhOnTpF27dvN7lt2PDlBVuMpYeHh9EPaR0dHZPa9fb2Co+/++47WrVqFRkMBrpx4wZ5enoa/ZBWU1MzqT9fXmALTmtrK3JzcyEWiyESiVBUVAQAKCkpQXZ2NuRyOQwGAzw8PFBdXX1X+7p8+TIyMzMhFosxNjaG1NRUKJVKAMChQ4fQ0tICiUSCFStWGH39VygUaG5uRnt7O4qLiyGTyRAUFAQAeOGFF/DSSy8BANRqNbZv345FixYZ7be0tBTPP/88dDodVq5cibKysrt6H7bKmrFctmwZ3nrrLURERICIsHv3bqxZswYAcPDgQYSEhCAxMREnTpxAVVUVJBIJnJyccO7cOYhEItjZ2eHdd9/FY489huvXryMxMRFxcXF3/RnMBp57gZnEdfx3j+deWBh47gXGGJvHOOkyxpgVcdJlNm0uBrP39vZiw4YNcHR0xO7du4XXDQYDkpKS4OfnB7lcjvT0dOj1eqsfn62zpZgBQFZWFgIDAxEYGAilUonff//9jn1mEyddxm7h5OSEwsJCk/X/mZmZ6OzsRGtrK3Q6Hd577705OEJ2q9vFrKioCC0tLWhpaUFCQgIOHjx4xz6ziZMus5i5stDy8nKEhYVBoVBAqVTizz//BDA+gmDnzp2IiYmBh4cHVCoVVCoVQkJCEBAQgO7ubqHdM888g8jISPj4+KCwsNDk/isqKrBu3ToEBQXhySefxPDwMAAgLy8PMpkMAQEBRmNqp8vZ2RlRUVGTztjEYjE2b94MYPzHldDQUJNlw7bkvx4zAFi6dKnw+OaS8Nv1mVUzOf6Ml4WzwMSYT3NloQMDA8JrKpWKXn31VSIaH/8ZFhYmlJcuWbKEiouLiYjonXfeEco88/PzSSqV0rVr12hkZISkUik1NjYS0b8T2ly+fJni4uJodHSUiIgKCwvpwIEDpNFoSCqV0tjYmNEx3Wx0dNRkOWpgYCBVVlZOaj/B3OQqE9sMCAigr776ymx/2MA4XY7ZuOzsbHJzcyOZTEb9/f0W9Zkw03HkcbrMYubKQjs6OpCXl4fBwUGMjo7C09NT6LN582ahvNTFxQWPP/44gPGxshcuXBDaJSUlwdHRUXh88eJFBAcHC+vPnz+PlpYWhIWFARgvPQ0NDYWzszMcHByQnp6OuLg4JCYmTjpue3t7NDc3z+hnsWvXLkRHRwuVabaKYzaupKQExcXFKCgogEqlQkFBwYxte6r48gKzmLmy0NTUVBw7dgxtbW1QqVT4559/hD7myk0nSk0tRUTYsWOHUHr6448/4vTp07Czs0N9fT1SUlJw6dIlhIaGTtquXq83WY6qUChQVVU15c9h//79GBoaQnFx8ZT7WhvH7F8ikQipqalzPg8Dn+kyi/X29uLee+9FcnIy/Pz8kJ6eDmB8ukN3d3cQEcrLy6e17crKSrz55psgIlRWVuKDDz4wWh8bG4uEhATs3bsXbm5u+Pvvv9HT0wM3NzdotVoolUrExMTggQcewLVr1+Di4iL0ncmzpuPHj+P7779HbW0txGLbP2fhmAFdXV3w9fUFAHz66aeQSqUzst3p4qTLLGauLLSoqAiRkZFwdXVFbGysMCRnKoKDgxEbG4u//voLzz33nNHXVACQSqU4evQotmzZIpwVHT58GI6Ojti2bRt0Oh0MBgP27dtn9M87HaOjo/Dy8oJWq4Ver8dnn32GiooKrF27Fnv37oWXlxfCw8MBAAkJCUZ3L7A1//WYRUREIDMzExqNBiKRCJ6enigtLb1jn9nEZcDMJGuWlB46dAgODg5Gt8tZCBZyGfBCjZkpXAbMGGPzGJ/pMpN48pS7t5DPdP9L+EyXMcbmMU66bNakpaXh7NmzVt+vpXMkjI6OYtOmTXBxcZk092pbWxuioqIQEBCAjRs3oq+vT1iXl5cHf39/+Pv7Q6VSzfr7mStzFT9g/M4PUVFR8PHxQXx8PIaGhia1uV38AODkyZNYs2YN/P398eyzz1rUxxo46bIFyZI5Euzs7PD666/jzJkzk9ZlZGQgPz8fra2teOWVV4QfjKqrq1FXV4fm5mY0NTVBrVYLpbFs5uTm5mLXrl346aefEBYWhiNHjkxqc7v4ffvttzhz5gwaGxvR3t6Oo0eP3rGPtXDSZRY5cOAAjh07Jjw/deoUXnzxRQBAdnY2QkNDsXbtWmRlZcHUdcXVq1fjjz/+ADB+C/CJuwIAwIkTJxAWFobAwEBkZmZOaQC+KZbOkSCRSPDII4/Ayclp0rqOjg6hekupVOLjjz8GALS3t2PDhg1YtGgRFi9ejOjo6DkfbG+J+RQ/IsIXX3yB5ORkAEB6errJe9zdLn4qlQr79+8XKuZWrlx5xz7WwkmXWSQlJQVqtVp4rlarsWPHDgDjw4caGhrQ1taGwcFBfP755xZv98KFC2hsbER9fT1aWlogFotN3lm3pKTEZHXSRHI1R6/X4/Tp04iPj7f4mIDx8tmJRPvRRx9Bq9VCo9FAoVCgpqYGIyMjGB4eRm1trc1PegPMr/hpNBosXboU9vb2AMZvWnnz5R1LdHZ24ocffkB4eDgiIiJw/vz5KfWfTVwcwSwik8kwNjaGrq4uLFmyBFeuXMH69esBAJ988glOnjyJ69evY2BgAAqFAlu2bLFou9XV1fj666+FgfU6nQ6urq6T2uXk5CAnJ2fKxz3dORLKy8uxZ88eHDlyBBs3bsR9990HiUSCRx99FE1NTYiOjoazszPWrVsHicT2/43ma/ym68aNG+jt7cWlS5fwyy+/ICYmBu3t7XB2drbaMZhj+38tzGZMnC05OzvjqaeegkgkwpUrV1BYWIiGhga4uroiLy/PqI5/gkQigcFgAACj9USEffv24eWXX77tvktKSkzeANLNzc3sTREn5kh4//33p/I2AQC+vr6oqakBAAwNDQnvGxi/3pibmwsAeO211+Dl5TXl7c+F+RK/5cuXY3h4GHq9Hvb29ujp6cH9998/pfe6atUqbNu2DWKxGN7e3vDy8kJXVxdCQ0OntJ3ZwJcXmMWSk5Nx9uxZo6+mIyMjuOeee7Bs2TIMDQ0JX8lv9eCDD6KxsREAjNrEx8ejrKxM+HV6cHAQV69endQ/JydHmDjl5sVcwp2YI+HDDz+c1hwJE/PLAkBBQYFwZ4GxsTFoNBoAQHd3N6qqqoTPwtbNl/iJRCJs2rRJGDlRVlaGpKSkKb3XJ554Al9++SUAoL+/H93d3UYzqc2pmZwnkpeFs8DE3KxERJGRkeTv72/0WkZGBnl7e9PDDz9MaWlplJ+fT0REO3fuJLVaTUREdXV15OvrSw899BDl5eWRn5+f0L+0tJTkcjnJ5XIKDg6muro6k/u21PDwMIlEIvL29hbmYH3jjTeIiKihoYEyMjKEtkFBQeTq6kqLFy8md3d3OnfuHBERHT9+nHx8fMjHx4eysrJIr9cTEZFOpyOpVEoymYyCgoLo4sWLZo8DNjCf7q3mQ/yIiK5evUrh4eHk7e1NSqVSmHPX0vjp9XpKS0sjmUxGcrmcKioq7tjHnJmOI1ekMZO4uunucUXawsAVaYwxNo9x0mWMMSvipMsYY1bESZcxxqyIx+kykxwcHPpFItHKuT6O+czBwaHf2vvjmM28mY4jj15gjDEr4ssLjDFmRZx0GWPMijjpMsaYFXHSZYwxK+KkyxhjVsRJlzHGrIiTLmOMWREnXcYYsyJOuowxZkWcdBljzIo46TLGmBVx0mWMMSv6P/vrN864jWPpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.85598879,  0.72500984])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X):\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
    "* Why is DCG performance so different from Ranklib, and it seems to wander around more (precision does this)\n",
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "Train DCGs\n",
      "mean    10.61593108150268\n",
      "median  10.739621334025406\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    14.34966497096901\n",
      "median  13.216212771507438\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    12.357610149958038\n",
      "median  11.567827400752915\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    14.24721713178369\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    13.441858214366144\n",
      "median  12.506165082920402\n",
      "----------\n",
      "round 5\n",
      "Train DCGs\n",
      "mean    14.091782646778626\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 6\n",
      "Train DCGs\n",
      "mean    13.426222703083178\n",
      "median  12.460843740423556\n",
      "----------\n",
      "round 7\n",
      "Train DCGs\n",
      "mean    14.008223263605812\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 8\n",
      "Train DCGs\n",
      "mean    13.434549080998451\n",
      "median  12.346789866126002\n",
      "----------\n",
      "round 9\n",
      "Train DCGs\n",
      "mean    13.862740868310802\n",
      "median  12.615453090513515\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=10, max_leaf_nodes=10, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                 262\n",
       "uid                               8_69315\n",
       "qid                                     8\n",
       "keywords             battlestar galactica\n",
       "docId                               69315\n",
       "grade                                   3\n",
       "features           [15.908707, 31.961138]\n",
       "last_prediction                       NaN\n",
       "display_rank                           26\n",
       "train_dcg                               0\n",
       "dcg                                     0\n",
       "lambda                                  0\n",
       "weight                                  0\n",
       "path                  1010001000000000001\n",
       "Name: (8, 26), dtype: object"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.iloc[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001000000000001         NaN\n",
       "1010001000000000010    1.995794\n",
       "1010010000100000000   -2.000000\n",
       "1010010001001010000    2.000000\n",
       "1010010001001100000    1.846154\n",
       "1010010001010000000    2.000000\n",
       "1100100010000000000    1.415385\n",
       "1100100100000000100    2.000000\n",
       "1100100100000001000   -2.000000\n",
       "1101000000000000000   -1.297171\n",
       "dtype: float64"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(141.64615384615385, 199.32, 'X[0] <= 10.666\\nmse = 0.044\\nsamples = 1390\\nvalue = 0.0'),\n",
       " Text(51.50769230769231, 163.07999999999998, 'X[0] <= 9.182\\nmse = 0.014\\nsamples = 1329\\nvalue = -0.029'),\n",
       " Text(25.753846153846155, 126.83999999999999, 'mse = 0.007\\nsamples = 1301\\nvalue = -0.035'),\n",
       " Text(77.26153846153846, 126.83999999999999, 'X[0] <= 9.451\\nmse = 0.252\\nsamples = 28\\nvalue = 0.244'),\n",
       " Text(51.50769230769231, 90.6, 'X[1] <= 4.527\\nmse = 0.302\\nsamples = 4\\nvalue = 0.847'),\n",
       " Text(25.753846153846155, 54.359999999999985, 'mse = 0.0\\nsamples = 1\\nvalue = -0.05'),\n",
       " Text(77.26153846153846, 54.359999999999985, 'mse = 0.045\\nsamples = 3\\nvalue = 1.146'),\n",
       " Text(103.01538461538462, 90.6, 'mse = 0.173\\nsamples = 24\\nvalue = 0.144'),\n",
       " Text(231.7846153846154, 163.07999999999998, 'X[0] <= 13.528\\nmse = 0.297\\nsamples = 61\\nvalue = 0.625'),\n",
       " Text(180.27692307692308, 126.83999999999999, 'X[1] <= 10.412\\nmse = 0.312\\nsamples = 32\\nvalue = 0.451'),\n",
       " Text(154.52307692307693, 90.6, 'X[0] <= 10.761\\nmse = 0.322\\nsamples = 27\\nvalue = 0.538'),\n",
       " Text(128.76923076923077, 54.359999999999985, 'mse = 0.003\\nsamples = 2\\nvalue = 1.331'),\n",
       " Text(180.27692307692308, 54.359999999999985, 'X[1] <= 9.943\\nmse = 0.293\\nsamples = 25\\nvalue = 0.474'),\n",
       " Text(154.52307692307693, 18.119999999999976, 'mse = 0.259\\nsamples = 22\\nvalue = 0.382'),\n",
       " Text(206.03076923076924, 18.119999999999976, 'mse = 0.023\\nsamples = 3\\nvalue = 1.15'),\n",
       " Text(206.03076923076924, 90.6, 'mse = 0.0\\nsamples = 5\\nvalue = -0.018'),\n",
       " Text(283.2923076923077, 126.83999999999999, 'X[1] <= 25.987\\nmse = 0.209\\nsamples = 29\\nvalue = 0.818'),\n",
       " Text(257.53846153846155, 90.6, 'mse = 0.192\\nsamples = 28\\nvalue = 0.847'),\n",
       " Text(309.04615384615386, 90.6, 'mse = -0.0\\nsamples = 1\\nvalue = 0.0')]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"41\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>2_1366</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1366</td>\n",
       "      <td>4</td>\n",
       "      <td>[10.00459, 8.694555]</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>2.485134</td>\n",
       "      <td>2.452373</td>\n",
       "      <td>1010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>2_152601</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>152601</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.022374</td>\n",
       "      <td>0.022079</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>2_198001</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>198001</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.032957</td>\n",
       "      <td>0.032522</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>2_170430</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>170430</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.039345</td>\n",
       "      <td>0.038826</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>2_18206</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>18206</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.043712</td>\n",
       "      <td>0.043136</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67</td>\n",
       "      <td>2_47059</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>47059</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.262650</td>\n",
       "      <td>0.525299</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>1.592257</td>\n",
       "      <td>0.861259</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68</td>\n",
       "      <td>2_18215</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>18215</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.055760</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>69</td>\n",
       "      <td>2_70808</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>70808</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>7</td>\n",
       "      <td>0.239812</td>\n",
       "      <td>0.479625</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>1.147813</td>\n",
       "      <td>0.645303</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>2_351862</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>351862</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>8</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>0.462756</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>0.986005</td>\n",
       "      <td>0.566714</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71</td>\n",
       "      <td>2_154019</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>154019</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>9</td>\n",
       "      <td>0.224244</td>\n",
       "      <td>0.448488</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>0.849135</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>72</td>\n",
       "      <td>2_70</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>40</td>\n",
       "      <td>0.156438</td>\n",
       "      <td>0.312876</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.059459</td>\n",
       "      <td>0.058675</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>73</td>\n",
       "      <td>2_22777</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>22777</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>11</td>\n",
       "      <td>0.212746</td>\n",
       "      <td>0.212746</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.167689</td>\n",
       "      <td>0.113340</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74</td>\n",
       "      <td>2_43189</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>43189</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>12</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.187613</td>\n",
       "      <td>0.123788</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>75</td>\n",
       "      <td>2_318</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>13</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.205381</td>\n",
       "      <td>0.133105</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>76</td>\n",
       "      <td>2_39829</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>39829</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>14</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.221361</td>\n",
       "      <td>0.141485</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>77</td>\n",
       "      <td>2_1578</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1578</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>15</td>\n",
       "      <td>0.196562</td>\n",
       "      <td>0.393123</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.064003</td>\n",
       "      <td>0.063159</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>2_25855</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>25855</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>16</td>\n",
       "      <td>0.193426</td>\n",
       "      <td>0.193426</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.249042</td>\n",
       "      <td>0.156001</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>79</td>\n",
       "      <td>2_42012</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>42012</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>17</td>\n",
       "      <td>0.190551</td>\n",
       "      <td>0.381103</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.065270</td>\n",
       "      <td>0.064410</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>80</td>\n",
       "      <td>2_32276</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>32276</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>18</td>\n",
       "      <td>0.187902</td>\n",
       "      <td>0.187902</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.272306</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>63</td>\n",
       "      <td>2_17819</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>17819</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>19</td>\n",
       "      <td>0.185449</td>\n",
       "      <td>0.185449</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.282634</td>\n",
       "      <td>0.173616</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>61</td>\n",
       "      <td>2_330459</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>330459</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>20</td>\n",
       "      <td>0.183169</td>\n",
       "      <td>0.183169</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.292234</td>\n",
       "      <td>0.178650</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>42</td>\n",
       "      <td>2_1374</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1374</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 9.808358]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>21</td>\n",
       "      <td>0.181043</td>\n",
       "      <td>1.448341</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.067276</td>\n",
       "      <td>0.066389</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>60</td>\n",
       "      <td>2_1893</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1893</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>22</td>\n",
       "      <td>0.179052</td>\n",
       "      <td>0.179052</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.309571</td>\n",
       "      <td>0.187741</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43</td>\n",
       "      <td>2_1371</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1371</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 5.557296]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>23</td>\n",
       "      <td>0.177184</td>\n",
       "      <td>1.417471</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.068090</td>\n",
       "      <td>0.067192</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44</td>\n",
       "      <td>2_1246</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1246</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 9.540649]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>24</td>\n",
       "      <td>0.175425</td>\n",
       "      <td>1.403401</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.068461</td>\n",
       "      <td>0.067558</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45</td>\n",
       "      <td>2_1375</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1375</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 9.472043]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.068811</td>\n",
       "      <td>0.067904</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46</td>\n",
       "      <td>2_1367</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1367</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 8.262948]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.069142</td>\n",
       "      <td>0.068231</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>47</td>\n",
       "      <td>2_60375</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>60375</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 7.819334]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>1.365658</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.069456</td>\n",
       "      <td>0.068540</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>48</td>\n",
       "      <td>2_110123</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>110123</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.8909245, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>0.338588</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.069754</td>\n",
       "      <td>0.068834</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>49</td>\n",
       "      <td>2_36685</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>36685</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.8909245, 5.777752]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>29</td>\n",
       "      <td>0.167949</td>\n",
       "      <td>0.335898</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.070038</td>\n",
       "      <td>0.069114</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>50</td>\n",
       "      <td>2_17711</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>17711</td>\n",
       "      <td>1</td>\n",
       "      <td>[6.8265696, 8.381828]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>30</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.070308</td>\n",
       "      <td>0.069381</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>51</td>\n",
       "      <td>2_2153</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>2153</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>31</td>\n",
       "      <td>0.165443</td>\n",
       "      <td>0.165443</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.366880</td>\n",
       "      <td>0.217793</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>52</td>\n",
       "      <td>2_339403</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>339403</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>32</td>\n",
       "      <td>0.164272</td>\n",
       "      <td>0.164272</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.371809</td>\n",
       "      <td>0.220378</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>53</td>\n",
       "      <td>2_21501</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>21501</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>33</td>\n",
       "      <td>0.163151</td>\n",
       "      <td>0.163151</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.376529</td>\n",
       "      <td>0.222853</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>54</td>\n",
       "      <td>2_81182</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>81182</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>34</td>\n",
       "      <td>0.162077</td>\n",
       "      <td>0.162077</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.381054</td>\n",
       "      <td>0.225226</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>55</td>\n",
       "      <td>2_62414</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>62414</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>35</td>\n",
       "      <td>0.161045</td>\n",
       "      <td>0.161045</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.385399</td>\n",
       "      <td>0.227504</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>56</td>\n",
       "      <td>2_21989</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>21989</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>36</td>\n",
       "      <td>0.160053</td>\n",
       "      <td>0.160053</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.389575</td>\n",
       "      <td>0.229694</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>57</td>\n",
       "      <td>2_11</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>37</td>\n",
       "      <td>0.159099</td>\n",
       "      <td>0.159099</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.393593</td>\n",
       "      <td>0.231801</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>58</td>\n",
       "      <td>2_12180</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>12180</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>38</td>\n",
       "      <td>0.158180</td>\n",
       "      <td>0.158180</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.397464</td>\n",
       "      <td>0.233831</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>59</td>\n",
       "      <td>2_140607</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>140607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>39</td>\n",
       "      <td>0.157293</td>\n",
       "      <td>0.157293</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.401196</td>\n",
       "      <td>0.235788</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>81</td>\n",
       "      <td>2_105536</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>105536</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>10</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.404799</td>\n",
       "      <td>0.237677</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       uid  qid keywords   docId  grade               features  \\\n",
       "qid                                                                           \n",
       "2   0      41    2_1366    2    rocky    1366      4   [10.00459, 8.694555]   \n",
       "    1      62  2_152601    2    rocky  152601      0             [0.0, 0.0]   \n",
       "    2      64  2_198001    2    rocky  198001      0             [0.0, 0.0]   \n",
       "    3      65  2_170430    2    rocky  170430      0             [0.0, 0.0]   \n",
       "    4      66   2_18206    2    rocky   18206      0             [0.0, 0.0]   \n",
       "    5      67   2_47059    2    rocky   47059      1             [0.0, 0.0]   \n",
       "    6      68   2_18215    2    rocky   18215      0             [0.0, 0.0]   \n",
       "    7      69   2_70808    2    rocky   70808      1             [0.0, 0.0]   \n",
       "    8      70  2_351862    2    rocky  351862      1             [0.0, 0.0]   \n",
       "    9      71  2_154019    2    rocky  154019      1             [0.0, 0.0]   \n",
       "    10     72      2_70    2    rocky      70      1             [0.0, 0.0]   \n",
       "    11     73   2_22777    2    rocky   22777      0             [0.0, 0.0]   \n",
       "    12     74   2_43189    2    rocky   43189      0             [0.0, 0.0]   \n",
       "    13     75     2_318    2    rocky     318      0             [0.0, 0.0]   \n",
       "    14     76   2_39829    2    rocky   39829      0             [0.0, 0.0]   \n",
       "    15     77    2_1578    2    rocky    1578      1             [0.0, 0.0]   \n",
       "    16     78   2_25855    2    rocky   25855      0             [0.0, 0.0]   \n",
       "    17     79   2_42012    2    rocky   42012      1             [0.0, 0.0]   \n",
       "    18     80   2_32276    2    rocky   32276      0             [0.0, 0.0]   \n",
       "    19     63   2_17819    2    rocky   17819      0             [0.0, 0.0]   \n",
       "    20     61  2_330459    2    rocky  330459      0             [0.0, 0.0]   \n",
       "    21     42    2_1374    2    rocky    1374      3   [8.115546, 9.808358]   \n",
       "    22     60    2_1893    2    rocky    1893      0             [0.0, 0.0]   \n",
       "    23     43    2_1371    2    rocky    1371      3   [8.115546, 5.557296]   \n",
       "    24     44    2_1246    2    rocky    1246      3   [8.115546, 9.540649]   \n",
       "    25     45    2_1375    2    rocky    1375      3   [8.115546, 9.472043]   \n",
       "    26     46    2_1367    2    rocky    1367      3   [8.115546, 8.262948]   \n",
       "    27     47   2_60375    2    rocky   60375      3   [8.115546, 7.819334]   \n",
       "    28     48  2_110123    2    rocky  110123      1       [5.8909245, 0.0]   \n",
       "    29     49   2_36685    2    rocky   36685      1  [5.8909245, 5.777752]   \n",
       "    30     50   2_17711    2    rocky   17711      1  [6.8265696, 8.381828]   \n",
       "    31     51    2_2153    2    rocky    2153      0             [0.0, 0.0]   \n",
       "    32     52  2_339403    2    rocky  339403      0             [0.0, 0.0]   \n",
       "    33     53   2_21501    2    rocky   21501      0             [0.0, 0.0]   \n",
       "    34     54   2_81182    2    rocky   81182      0             [0.0, 0.0]   \n",
       "    35     55   2_62414    2    rocky   62414      0             [0.0, 0.0]   \n",
       "    36     56   2_21989    2    rocky   21989      0             [0.0, 0.0]   \n",
       "    37     57      2_11    2    rocky      11      0             [0.0, 0.0]   \n",
       "    38     58   2_12180    2    rocky   12180      0             [0.0, 0.0]   \n",
       "    39     59  2_140607    2    rocky  140607      0             [0.0, 0.0]   \n",
       "    40     81  2_105536    2    rocky  105536      0             [0.0, 0.0]   \n",
       "\n",
       "        last_prediction  display_rank  discount      gain  train_dcg  \\\n",
       "qid                                                                    \n",
       "2   0          3.798236             0  0.500000  8.000000  11.466327   \n",
       "    1         -0.517338             1  0.386853  0.386853  11.466327   \n",
       "    2         -0.517338             2  0.333333  0.333333  11.466327   \n",
       "    3         -0.517338             3  0.301030  0.301030  11.466327   \n",
       "    4         -0.517338             4  0.278943  0.278943  11.466327   \n",
       "    5         -0.517338             5  0.262650  0.525299  11.466327   \n",
       "    6         -0.517338             6  0.250000  0.250000  11.466327   \n",
       "    7         -0.517338             7  0.239812  0.479625  11.466327   \n",
       "    8         -0.517338             8  0.231378  0.462756  11.466327   \n",
       "    9         -0.517338             9  0.224244  0.448488  11.466327   \n",
       "    10        -0.517338            40  0.156438  0.312876  11.466327   \n",
       "    11        -0.517338            11  0.212746  0.212746  11.466327   \n",
       "    12        -0.517338            12  0.208015  0.208015  11.466327   \n",
       "    13        -0.517338            13  0.203795  0.203795  11.466327   \n",
       "    14        -0.517338            14  0.200000  0.200000  11.466327   \n",
       "    15        -0.517338            15  0.196562  0.393123  11.466327   \n",
       "    16        -0.517338            16  0.193426  0.193426  11.466327   \n",
       "    17        -0.517338            17  0.190551  0.381103  11.466327   \n",
       "    18        -0.517338            18  0.187902  0.187902  11.466327   \n",
       "    19        -0.517338            19  0.185449  0.185449  11.466327   \n",
       "    20        -0.517338            20  0.183169  0.183169  11.466327   \n",
       "    21        -0.517338            21  0.181043  1.448341  11.466327   \n",
       "    22        -0.517338            22  0.179052  0.179052  11.466327   \n",
       "    23        -0.517338            23  0.177184  1.417471  11.466327   \n",
       "    24        -0.517338            24  0.175425  1.403401  11.466327   \n",
       "    25        -0.517338            25  0.173765  1.390123  11.466327   \n",
       "    26        -0.517338            26  0.172195  1.377563  11.466327   \n",
       "    27        -0.517338            27  0.170707  1.365658  11.466327   \n",
       "    28        -0.517338            28  0.169294  0.338588  11.466327   \n",
       "    29        -0.517338            29  0.167949  0.335898  11.466327   \n",
       "    30        -0.517338            30  0.166667  0.333333  11.466327   \n",
       "    31        -0.517338            31  0.165443  0.165443  11.466327   \n",
       "    32        -0.517338            32  0.164272  0.164272  11.466327   \n",
       "    33        -0.517338            33  0.163151  0.163151  11.466327   \n",
       "    34        -0.517338            34  0.162077  0.162077  11.466327   \n",
       "    35        -0.517338            35  0.161045  0.161045  11.466327   \n",
       "    36        -0.517338            36  0.160053  0.160053  11.466327   \n",
       "    37        -0.517338            37  0.159099  0.159099  11.466327   \n",
       "    38        -0.517338            38  0.158180  0.158180  11.466327   \n",
       "    39        -0.517338            39  0.157293  0.157293  11.466327   \n",
       "    40        -0.517338            10  0.218104  0.218104  11.466327   \n",
       "\n",
       "              dcg    lambda    weight     path  \n",
       "qid                                             \n",
       "2   0   11.466327  2.485134  2.452373  1010101  \n",
       "    1   11.466327 -0.022374  0.022079  1100000  \n",
       "    2   11.466327 -0.032957  0.032522  1100000  \n",
       "    3   11.466327 -0.039345  0.038826  1100000  \n",
       "    4   11.466327 -0.043712  0.043136  1100000  \n",
       "    5   11.466327  1.592257  0.861259  1100000  \n",
       "    6   11.466327 -0.055760  0.051946  1100000  \n",
       "    7   11.466327  1.147813  0.645303  1100000  \n",
       "    8   11.466327  0.986005  0.566714  1100000  \n",
       "    9   11.466327  0.849135  0.500237  1100000  \n",
       "    10  11.466327 -0.059459  0.058675  1100000  \n",
       "    11  11.466327 -0.167689  0.113340  1100000  \n",
       "    12  11.466327 -0.187613  0.123788  1100000  \n",
       "    13  11.466327 -0.205381  0.133105  1100000  \n",
       "    14  11.466327 -0.221361  0.141485  1100000  \n",
       "    15  11.466327 -0.064003  0.063159  1100000  \n",
       "    16  11.466327 -0.249042  0.156001  1100000  \n",
       "    17  11.466327 -0.065270  0.064410  1100000  \n",
       "    18  11.466327 -0.272306  0.168200  1100000  \n",
       "    19  11.466327 -0.282634  0.173616  1100000  \n",
       "    20  11.466327 -0.292234  0.178650  1100000  \n",
       "    21  11.466327 -0.067276  0.066389  1100000  \n",
       "    22  11.466327 -0.309571  0.187741  1100000  \n",
       "    23  11.466327 -0.068090  0.067192  1100000  \n",
       "    24  11.466327 -0.068461  0.067558  1100000  \n",
       "    25  11.466327 -0.068811  0.067904  1100000  \n",
       "    26  11.466327 -0.069142  0.068231  1100000  \n",
       "    27  11.466327 -0.069456  0.068540  1100000  \n",
       "    28  11.466327 -0.069754  0.068834  1100000  \n",
       "    29  11.466327 -0.070038  0.069114  1100000  \n",
       "    30  11.466327 -0.070308  0.069381  1100000  \n",
       "    31  11.466327 -0.366880  0.217793  1100000  \n",
       "    32  11.466327 -0.371809  0.220378  1100000  \n",
       "    33  11.466327 -0.376529  0.222853  1100000  \n",
       "    34  11.466327 -0.381054  0.225226  1100000  \n",
       "    35  11.466327 -0.385399  0.227504  1100000  \n",
       "    36  11.466327 -0.389575  0.229694  1100000  \n",
       "    37  11.466327 -0.393593  0.231801  1100000  \n",
       "    38  11.466327 -0.397464  0.233831  1100000  \n",
       "    39  11.466327 -0.401196  0.235788  1100000  \n",
       "    40  11.466327 -0.404799  0.237677  1100000  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
