{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log Elasticsearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with Elasticsearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
    "1. **Likely bug** - When using DCG, the model here seems to wander around more than Ranklib, instead of converging. I think likely this points at an underlying bug that's actively being investigated.\n",
    "2. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the Elasticsearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries...\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>183.343798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>6.189645</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>116.134366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>39.713833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>2.408240</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>30.087439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>1.115772</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>8.628473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1371</td>\n",
       "      <td>40_81899</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>81899</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-10.968332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1370</td>\n",
       "      <td>40_209276</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>209276</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.062526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1368</td>\n",
       "      <td>40_52959</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>52959</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.151815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1367</td>\n",
       "      <td>40_85783</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>85783</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.236624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.317325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       1     1_1370    1      rambo    1370      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1371   40_81899   40  star wars   81899      0   \n",
       "    26   1370  40_209276   40  star wars  209276      0   \n",
       "    27   1368   40_52959   40  star wars   52959      0   \n",
       "    28   1367   40_85783   40  star wars   85783      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount      gain        dcg  \\\n",
       "qid                                                                           \n",
       "1   0   [11.657399, 10.083591]             0  0.500000  8.000000  22.796491   \n",
       "    1         [0.0, 11.113943]             1  0.386853  6.189645  22.796491   \n",
       "    2    [6.036743, 11.113943]             2  0.333333  2.666667  22.796491   \n",
       "    3    [9.456276, 13.265001]             3  0.301030  2.408240  22.796491   \n",
       "    4          [0.0, 6.869545]             4  0.278943  1.115772  22.796491   \n",
       "...                        ...           ...       ...       ...        ...   \n",
       "40  25         [0.0, 6.868508]            25  0.173765  0.173765  21.491637   \n",
       "    26        [5.8994045, 0.0]            26  0.172195  0.172195  21.491637   \n",
       "    27           [7.2726, 0.0]            27  0.170707  0.170707  21.491637   \n",
       "    28           [7.2726, 0.0]            28  0.169294  0.169294  21.491637   \n",
       "    29              [0.0, 0.0]             8  0.231378  0.231378  21.491637   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   183.343798  \n",
       "    1   116.134366  \n",
       "    2    39.713833  \n",
       "    3    30.087439  \n",
       "    4     8.628473  \n",
       "...            ...  \n",
       "40  25  -10.968332  \n",
       "    26  -11.062526  \n",
       "    27  -11.151815  \n",
       "    28  -11.236624  \n",
       "    29  -11.317325  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
    "    ranked_list['discount'] = 1 / (1 + (np.log2(2 + ranked_list['display_rank'])))\n",
    "    ranked_list['gain'] = (2**ranked_list['grade']) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('grade', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = best_dcg - dcg_after_swap\n",
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
       "1     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "2     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "6     169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "7     170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "8     171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "9     173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "10    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "11    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "12    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "13    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "14    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "15    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "16    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "17    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "18    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "19    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "20    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "21    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "22    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "23    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "24    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "25    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "26    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "27    157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "28    156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "29    155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "30    154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "31    153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>183.343798</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.134366</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.713833</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.087439</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.628473</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>-10.968332</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-11.062526</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-11.151815</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-11.236624</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-11.317325</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
       "1   0   183.343798  [11.657399, 10.083591]\n",
       "    1   116.134366        [0.0, 11.113943]\n",
       "    2    39.713833   [6.036743, 11.113943]\n",
       "    3    30.087439   [9.456276, 13.265001]\n",
       "    4     8.628473         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -10.968332         [0.0, 6.868508]\n",
       "    26  -11.062526        [5.8994045, 0.0]\n",
       "    27  -11.151815           [7.2726, 0.0]\n",
       "    28  -11.236624           [7.2726, 0.0]\n",
       "    29  -11.317325              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([183.34379848])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.80365819])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 181.2, 'X[0] <= 10.666\\nmse = 793.283\\nsamples = 1390\\nvalue = 0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 222.998\\nsamples = 1329\\nvalue = -4.264'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 107.23\\nsamples = 1301\\nvalue = -5.173'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 3778.904\\nsamples = 28\\nvalue = 37.982'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 13.782\\nmse = 4190.964\\nsamples = 61\\nvalue = 92.903'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 4938.44\\nsamples = 34\\nvalue = 72.93'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 2114.76\\nsamples = 27\\nvalue = 118.054')]"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>7.292951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.100504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>3.094822</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.697540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>1_32221</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>32221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>1_801</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>801</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>2.992312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1_70</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.022346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1366</td>\n",
       "      <td>40_1891</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1891</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1365</td>\n",
       "      <td>40_1892</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1892</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 2.4547963]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1364</td>\n",
       "      <td>40_1895</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.487482, 2.062405]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.682829</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1363</td>\n",
       "      <td>40_330459</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>330459</td>\n",
       "      <td>3</td>\n",
       "      <td>[7.2694716, 4.237955]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>1.354350</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.013697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2      39    1_32221    1      rambo   32221      0   \n",
       "    3      29      1_801    1      rambo     801      1   \n",
       "    4      22       1_70    1      rambo      70      0   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1366    40_1891   40  star wars    1891      3   \n",
       "    26   1365    40_1892   40  star wars    1892      3   \n",
       "    27   1364    40_1895   40  star wars    1895      2   \n",
       "    28   1363  40_330459   40  star wars  330459      3   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]         7.292951             0  0.500000   \n",
       "    1    [9.456276, 13.265001]         3.798236             1  0.386853   \n",
       "    2               [0.0, 0.0]        -0.517338             2  0.333333   \n",
       "    3               [0.0, 0.0]        -0.517338             3  0.301030   \n",
       "    4               [0.0, 0.0]        -0.517338             4  0.278943   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]        -0.517338            25  0.173765   \n",
       "    26        [0.0, 2.4547963]        -0.517338            26  0.172195   \n",
       "    27    [6.487482, 2.062405]        -0.517338            27  0.170707   \n",
       "    28   [7.2694716, 4.237955]        -0.517338            28  0.169294   \n",
       "    29              [0.0, 0.0]        -0.517338             1  0.386853   \n",
       "\n",
       "            gain        dcg    lambda  \n",
       "qid                                    \n",
       "1   0   8.000000  13.741487  0.100504  \n",
       "    1   3.094822  13.741487  0.697540  \n",
       "    2   0.333333  13.741487 -0.005952  \n",
       "    3   0.602060  13.741487  2.992312  \n",
       "    4   0.278943  13.741487 -0.022346  \n",
       "...          ...        ...       ...  \n",
       "40  25  1.390123  10.793185 -0.002116  \n",
       "    26  1.377563  10.793185 -0.002126  \n",
       "    27  0.682829  10.793185 -0.002136  \n",
       "    28  1.354350  10.793185 -0.002145  \n",
       "    29  0.386853  10.793185 -0.013697  \n",
       "\n",
       "[1390 rows x 13 columns]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = best_dcg - dcg_after_swap\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
    "\n",
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>7.292951</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.002105</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>0.697540</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.024589</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.019625</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.025089</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.024931</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.020225</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.023536</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1.098537</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2.992312</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.022346</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.237120</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.235248</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.233307</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.231291</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.229197</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.227018</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.224748</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.222381</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.219909</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.217324</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.214616</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.032095</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.050806</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.039664</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.195112</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.191166</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.186963</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.177656</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.172476</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.166880</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.160809</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.154187</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.146926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.138911</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.129999</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.238927</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keywords  grade  last_prediction    lambda                features\n",
       "0     rambo      4         7.292951  0.100504  [11.657399, 10.083591]\n",
       "26    rambo      4        -0.517338  0.000000        [0.0, 11.113943]\n",
       "24    rambo      3        -0.517338 -0.002105   [6.036743, 11.113943]\n",
       "1     rambo      3         3.798236  0.697540   [9.456276, 13.265001]\n",
       "25    rambo      2        -0.517338 -0.024589         [0.0, 6.869545]\n",
       "10    rambo      1        -0.517338 -0.019625              [0.0, 0.0]\n",
       "28    rambo      1        -0.517338 -0.025089         [0.0, 4.563677]\n",
       "27    rambo      1        -0.517338 -0.024931        [0.0, 7.8627386]\n",
       "11    rambo      1        -0.517338 -0.020225              [0.0, 0.0]\n",
       "20    rambo      1        -0.517338 -0.023536              [0.0, 0.0]\n",
       "9     rambo      1        -0.517338  1.098537              [0.0, 0.0]\n",
       "3     rambo      1        -0.517338  2.992312              [0.0, 0.0]\n",
       "4     rambo      0        -0.517338 -0.022346              [0.0, 0.0]\n",
       "39    rambo      0        -0.517338 -0.237120              [0.0, 0.0]\n",
       "38    rambo      0        -0.517338 -0.235248              [0.0, 0.0]\n",
       "37    rambo      0        -0.517338 -0.233307              [0.0, 0.0]\n",
       "36    rambo      0        -0.517338 -0.231291              [0.0, 0.0]\n",
       "35    rambo      0        -0.517338 -0.229197              [0.0, 0.0]\n",
       "34    rambo      0        -0.517338 -0.227018              [0.0, 0.0]\n",
       "33    rambo      0        -0.517338 -0.224748              [0.0, 0.0]\n",
       "32    rambo      0        -0.517338 -0.222381              [0.0, 0.0]\n",
       "31    rambo      0        -0.517338 -0.219909              [0.0, 0.0]\n",
       "30    rambo      0        -0.517338 -0.217324              [0.0, 0.0]\n",
       "29    rambo      0        -0.517338 -0.214616              [0.0, 0.0]\n",
       "2     rambo      0        -0.517338 -0.005952              [0.0, 0.0]\n",
       "5     rambo      0        -0.517338 -0.032095              [0.0, 0.0]\n",
       "8     rambo      0        -0.517338 -0.050806              [0.0, 0.0]\n",
       "6     rambo      0        -0.517338 -0.039664              [0.0, 0.0]\n",
       "23    rambo      0        -0.517338 -0.195112              [0.0, 0.0]\n",
       "22    rambo      0        -0.517338 -0.191166              [0.0, 0.0]\n",
       "21    rambo      0        -0.517338 -0.186963              [0.0, 0.0]\n",
       "7     rambo      0        -0.517338 -0.045760              [0.0, 0.0]\n",
       "19    rambo      0        -0.517338 -0.177656              [0.0, 0.0]\n",
       "18    rambo      0        -0.517338 -0.172476              [0.0, 0.0]\n",
       "17    rambo      0        -0.517338 -0.166880              [0.0, 0.0]\n",
       "16    rambo      0        -0.517338 -0.160809              [0.0, 0.0]\n",
       "15    rambo      0        -0.517338 -0.154187              [0.0, 0.0]\n",
       "14    rambo      0        -0.517338 -0.146926              [0.0, 0.0]\n",
       "13    rambo      0        -0.517338 -0.138911              [0.0, 0.0]\n",
       "12    rambo      0        -0.517338 -0.129999              [0.0, 0.0]\n",
       "40    rambo      0        -0.517338 -0.238927              [0.0, 0.0]"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.loc[1, :][['keywords', 'grade', 'last_prediction', 'lambda', 'features']].sort_values('grade', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10050401])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02458865])"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>7.292951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>0.099688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>3.094822</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.697540</td>\n",
       "      <td>0.740546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>1_32221</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>32221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>0.005887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>1_801</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>801</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>2.992312</td>\n",
       "      <td>1.507942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1_70</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.022346</td>\n",
       "      <td>0.016692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1366</td>\n",
       "      <td>40_1891</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1891</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1365</td>\n",
       "      <td>40_1892</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1892</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 2.4547963]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002126</td>\n",
       "      <td>0.002125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1364</td>\n",
       "      <td>40_1895</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.487482, 2.062405]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.682829</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002136</td>\n",
       "      <td>0.002135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1363</td>\n",
       "      <td>40_330459</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>330459</td>\n",
       "      <td>3</td>\n",
       "      <td>[7.2694716, 4.237955]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>1.354350</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002145</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>0.013544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2      39    1_32221    1      rambo   32221      0   \n",
       "    3      29      1_801    1      rambo     801      1   \n",
       "    4      22       1_70    1      rambo      70      0   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1366    40_1891   40  star wars    1891      3   \n",
       "    26   1365    40_1892   40  star wars    1892      3   \n",
       "    27   1364    40_1895   40  star wars    1895      2   \n",
       "    28   1363  40_330459   40  star wars  330459      3   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]         7.292951             0  0.500000   \n",
       "    1    [9.456276, 13.265001]         3.798236             1  0.386853   \n",
       "    2               [0.0, 0.0]        -0.517338             2  0.333333   \n",
       "    3               [0.0, 0.0]        -0.517338             3  0.301030   \n",
       "    4               [0.0, 0.0]        -0.517338             4  0.278943   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]        -0.517338            25  0.173765   \n",
       "    26        [0.0, 2.4547963]        -0.517338            26  0.172195   \n",
       "    27    [6.487482, 2.062405]        -0.517338            27  0.170707   \n",
       "    28   [7.2694716, 4.237955]        -0.517338            28  0.169294   \n",
       "    29              [0.0, 0.0]        -0.517338             1  0.386853   \n",
       "\n",
       "            gain  train_dcg        dcg    lambda    weight  \n",
       "qid                                                         \n",
       "1   0   8.000000  13.741487  13.741487  0.100504  0.099688  \n",
       "    1   3.094822  13.741487  13.741487  0.697540  0.740546  \n",
       "    2   0.333333  13.741487  13.741487 -0.005952  0.005887  \n",
       "    3   0.602060  13.741487  13.741487  2.992312  1.507942  \n",
       "    4   0.278943  13.741487  13.741487 -0.022346  0.016692  \n",
       "...          ...        ...        ...       ...       ...  \n",
       "40  25  1.390123  11.668802  10.793185 -0.002116  0.002115  \n",
       "    26  1.377563  11.668802  10.793185 -0.002126  0.002125  \n",
       "    27  0.682829  11.668802  10.793185 -0.002136  0.002135  \n",
       "    28  1.354350  11.668802  10.793185 -0.002145  0.002144  \n",
       "    29  0.386853  11.668802  10.793185 -0.013697  0.013544  \n",
       "\n",
       "[1390 rows x 15 columns]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = train_dcg - dcg_after_swap\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    assert(not math.isnan(query_judgments.loc[better, 'last_prediction']))\n",
    "                    assert(not math.isnan(query_judgments.loc[worse, 'last_prediction']))\n",
    "                    \n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff))   \n",
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=4)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label each row with its unique prediction (ie tree path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "lambdas_per_query['path'] = tree_paths(tree3, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010101     41.871274\n",
       "1010110     17.304968\n",
       "1011000     39.105907\n",
       "1100000    129.959528\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010101     50.899615\n",
       "1010110     34.429523\n",
       "1011000     78.211815\n",
       "1100000   -163.540952\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1010101': 1.2156213554683677,\n",
       " '1010110': 1.989574489468554,\n",
       " '1011000': 2.0,\n",
       " '1100000': -1.25839909859753}"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(111.60000000000001, 190.26, 'X[0] <= 8.205\\nmse = 5.779\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(55.800000000000004, 135.9, 'mse = 0.912\\nsamples = 1246\\nvalue = -0.101'),\n",
       " Text(167.4, 135.9, 'X[0] <= 8.242\\nmse = 47.043\\nsamples = 144\\nvalue = 0.873'),\n",
       " Text(111.60000000000001, 81.53999999999999, 'mse = 0.0\\nsamples = 1\\nvalue = 78.212'),\n",
       " Text(223.20000000000002, 81.53999999999999, 'X[0] <= 8.4\\nmse = 5.252\\nsamples = 143\\nvalue = 0.332'),\n",
       " Text(167.4, 27.180000000000007, 'mse = 53.276\\nsamples = 12\\nvalue = 2.199'),\n",
       " Text(279.0, 27.180000000000007, 'mse = 0.504\\nsamples = 131\\nvalue = 0.161')]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the original tree, and provide new value\n",
    "\n",
    "Instead of directly using the provided decision tree, we want to use our prediction for each leaf. This `OverridenDecisionTree` takes the original tree, looks up the prediction path in the original tree, but uses our values for the predicted variable instead of what's here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2583991 ,  1.21562136])"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X):\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
    "* Why is DCG performance so different from Ranklib, and it seems to wander around more (precision does this)\n",
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=10, max_leaf_nodes=10, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query.iloc[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
