{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log Elasticsearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with Elasticsearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
    "1. **Likely bug** - When using DCG, the model here seems to wander around more than Ranklib, instead of converging. I think likely this points at an underlying bug that's actively being investigated.\n",
    "2. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doug/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the Elasticsearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries...\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>183.343798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>6.189645</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>116.134366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>39.713833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>2.408240</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>30.087439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>1.115772</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>8.628473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1371</td>\n",
       "      <td>40_81899</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>81899</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-10.968332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1370</td>\n",
       "      <td>40_209276</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>209276</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.062526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1368</td>\n",
       "      <td>40_52959</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>52959</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.151815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1367</td>\n",
       "      <td>40_85783</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>85783</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.236624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.317325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       1     1_1370    1      rambo    1370      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1371   40_81899   40  star wars   81899      0   \n",
       "    26   1370  40_209276   40  star wars  209276      0   \n",
       "    27   1368   40_52959   40  star wars   52959      0   \n",
       "    28   1367   40_85783   40  star wars   85783      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount      gain        dcg  \\\n",
       "qid                                                                           \n",
       "1   0   [11.657399, 10.083591]             0  0.500000  8.000000  22.796491   \n",
       "    1         [0.0, 11.113943]             1  0.386853  6.189645  22.796491   \n",
       "    2    [6.036743, 11.113943]             2  0.333333  2.666667  22.796491   \n",
       "    3    [9.456276, 13.265001]             3  0.301030  2.408240  22.796491   \n",
       "    4          [0.0, 6.869545]             4  0.278943  1.115772  22.796491   \n",
       "...                        ...           ...       ...       ...        ...   \n",
       "40  25         [0.0, 6.868508]            25  0.173765  0.173765  21.491637   \n",
       "    26        [5.8994045, 0.0]            26  0.172195  0.172195  21.491637   \n",
       "    27           [7.2726, 0.0]            27  0.170707  0.170707  21.491637   \n",
       "    28           [7.2726, 0.0]            28  0.169294  0.169294  21.491637   \n",
       "    29              [0.0, 0.0]             8  0.231378  0.231378  21.491637   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   183.343798  \n",
       "    1   116.134366  \n",
       "    2    39.713833  \n",
       "    3    30.087439  \n",
       "    4     8.628473  \n",
       "...            ...  \n",
       "40  25  -10.968332  \n",
       "    26  -11.062526  \n",
       "    27  -11.151815  \n",
       "    28  -11.236624  \n",
       "    29  -11.317325  \n",
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
    "    ranked_list['discount'] = 1 / (1 + (np.log2(2 + ranked_list['display_rank'])))\n",
    "    ranked_list['gain'] = (2**ranked_list['grade']) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('grade', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
       "1     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "2     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "6     169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "7     170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "8     171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "9     173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "10    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "11    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "12    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "13    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "14    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "15    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "16    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "17    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "18    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "19    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "20    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "21    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "22    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "23    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "24    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "25    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "26    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "27    157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "28    156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "29    155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "30    154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "31    153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>183.343798</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.134366</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.713833</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.087439</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.628473</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>-10.968332</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-11.062526</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-11.151815</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-11.236624</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-11.317325</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
       "1   0   183.343798  [11.657399, 10.083591]\n",
       "    1   116.134366        [0.0, 11.113943]\n",
       "    2    39.713833   [6.036743, 11.113943]\n",
       "    3    30.087439   [9.456276, 13.265001]\n",
       "    4     8.628473         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -10.968332         [0.0, 6.868508]\n",
       "    26  -11.062526        [5.8994045, 0.0]\n",
       "    27  -11.151815           [7.2726, 0.0]\n",
       "    28  -11.236624           [7.2726, 0.0]\n",
       "    29  -11.317325              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([183.34379848])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.80365819])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(248.0, 308.0, 'X[0] <= 10.666\\nmse = 793.283\\nsamples = 1390\\nvalue = 0.0'),\n",
       " Text(124.0, 184.79999999999998, 'X[0] <= 9.182\\nmse = 222.998\\nsamples = 1329\\nvalue = -4.264'),\n",
       " Text(62.0, 61.599999999999966, 'mse = 107.23\\nsamples = 1301\\nvalue = -5.173'),\n",
       " Text(186.0, 61.599999999999966, 'mse = 3778.904\\nsamples = 28\\nvalue = 37.982'),\n",
       " Text(372.0, 184.79999999999998, 'X[0] <= 13.782\\nmse = 4190.964\\nsamples = 61\\nvalue = 92.903'),\n",
       " Text(310.0, 61.599999999999966, 'mse = 4938.44\\nsamples = 34\\nvalue = 72.93'),\n",
       " Text(434.0, 61.599999999999966, 'mse = 2114.76\\nsamples = 27\\nvalue = 118.054')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.453678</td>\n",
       "      <td>91.640615</td>\n",
       "      <td>183.281230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>1_223195</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>223195</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>11.453678</td>\n",
       "      <td>-0.848604</td>\n",
       "      <td>-1.697208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>1_22777</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>22777</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11.453678</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1_43189</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>43189</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>11.453678</td>\n",
       "      <td>-1.492275</td>\n",
       "      <td>-2.984550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1_318</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>11.453678</td>\n",
       "      <td>-1.657928</td>\n",
       "      <td>-3.315856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1365</td>\n",
       "      <td>40_1892</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1892</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 2.4547963]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>13.216213</td>\n",
       "      <td>-2.609877</td>\n",
       "      <td>-5.219755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1364</td>\n",
       "      <td>40_1895</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.487482, 2.062405]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>0.688782</td>\n",
       "      <td>13.216213</td>\n",
       "      <td>-3.481066</td>\n",
       "      <td>-6.962132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1363</td>\n",
       "      <td>40_330459</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>330459</td>\n",
       "      <td>3</td>\n",
       "      <td>[7.2694716, 4.237955]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>1.365658</td>\n",
       "      <td>13.216213</td>\n",
       "      <td>-2.634342</td>\n",
       "      <td>-5.268684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1362</td>\n",
       "      <td>40_12180</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>12180</td>\n",
       "      <td>2</td>\n",
       "      <td>[10.194733, 6.8944135]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>0.677175</td>\n",
       "      <td>13.216213</td>\n",
       "      <td>-3.515886</td>\n",
       "      <td>-7.031771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>13.216213</td>\n",
       "      <td>-3.532026</td>\n",
       "      <td>-7.064052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1      21   1_223195    1      rambo  223195      0   \n",
       "    2      23    1_22777    1      rambo   22777      0   \n",
       "    3      24    1_43189    1      rambo   43189      0   \n",
       "    4      25      1_318    1      rambo     318      0   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1365    40_1892   40  star wars    1892      3   \n",
       "    26   1364    40_1895   40  star wars    1895      2   \n",
       "    27   1363  40_330459   40  star wars  330459      3   \n",
       "    28   1362   40_12180   40  star wars   12180      2   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]              0.0             0  0.500000   \n",
       "    1               [0.0, 0.0]              0.0             1  0.386853   \n",
       "    2               [0.0, 0.0]              0.0             2  0.333333   \n",
       "    3               [0.0, 0.0]              0.0             3  0.301030   \n",
       "    4               [0.0, 0.0]              0.0             4  0.278943   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25        [0.0, 2.4547963]              0.0            25  0.173765   \n",
       "    26    [6.487482, 2.062405]              0.0            26  0.172195   \n",
       "    27   [7.2694716, 4.237955]              0.0            27  0.170707   \n",
       "    28  [10.194733, 6.8944135]              0.0            28  0.169294   \n",
       "    29              [0.0, 0.0]              0.0            10  0.218104   \n",
       "\n",
       "            gain        dcg     lambda       delta  \n",
       "qid                                                 \n",
       "1   0   8.000000  11.453678  91.640615  183.281230  \n",
       "    1   0.386853  11.453678  -0.848604   -1.697208  \n",
       "    2   0.333333  11.453678  -1.250000   -2.500000  \n",
       "    3   0.301030  11.453678  -1.492275   -2.984550  \n",
       "    4   0.278943  11.453678  -1.657928   -3.315856  \n",
       "...          ...        ...        ...         ...  \n",
       "40  25  1.390123  13.216213  -2.609877   -5.219755  \n",
       "    26  0.688782  13.216213  -3.481066   -6.962132  \n",
       "    27  1.365658  13.216213  -2.634342   -5.268684  \n",
       "    28  0.677175  13.216213  -3.515886   -7.031771  \n",
       "    29  0.218104  13.216213  -3.532026   -7.064052  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['delta'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                swap_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                dcg_after_swap = metric(swap_judgments, at=at)\n",
    "\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
    "                    query_judgments.loc[better, 'delta'] += delta\n",
    "                    \n",
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "                    query_judgments.loc[worse, 'delta'] -= delta\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0.0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "#\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.281230</td>\n",
       "      <td>91.640615</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.103318</td>\n",
       "      <td>-2.551659</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.165059</td>\n",
       "      <td>-2.582529</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.193199</td>\n",
       "      <td>-2.596599</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.244873</td>\n",
       "      <td>-2.622437</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.268684</td>\n",
       "      <td>-2.634342</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.827818</td>\n",
       "      <td>-2.913909</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.860098</td>\n",
       "      <td>-2.930049</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.890869</td>\n",
       "      <td>-2.945435</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.920248</td>\n",
       "      <td>-2.960124</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.948340</td>\n",
       "      <td>-2.974170</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.975240</td>\n",
       "      <td>-2.987620</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.001032</td>\n",
       "      <td>-3.000516</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.025794</td>\n",
       "      <td>-3.012897</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.049595</td>\n",
       "      <td>-3.024798</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.072498</td>\n",
       "      <td>-3.036249</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.094559</td>\n",
       "      <td>-3.047279</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.115831</td>\n",
       "      <td>-3.057916</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.593615</td>\n",
       "      <td>-2.796808</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.494807</td>\n",
       "      <td>-2.747403</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.697208</td>\n",
       "      <td>-0.848604</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.440092</td>\n",
       "      <td>-2.720046</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.984550</td>\n",
       "      <td>-1.492275</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.315856</td>\n",
       "      <td>-1.657928</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.560257</td>\n",
       "      <td>-1.780128</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003253</td>\n",
       "      <td>-0.001626</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.634880</td>\n",
       "      <td>-0.317440</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.157804</td>\n",
       "      <td>-0.578902</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.600136</td>\n",
       "      <td>-0.800068</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.510331</td>\n",
       "      <td>-2.255166</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.784964</td>\n",
       "      <td>-2.392482</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.898519</td>\n",
       "      <td>-2.449259</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.999788</td>\n",
       "      <td>-2.499894</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.090869</td>\n",
       "      <td>-2.545435</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.173390</td>\n",
       "      <td>-2.586695</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.248635</td>\n",
       "      <td>-2.624318</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.317635</td>\n",
       "      <td>-2.658818</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.381225</td>\n",
       "      <td>-2.690613</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.136362</td>\n",
       "      <td>-3.068181</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keywords  display_rank  grade  last_prediction       delta     lambda  \\\n",
       "0     rambo             0      4              0.0  183.281230  91.640615   \n",
       "21    rambo            21      3              0.0   -5.103318  -2.551659   \n",
       "23    rambo            23      3              0.0   -5.165059  -2.582529   \n",
       "24    rambo            24      2              0.0   -5.193199  -2.596599   \n",
       "25    rambo            25      4              0.0    0.000000   0.000000   \n",
       "26    rambo            26      1              0.0   -5.244873  -2.622437   \n",
       "27    rambo            27      1              0.0   -5.268684  -2.634342   \n",
       "28    rambo            28      0              0.0   -5.827818  -2.913909   \n",
       "29    rambo            29      0              0.0   -5.860098  -2.930049   \n",
       "30    rambo            30      0              0.0   -5.890869  -2.945435   \n",
       "31    rambo            31      0              0.0   -5.920248  -2.960124   \n",
       "32    rambo            32      0              0.0   -5.948340  -2.974170   \n",
       "33    rambo            33      0              0.0   -5.975240  -2.987620   \n",
       "34    rambo            34      0              0.0   -6.001032  -3.000516   \n",
       "35    rambo            35      0              0.0   -6.025794  -3.012897   \n",
       "36    rambo            36      0              0.0   -6.049595  -3.024798   \n",
       "37    rambo            37      0              0.0   -6.072498  -3.036249   \n",
       "38    rambo            38      0              0.0   -6.094559  -3.047279   \n",
       "39    rambo            39      0              0.0   -6.115831  -3.057916   \n",
       "22    rambo            22      0              0.0   -5.593615  -2.796808   \n",
       "20    rambo            20      0              0.0   -5.494807  -2.747403   \n",
       "1     rambo             1      0              0.0   -1.697208  -0.848604   \n",
       "19    rambo            19      0              0.0   -5.440092  -2.720046   \n",
       "2     rambo             2      0              0.0   -2.500000  -1.250000   \n",
       "3     rambo             3      0              0.0   -2.984550  -1.492275   \n",
       "4     rambo             4      0              0.0   -3.315856  -1.657928   \n",
       "5     rambo             5      0              0.0   -3.560257  -1.780128   \n",
       "6     rambo             6      1              0.0   -0.003253  -0.001626   \n",
       "7     rambo             7      1              0.0   -0.634880  -0.317440   \n",
       "8     rambo             8      1              0.0   -1.157804  -0.578902   \n",
       "9     rambo             9      1              0.0   -1.600136  -0.800068   \n",
       "10    rambo            40      1              0.0   -4.510331  -2.255166   \n",
       "11    rambo            11      0              0.0   -4.784964  -2.392482   \n",
       "12    rambo            12      0              0.0   -4.898519  -2.449259   \n",
       "13    rambo            13      0              0.0   -4.999788  -2.499894   \n",
       "14    rambo            14      0              0.0   -5.090869  -2.545435   \n",
       "15    rambo            15      0              0.0   -5.173390  -2.586695   \n",
       "16    rambo            16      0              0.0   -5.248635  -2.624318   \n",
       "17    rambo            17      0              0.0   -5.317635  -2.658818   \n",
       "18    rambo            18      0              0.0   -5.381225  -2.690613   \n",
       "40    rambo            10      0              0.0   -6.136362  -3.068181   \n",
       "\n",
       "                  features  \n",
       "0   [11.657399, 10.083591]  \n",
       "21   [9.456276, 13.265001]  \n",
       "23   [6.036743, 11.113943]  \n",
       "24         [0.0, 6.869545]  \n",
       "25        [0.0, 11.113943]  \n",
       "26        [0.0, 7.8627386]  \n",
       "27         [0.0, 4.563677]  \n",
       "28              [0.0, 0.0]  \n",
       "29              [0.0, 0.0]  \n",
       "30              [0.0, 0.0]  \n",
       "31              [0.0, 0.0]  \n",
       "32              [0.0, 0.0]  \n",
       "33              [0.0, 0.0]  \n",
       "34              [0.0, 0.0]  \n",
       "35              [0.0, 0.0]  \n",
       "36              [0.0, 0.0]  \n",
       "37              [0.0, 0.0]  \n",
       "38              [0.0, 0.0]  \n",
       "39              [0.0, 0.0]  \n",
       "22              [0.0, 0.0]  \n",
       "20              [0.0, 0.0]  \n",
       "1               [0.0, 0.0]  \n",
       "19              [0.0, 0.0]  \n",
       "2               [0.0, 0.0]  \n",
       "3               [0.0, 0.0]  \n",
       "4               [0.0, 0.0]  \n",
       "5               [0.0, 0.0]  \n",
       "6               [0.0, 0.0]  \n",
       "7               [0.0, 0.0]  \n",
       "8               [0.0, 0.0]  \n",
       "9               [0.0, 0.0]  \n",
       "10              [0.0, 0.0]  \n",
       "11              [0.0, 0.0]  \n",
       "12              [0.0, 0.0]  \n",
       "13              [0.0, 0.0]  \n",
       "14              [0.0, 0.0]  \n",
       "15              [0.0, 0.0]  \n",
       "16              [0.0, 0.0]  \n",
       "17              [0.0, 0.0]  \n",
       "18              [0.0, 0.0]  \n",
       "40              [0.0, 0.0]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.loc[1, :][['keywords', 'display_rank',  'grade', 'last_prediction', 'delta', 'lambda', 'features']].sort_values('last_prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([91.64061509])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.59659949])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6e66ced34d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mjudgments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftr_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mjudgments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mlambdas_per_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjudgments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_swaps_scaled_with_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mlambdas_per_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \"\"\"\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nan\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-6e66ced34d06>\u001b[0m in \u001b[0;36mcompute_swaps_scaled_with_weights\u001b[0;34m(query_judgments, axis, metric, at)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_judgments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_judgments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = abs(train_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff))   \n",
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label each row with its unique prediction (ie tree path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "lambdas_per_query['path'] = tree_paths(tree3, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010101     41.871274\n",
       "1010110     17.304968\n",
       "1011000     39.105907\n",
       "1100000    129.959528\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010101     50.899615\n",
       "1010110     34.429523\n",
       "1011000     78.211815\n",
       "1100000   -163.540952\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1010101': 1.2156213554683677,\n",
       " '1010110': 1.989574489468554,\n",
       " '1011000': 2.0,\n",
       " '1100000': -1.25839909859753}"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(111.60000000000001, 190.26, 'X[0] <= 8.205\\nmse = 5.779\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(55.800000000000004, 135.9, 'mse = 0.912\\nsamples = 1246\\nvalue = -0.101'),\n",
       " Text(167.4, 135.9, 'X[0] <= 8.242\\nmse = 47.043\\nsamples = 144\\nvalue = 0.873'),\n",
       " Text(111.60000000000001, 81.53999999999999, 'mse = 0.0\\nsamples = 1\\nvalue = 78.212'),\n",
       " Text(223.20000000000002, 81.53999999999999, 'X[0] <= 8.4\\nmse = 5.252\\nsamples = 143\\nvalue = 0.332'),\n",
       " Text(167.4, 27.180000000000007, 'mse = 53.276\\nsamples = 12\\nvalue = 2.199'),\n",
       " Text(279.0, 27.180000000000007, 'mse = 0.504\\nsamples = 131\\nvalue = 0.161')]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the original tree, and provide new value\n",
    "\n",
    "Instead of directly using the provided decision tree, we want to use our prediction for each leaf. This `OverridenDecisionTree` takes the original tree, looks up the prediction path in the original tree, but uses our values for the predicted variable instead of what's here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2583991 ,  1.21562136])"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
    "    def predict(self, X):\n",
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
    "* Why is DCG performance so different from Ranklib, and it seems to wander around more (precision does this)\n",
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "Train DCGs\n",
      "mean    10.61593108150268\n",
      "median  10.739621334025406\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    14.34966497096901\n",
      "median  13.216212771507438\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    12.357610149958038\n",
      "median  11.567827400752915\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    14.24721713178369\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    13.441858214366144\n",
      "median  12.506165082920402\n",
      "----------\n",
      "round 5\n",
      "Train DCGs\n",
      "mean    14.091782646778626\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 6\n",
      "Train DCGs\n",
      "mean    13.426222703083178\n",
      "median  12.460843740423556\n",
      "----------\n",
      "round 7\n",
      "Train DCGs\n",
      "mean    14.008223263605812\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 8\n",
      "Train DCGs\n",
      "mean    13.434549080998451\n",
      "median  12.346789866126002\n",
      "----------\n",
      "round 9\n",
      "Train DCGs\n",
      "mean    13.862740868310802\n",
      "median  12.615453090513515\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=10, max_leaf_nodes=10, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-549-2e26dd037e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambdas_per_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m282\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "lambdas_per_query.iloc[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
