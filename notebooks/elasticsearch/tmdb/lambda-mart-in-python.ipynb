{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART in Python\n",
    "\n",
    "This is an implementation of LambdaMART in Python using sklearn and pandas. This is for educational purposes. \n",
    "\n",
    "But a secondary goal in getting this into Python is to more easily hack the algorithm to try new ideas. For example, this [blog article on two-sided marketplaces](https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/), perhaps as more of an online algorithm (retiring old trees in the ensemble, adding new ones over time), perhaps with different model architectures in the ensemble (BERTy transformery things?) but all that preserve some of the nice things about LambdaMART (directly optimizing a list-wise metric)\n",
    "\n",
    "This is adapted from [RankLib](https://github.com/o19s/RankyMcRankFace/blob/master/src/ciir/umass/edu/learning/tree/LambdaMART.java#L444) based on [this paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) from Microsoft Research.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup - TheMovieDB corpus and log Elasticsearch features from TMDB](#Part-Zero---Setup---Get-TheMovieDB-Corpus-and-Log-Simple-Features) - plumbing to interact with Elasticsearch Learning to Rank to log a few basic features for our exploration\n",
    "2. [Pairwise swapping](#Part-One---Collect-pair-wise-DCG-diffs) - here we demonstrate the core operation of LambdaMART - pairwise swapping of pairs and examining DCG (or another ranking metric) impact\n",
    "3. [Scale to learn errors, not just swaps](#Part-Two---Compute-the-swaps-but-scaled-to-current-model's-error) - here we show how LambdaMART isn't just about learning the pairwise DCG difference of a swap, but the error currently in the model at predicting the DCG impact of that swap\n",
    "4. [Weigh predictions](#Part-Three---Weigh-each-leaf's-predictions) - here we weigh the predictions of the next model in the ensemble based on how much DCG remains to be learned. \n",
    "5. [Putting it all together](#Part-Four---Putting-it-all-together,-from-the-top!) - the full algorithm in one place. You can also compare this notebook's output and learning to Ranklib.\n",
    "\n",
<<<<<<< HEAD
    "---\n",
    "\n",
    "6. [A Pandas version!](#5.-Pure-Pandas-Implementation?) -- walking through a faster version computing the per-tree training data using Pandas - a much for useful toy example.\n",
    "\n",
=======
>>>>>>> 4ef4e05 (Save lambda mart)
    "## Known Issues\n",
    "\n",
    "I'm still learning the nooks and crannies of the algorithm. So there are some known issues as this is actively being developed.\n",
    "\n",
<<<<<<< HEAD
    "1. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
=======
    "1. **Likely bug** - When using DCG, the model here seems to wander around more than Ranklib, instead of converging. I think likely this points at an underlying bug that's actively being investigated.\n",
    "2. **Performance** - a single training round takes about 9 seconds. There's room for improvement in the hot part of the loop (dcg computation and swapping)"
>>>>>>> 4ef4e05 (Save lambda mart)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero - Setup - Get TheMovieDB Corpus and Log Simple Features\n",
    "\n",
    "In this step we download TheMovieDB Corpus and log some featurs (title and overview BM25). At the end we have a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 80,
=======
   "execution_count": 475,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 517,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client = ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and index TMDB corpus and training set\n",
    "\n",
    "Download [TheMovieDB](http://themoviedb.org) corpus and small toy training set with 40 queries labeled."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 81,
=======
   "execution_count": 476,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 518,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmdb.json already exists\n",
      "data/title_judgments.txt already exists\n",
      "Index tmdb already exists. Use `force = True` to delete and recreate\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two features - title & overview\n",
    "\n",
    "Using the Elasticsearch Learning to Rank plugin, we:\n",
    "\n",
    "1. Log two features: title and overview bm25\n",
    "2. Create a pandas dataframe containing the labels and features"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 82,
=======
   "execution_count": 477,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 519,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create movies feature set [Status: 201]\n",
      "Recognizing 40 queries...\n"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr.judgments import to_dataframe\n",
    "\n",
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\"validation\": {\n",
    "              \"index\": \"tmdb\",\n",
    "              \"params\": {\n",
    "                  \"keywords\": \"rambo\"\n",
    "              }\n",
    "    \n",
    "           },\n",
    "           \"featureset\": {\n",
    "            \"features\": [\n",
    "                { #1\n",
    "                    \"name\": \"title_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"title\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                },\n",
    "                { #2\n",
    "                    \"name\": \"overview_bm25\",\n",
    "                    \"params\": [\"keywords\"],\n",
    "                    \"template\": {\n",
    "                        \"match\": {\"overview\": \"{{keywords}}\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "    }}\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='movies', ftr_config=config)\n",
    "\n",
    "# Log features for each query\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='movies')\n",
    "with judgments_open('data/title_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))\n",
    "        \n",
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine judgments dataframe\n",
    "\n",
    "In the dataframe we have a set of (query, document, grade) that label how relevant a document (movie) is for each query.\n",
    "\n",
    "* qid - 'query id' - a unique identifier for this query\n",
    "* docId - an identifier for the document (here movie) being labeled\n",
    "* grade - how relevant a movie is on a 0-4 scale\n",
    "* keywords - the query keywords that go along with the query id\n",
    "* features - the two features we logged, 0th is title_bm25, 1st is overview_bm25"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 83,
=======
   "execution_count": 478,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 520,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            uid  qid   keywords   docId  grade                features\n",
       "0        1_7555    1      rambo    7555      4  [11.657399, 10.083591]\n",
       "1        1_1370    1      rambo    1370      3   [9.456276, 13.265001]\n",
       "2        1_1369    1      rambo    1369      3   [6.036743, 11.113943]\n",
       "3       1_13258    1      rambo   13258      2         [0.0, 6.869545]\n",
       "4        1_1368    1      rambo    1368      4        [0.0, 11.113943]\n",
       "...         ...  ...        ...     ...    ...                     ...\n",
       "1385   40_37079   40  star wars   37079      0              [0.0, 0.0]\n",
       "1386  40_126757   40  star wars  126757      0              [0.0, 0.0]\n",
       "1387   40_39797   40  star wars   39797      0              [0.0, 0.0]\n",
       "1388   40_18112   40  star wars   18112      0              [0.0, 0.0]\n",
       "1389   40_43052   40  star wars   43052      0              [0.0, 0.0]\n",
       "\n",
       "[1390 rows x 6 columns]"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 83,
=======
     "execution_count": 478,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 520,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One - Collect pair-wise DCG diffs\n",
    "\n",
    "The first-pass iteration of LambdaMART, for each query, we examine the DCG\\* impact of swapping each result with another result in the listing.\n",
    "\n",
    "\\* replace DCG with your metric of interest: MAP, Precision@N, etc"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 84,
=======
   "execution_count": 480,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 521,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>427.587115</td>\n",
=======
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>183.343798</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>1</td>\n",
<<<<<<< HEAD
       "      <td>0.630930</td>\n",
       "      <td>9.463946</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>219.800566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>62.204093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
=======
       "      <td>0.386853</td>\n",
       "      <td>6.189645</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>116.134366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
<<<<<<< HEAD
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>3.014736</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>43.694734</td>\n",
=======
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>39.713833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>2.408240</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>30.087439</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>4</td>\n",
<<<<<<< HEAD
       "      <td>0.386853</td>\n",
       "      <td>1.160558</td>\n",
       "      <td>33.734341</td>\n",
       "      <td>5.542298</td>\n",
=======
       "      <td>0.278943</td>\n",
       "      <td>1.115772</td>\n",
       "      <td>22.796491</td>\n",
       "      <td>8.628473</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
<<<<<<< HEAD
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.598524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.715586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.826142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-20.930783</td>\n",
=======
       "      <td>1371</td>\n",
       "      <td>40_81899</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>81899</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-10.968332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1370</td>\n",
       "      <td>40_209276</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>209276</td>\n",
       "      <td>0</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.062526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1368</td>\n",
       "      <td>40_52959</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>52959</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.151815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1367</td>\n",
       "      <td>40_85783</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>85783</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.236624</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>8</td>\n",
<<<<<<< HEAD
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.225149</td>\n",
       "      <td>-21.030027</td>\n",
=======
       "      <td>0.231378</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>21.491637</td>\n",
       "      <td>-11.317325</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       4     1_1368    1      rambo    1368      4   \n",
<<<<<<< HEAD
       "    2       1     1_1370    1      rambo    1370      3   \n",
       "    3       2     1_1369    1      rambo    1369      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount       gain        dcg  \\\n",
       "qid                                                                            \n",
       "1   0   [11.657399, 10.083591]             0  1.000000  15.000000  33.734341   \n",
       "    1         [0.0, 11.113943]             1  0.630930   9.463946  33.734341   \n",
       "    2    [9.456276, 13.265001]             2  0.500000   3.500000  33.734341   \n",
       "    3    [6.036743, 11.113943]             3  0.430677   3.014736  33.734341   \n",
       "    4          [0.0, 6.869545]             4  0.386853   1.160558  33.734341   \n",
       "...                        ...           ...       ...        ...        ...   \n",
       "40  25              [0.0, 0.0]            25  0.210310   0.000000  31.225149   \n",
       "    26              [0.0, 0.0]            26  0.208015   0.000000  31.225149   \n",
       "    27              [0.0, 0.0]            27  0.205847   0.000000  31.225149   \n",
       "    28              [0.0, 0.0]            28  0.203795   0.000000  31.225149   \n",
       "    29              [0.0, 0.0]             8  0.301030   0.000000  31.225149   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   427.587115  \n",
       "    1   219.800566  \n",
       "    2    62.204093  \n",
       "    3    43.694734  \n",
       "    4     5.542298  \n",
       "...            ...  \n",
       "40  25  -20.598524  \n",
       "    26  -20.715586  \n",
       "    27  -20.826142  \n",
       "    28  -20.930783  \n",
       "    29  -21.030027  \n",
=======
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       1     1_1370    1      rambo    1370      3   \n",
       "    4       3    1_13258    1      rambo   13258      2   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1371   40_81899   40  star wars   81899      0   \n",
       "    26   1370  40_209276   40  star wars  209276      0   \n",
       "    27   1368   40_52959   40  star wars   52959      0   \n",
       "    28   1367   40_85783   40  star wars   85783      0   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  display_rank  discount      gain        dcg  \\\n",
       "qid                                                                           \n",
       "1   0   [11.657399, 10.083591]             0  0.500000  8.000000  22.796491   \n",
       "    1         [0.0, 11.113943]             1  0.386853  6.189645  22.796491   \n",
       "    2    [6.036743, 11.113943]             2  0.333333  2.666667  22.796491   \n",
       "    3    [9.456276, 13.265001]             3  0.301030  2.408240  22.796491   \n",
       "    4          [0.0, 6.869545]             4  0.278943  1.115772  22.796491   \n",
       "...                        ...           ...       ...       ...        ...   \n",
       "40  25         [0.0, 6.868508]            25  0.173765  0.173765  21.491637   \n",
       "    26        [5.8994045, 0.0]            26  0.172195  0.172195  21.491637   \n",
       "    27           [7.2726, 0.0]            27  0.170707  0.170707  21.491637   \n",
       "    28           [7.2726, 0.0]            28  0.169294  0.169294  21.491637   \n",
       "    29              [0.0, 0.0]             8  0.231378  0.231378  21.491637   \n",
       "\n",
       "            lambda  \n",
       "qid                 \n",
       "1   0   183.343798  \n",
       "    1   116.134366  \n",
       "    2    39.713833  \n",
       "    3    30.087439  \n",
       "    4     8.628473  \n",
       "...            ...  \n",
       "40  25  -10.968332  \n",
       "    26  -11.062526  \n",
       "    27  -11.151815  \n",
       "    28  -11.236624  \n",
       "    29  -11.317325  \n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "\n",
       "[1390 rows x 12 columns]"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 84,
=======
     "execution_count": 480,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 521,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "import numpy as np \n",
    "\n",
    "def rank_with_swap(ranked_list, rank1=0, rank2=0):\n",
    "    \"\"\" Set the display rank of positions given the provided swap \"\"\"\n",
    "    ranked_list['display_rank'] = ranked_list.index.to_series()\n",
    "    \n",
    "    if rank1 != rank2:\n",
    "        ranked_list.loc[rank1, 'display_rank'] = rank2\n",
    "        ranked_list.loc[rank2, 'display_rank'] = rank1\n",
    "    return ranked_list\n",
    "    \n",
    "\n",
    "def dcg(ranked_list, at=10):\n",
    "    \"\"\"Given a list, compute DCG -- \n",
    "       uses same variant as lambdamart 2**grade / log2(displayrank)\n",
    "    \"\"\"\n",
<<<<<<< HEAD
    "    ranked_list['discount'] = 1 / np.log2(2 + ranked_list['display_rank'])\n",
    "    ranked_list['gain'] = (2**ranked_list['grade'] - 1) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
=======
    "    ranked_list['discount'] = 1 / (1 + (np.log2(2 + ranked_list['display_rank'])))\n",
    "    ranked_list['gain'] = (2**ranked_list['grade']) * ranked_list['discount'] # TODO - precompute gain on swapping\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "    return sum(ranked_list['gain'].head(at))\n",
    "\n",
    "def compute_swaps(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
<<<<<<< HEAD
    "    query_judgments = query_judgments.sort_values('grade', kind='stable', ascending=False).reset_index()\n",
=======
    "    query_judgments = query_judgments.sort_values('grade', ascending=False).reset_index()\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    \n",
    "    # TODO - redo inner body as \n",
    "    for better in range(0,len(query_judgments)):\n",
<<<<<<< HEAD
    "        for worse in range(0,len(query_judgments)):\n",
=======
    "        for worse in range(better+1,len(query_judgments)):\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "            if better > at and worse > at:\n",
    "                break\n",
    "\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
<<<<<<< HEAD
    "                delta = abs(best_dcg - dcg_after_swap)\n",
=======
    "                delta = best_dcg - dcg_after_swap\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "\n",
    "                if delta > 0.0:\n",
    "\n",
    "                    # Add delta to better's lambda (-delta to worse's lambda)\n",
    "                    query_judgments.loc[better, 'lambda'] += delta\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta\n",
    "\n",
    "    # print(query_judgments[['keywords', 'docId', 'grade', 'lambda', 'features']])\n",
    "    return query_judgments\n",
    "\n",
    "# For each query, compute lambdas\n",
    "# %prun -s cumulative lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "# judgments\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps, axis=1)\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Precision instead of DCG\n",
    "\n",
    "We can really use any ranking metric to achieve goals important to our product. This includes potentially ones we invent or come up with ourselves!"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 85,
=======
   "execution_count": 481,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 522,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>5_603</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>603</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.040129]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
<<<<<<< HEAD
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
=======
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>151</td>\n",
       "      <td>5_605</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>605</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 0.0]</td>\n",
<<<<<<< HEAD
=======
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>5_604</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>604</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 9.392262]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>5_55931</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>55931</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 10.798681]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>5_73262</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>73262</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>32</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
<<<<<<< HEAD
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
=======
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
<<<<<<< HEAD
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
=======
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
<<<<<<< HEAD
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
=======
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
<<<<<<< HEAD
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
=======
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
<<<<<<< HEAD
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
=======
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
<<<<<<< HEAD
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
=======
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
<<<<<<< HEAD
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
=======
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
<<<<<<< HEAD
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
=======
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
<<<<<<< HEAD
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
=======
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
<<<<<<< HEAD
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
=======
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
<<<<<<< HEAD
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
=======
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
<<<<<<< HEAD
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
=======
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
<<<<<<< HEAD
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
=======
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
<<<<<<< HEAD
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
=======
       "      <td>165</td>\n",
       "      <td>5_213110</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>213110</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
<<<<<<< HEAD
       "      <td>167</td>\n",
       "      <td>5_104221</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>104221</td>\n",
=======
       "      <td>166</td>\n",
       "      <td>5_13805</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13805</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
<<<<<<< HEAD
       "      <td>168</td>\n",
       "      <td>5_183894</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>183894</td>\n",
=======
       "      <td>164</td>\n",
       "      <td>5_11253</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>11253</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
<<<<<<< HEAD
       "      <td>169</td>\n",
       "      <td>5_3573</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>3573</td>\n",
=======
       "      <td>163</td>\n",
       "      <td>5_72867</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>72867</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
<<<<<<< HEAD
       "      <td>170</td>\n",
       "      <td>5_12254</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>12254</td>\n",
=======
       "      <td>162</td>\n",
       "      <td>5_1487</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1487</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>22</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
<<<<<<< HEAD
       "      <td>171</td>\n",
       "      <td>5_17813</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17813</td>\n",
=======
       "      <td>161</td>\n",
       "      <td>5_124080</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>124080</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>23</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
<<<<<<< HEAD
       "      <td>173</td>\n",
       "      <td>5_17960</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>17960</td>\n",
=======
       "      <td>160</td>\n",
       "      <td>5_56441</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>56441</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
<<<<<<< HEAD
       "      <td>174</td>\n",
       "      <td>5_33068</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>33068</td>\n",
=======
       "      <td>159</td>\n",
       "      <td>5_125607</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>125607</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>25</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
<<<<<<< HEAD
       "      <td>175</td>\n",
       "      <td>5_28377</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28377</td>\n",
=======
       "      <td>158</td>\n",
       "      <td>5_21208</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21208</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
<<<<<<< HEAD
       "      <td>176</td>\n",
       "      <td>5_13300</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>13300</td>\n",
=======
       "      <td>157</td>\n",
       "      <td>5_181886</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>181886</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
<<<<<<< HEAD
       "      <td>177</td>\n",
       "      <td>5_680</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
=======
       "      <td>156</td>\n",
       "      <td>5_21874</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>21874</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
<<<<<<< HEAD
       "      <td>178</td>\n",
       "      <td>5_28131</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>28131</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
=======
       "      <td>155</td>\n",
       "      <td>5_4247</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>4247</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 8.114125]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>29</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
<<<<<<< HEAD
       "      <td>179</td>\n",
       "      <td>5_37988</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>37988</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
=======
       "      <td>154</td>\n",
       "      <td>5_10999</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>10999</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 11.466951]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
<<<<<<< HEAD
       "      <td>180</td>\n",
       "      <td>5_18451</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>18451</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
=======
       "      <td>153</td>\n",
       "      <td>5_1857</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>1857</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 9.65805]</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>181</td>\n",
       "      <td>5_75404</td>\n",
       "      <td>5</td>\n",
       "      <td>matrix</td>\n",
       "      <td>75404</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index       uid  qid keywords   docId  grade                features  \\\n",
       "0     149     5_603    5   matrix     603      4  [11.657399, 10.040129]   \n",
<<<<<<< HEAD
       "1     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "2     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
       "6     154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "7     155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "8     156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "9     157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "10    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "11    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "12    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "13    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "14    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "15    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "16    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "17    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "18    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "19    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "20    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "21    169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "22    170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "23    171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "24    173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "25    174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "26    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "27    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "28    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "29    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "30    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "31    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
=======
       "1     151     5_605    5   matrix     605      3         [9.456276, 0.0]   \n",
       "2     150     5_604    5   matrix     604      3    [9.456276, 9.392262]   \n",
       "3     152   5_55931    5   matrix   55931      2        [0.0, 10.798681]   \n",
       "4     172   5_73262    5   matrix   73262      1              [0.0, 0.0]   \n",
       "5     174   5_33068    5   matrix   33068      0              [0.0, 0.0]   \n",
       "6     169    5_3573    5   matrix    3573      0              [0.0, 0.0]   \n",
       "7     170   5_12254    5   matrix   12254      0              [0.0, 0.0]   \n",
       "8     171   5_17813    5   matrix   17813      0              [0.0, 0.0]   \n",
       "9     173   5_17960    5   matrix   17960      0              [0.0, 0.0]   \n",
       "10    175   5_28377    5   matrix   28377      0              [0.0, 0.0]   \n",
       "11    167  5_104221    5   matrix  104221      0              [0.0, 0.0]   \n",
       "12    176   5_13300    5   matrix   13300      0              [0.0, 0.0]   \n",
       "13    177     5_680    5   matrix     680      0              [0.0, 0.0]   \n",
       "14    178   5_28131    5   matrix   28131      0              [0.0, 0.0]   \n",
       "15    179   5_37988    5   matrix   37988      0              [0.0, 0.0]   \n",
       "16    180   5_18451    5   matrix   18451      0              [0.0, 0.0]   \n",
       "17    168  5_183894    5   matrix  183894      0              [0.0, 0.0]   \n",
       "18    165  5_213110    5   matrix  213110      0              [0.0, 0.0]   \n",
       "19    166   5_13805    5   matrix   13805      0              [0.0, 0.0]   \n",
       "20    164   5_11253    5   matrix   11253      0              [0.0, 0.0]   \n",
       "21    163   5_72867    5   matrix   72867      0              [0.0, 0.0]   \n",
       "22    162    5_1487    5   matrix    1487      0              [0.0, 0.0]   \n",
       "23    161  5_124080    5   matrix  124080      0              [0.0, 0.0]   \n",
       "24    160   5_56441    5   matrix   56441      0              [0.0, 0.0]   \n",
       "25    159  5_125607    5   matrix  125607      0              [0.0, 0.0]   \n",
       "26    158   5_21208    5   matrix   21208      0              [0.0, 0.0]   \n",
       "27    157  5_181886    5   matrix  181886      0              [0.0, 0.0]   \n",
       "28    156   5_21874    5   matrix   21874      0        [0.0, 7.8627386]   \n",
       "29    155    5_4247    5   matrix    4247      0         [0.0, 8.114125]   \n",
       "30    154   5_10999    5   matrix   10999      0        [0.0, 11.466951]   \n",
       "31    153    5_1857    5   matrix    1857      0          [0.0, 9.65805]   \n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "32    181   5_75404    5   matrix   75404      0              [0.0, 0.0]   \n",
       "\n",
       "    display_rank  dcg  lambda  \n",
       "0              0  0.3   2.300  \n",
       "1              1  0.3   1.725  \n",
       "2              2  0.3   1.725  \n",
       "3              3  0.3   1.150  \n",
       "4             32  0.3   0.575  \n",
       "5              5  0.3   0.000  \n",
       "6              6  0.3   0.000  \n",
       "7              7  0.3   0.000  \n",
       "8              8  0.3   0.000  \n",
       "9              9  0.3   0.000  \n",
       "10            10  0.3  -0.325  \n",
       "11            11  0.3  -0.325  \n",
       "12            12  0.3  -0.325  \n",
       "13            13  0.3  -0.325  \n",
       "14            14  0.3  -0.325  \n",
       "15            15  0.3  -0.325  \n",
       "16            16  0.3  -0.325  \n",
       "17            17  0.3  -0.325  \n",
       "18            18  0.3  -0.325  \n",
       "19            19  0.3  -0.325  \n",
       "20            20  0.3  -0.325  \n",
       "21            21  0.3  -0.325  \n",
       "22            22  0.3  -0.325  \n",
       "23            23  0.3  -0.325  \n",
       "24            24  0.3  -0.325  \n",
       "25            25  0.3  -0.325  \n",
       "26            26  0.3  -0.325  \n",
       "27            27  0.3  -0.325  \n",
       "28            28  0.3  -0.325  \n",
       "29            29  0.3  -0.325  \n",
       "30            30  0.3  -0.325  \n",
       "31            31  0.3  -0.325  \n",
       "32             4  0.3  -0.325  "
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 85,
=======
     "execution_count": 481,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 522,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(ranked_list, max_grade=4.0, at=10):\n",
    "    \"\"\"Given a list, compute simple precision. Really this is cumalitive gain.\"\"\"\n",
    "    above_n = ranked_list[ranked_list['display_rank'] < at]\n",
    "    \n",
    "    if (max_grade * at) == 0.0:\n",
    "        print(\"0\")\n",
    "        return 0.0\n",
    "    \n",
    "    return float(sum(above_n['grade'])) / (max_grade * at)\n",
    "\n",
    "\n",
    "lambdas_per_query_prec = judgments.groupby('qid').apply(compute_swaps, axis=1, metric=precision)\n",
    "lambdas_per_query_prec.loc[5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on the lambdas\n",
    "\n",
    "The core operation is fitting an operation on the lambdas (the accumulated pairwise differences)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 86,
=======
   "execution_count": 482,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 523,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
<<<<<<< HEAD
       "      <td>427.587115</td>\n",
=======
       "      <td>183.343798</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
<<<<<<< HEAD
       "      <td>219.800566</td>\n",
=======
       "      <td>116.134366</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
<<<<<<< HEAD
       "      <td>62.204093</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.694734</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.542298</td>\n",
=======
       "      <td>39.713833</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.087439</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.628473</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
<<<<<<< HEAD
       "      <td>-20.598524</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-20.715586</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-20.826142</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-20.930783</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-21.030027</td>\n",
=======
       "      <td>-10.968332</td>\n",
       "      <td>[0.0, 6.868508]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-11.062526</td>\n",
       "      <td>[5.8994045, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-11.151815</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-11.236624</td>\n",
       "      <td>[7.2726, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-11.317325</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lambda                features\n",
       "qid                                       \n",
<<<<<<< HEAD
       "1   0   427.587115  [11.657399, 10.083591]\n",
       "    1   219.800566        [0.0, 11.113943]\n",
       "    2    62.204093   [9.456276, 13.265001]\n",
       "    3    43.694734   [6.036743, 11.113943]\n",
       "    4     5.542298         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -20.598524              [0.0, 0.0]\n",
       "    26  -20.715586              [0.0, 0.0]\n",
       "    27  -20.826142              [0.0, 0.0]\n",
       "    28  -20.930783              [0.0, 0.0]\n",
       "    29  -21.030027              [0.0, 0.0]\n",
=======
       "1   0   183.343798  [11.657399, 10.083591]\n",
       "    1   116.134366        [0.0, 11.113943]\n",
       "    2    39.713833   [6.036743, 11.113943]\n",
       "    3    30.087439   [9.456276, 13.265001]\n",
       "    4     8.628473         [0.0, 6.869545]\n",
       "...            ...                     ...\n",
       "40  25  -10.968332         [0.0, 6.868508]\n",
       "    26  -11.062526        [5.8994045, 0.0]\n",
       "    27  -11.151815           [7.2726, 0.0]\n",
       "    28  -11.236624           [7.2726, 0.0]\n",
       "    29  -11.317325              [0.0, 0.0]\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "\n",
       "[1390 rows x 2 columns]"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 86,
=======
     "execution_count": 482,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 523,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 87,
=======
   "execution_count": 483,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 524,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 87,
=======
     "execution_count": 483,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 524,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG-based Lambda Predictions\n",
    "\n",
    "We show predicting some known examples. In the first case, strong title and overview scores. In the second case, no title or overview scores"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 88,
=======
   "execution_count": 484,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 525,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([445.85277429])"
      ]
     },
     "execution_count": 88,
=======
       "array([183.34379848])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 484,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 525,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "tree.predict([[11.1, 10.08]])"
=======
    "tree.predict([[11.6, 10.08]])"
>>>>>>> 4ef4e05 (Save lambda mart)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 89,
=======
   "execution_count": 485,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 526,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([-15.3987952])"
      ]
     },
     "execution_count": 89,
=======
       "array([-6.80365819])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 485,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 526,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more typical we would restrict the complexity of each tree in the ensemble. We can dump the tree see [understanding sklearn's tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 90,
=======
   "execution_count": 379,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 527,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[Text(167.4, 181.2, 'X[0] <= 10.666\\nmse = 4161.543\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 1038.341\\nsamples = 1329\\nvalue = -9.745'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 421.042\\nsamples = 1301\\nvalue = -11.724'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 21086.54\\nsamples = 28\\nvalue = 82.191'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 18.186\\nmse = 25061.546\\nsamples = 61\\nvalue = 212.311'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 26150.423\\nsamples = 51\\nvalue = 188.147'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 1343.167\\nsamples = 10\\nvalue = 335.547')]"
      ]
     },
     "execution_count": 90,
=======
       "[Text(167.4, 181.2, 'X[0] <= 10.666\\nmse = 793.283\\nsamples = 1390\\nvalue = 0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 222.998\\nsamples = 1329\\nvalue = -4.264'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 107.23\\nsamples = 1301\\nvalue = -5.173'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 3778.904\\nsamples = 28\\nvalue = 37.982'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 13.782\\nmse = 4190.964\\nsamples = 61\\nvalue = 92.903'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 4938.44\\nsamples = 34\\nvalue = 72.93'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 2114.76\\nsamples = 27\\nvalue = 118.054')]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 379,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 527,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
<<<<<<< HEAD
      "image/png": "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\n",
=======
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOyde1hU1fr4P1tDQSlFs46U5km8hZYXVG4ywyUveAsVE000NfNCWR0vaeWlLD3RsU5oHkvtYHZI64gRlvo9CZqZHlNPKfLLW+Ql74B4AYbL+/tjZMcwM4qKzAjr8zzrgVmz9t7vXvPOO2uv9b7r1UQEhUKhUFQONRwtgEKhUFQnlNFVKBSKSkQZXYVCoahElNFVKBSKSkQZXYVCoahElNFVKBSKSkQZXYVCoahElNFVKBSKSkQZXYVCoahElNFVKBSKSkQZXYVCoahElNFVKBSKSkQZXYVCoahElNFVKBSKSkQZXYVCoahElNFVKBSKSkQZXYVCoahElNFVKBSKSuQuRwugcE7c3NxO5eXl3e9oOe50XF1dT+fm5v7J0XIonAdN5UhT2ELTNFG6cetomoaIaI6WQ+E8qOkFhUKhqESU0VUoFIpKRBldhUKhqESU0VWUi0OHDtGpUydMJhMAsbGxzJw5EwA3Nzf69Omjt42NjSUgIIBu3bqxd+9eALZu3Ur79u0ZM2bMbZMxLi6Oli1b4uXlZVH/7bff4ufnh5+fHwkJCTaP3bRpE2FhYQQHBzN9+nS9/rPPPiM0NJTg4GD+/ve/6/Xvvvuu3n7VqlW354YUVRMRUUUVq2JWDUvefPNNmT17thw5ckQ6duwoeXl5IiLSvHlzvc0vv/wiRqNRiouLJT09XYxGo/5eSkqKjB492uq8trh48WK52pXm1KlTYjKZLOQpLCyURx99VM6dOydXrlyRxx57THJyciyOO3funISHh+v3U8L+/ftl6NChUlRUZFG/fv16mTp1arlkutqPDv88VXGeoka6inIzZcoUkpOTiYqK4r333qN27dpWbVJSUujXrx+aptG6dWvOnj1LYWFhuc5vMplITEwkMjKSYcOG3bB8999/Py4uLhZ1hw4dolmzZjRs2BA3Nzf8/f3ZuXOnRZt169bRqFEjnnjiCcLCwti+fTsAn3/+OfXq1aNnz5707t2bX375BYBVq1ZRVFREWFgYkZGRnDp16oZlVVRflJ+uoty4uLgQFBREcnIygYGBNtucP38eT09P/XW9evXIysqiUaNGds+7Y8cOli9fzqFDh+jZsycLFiygSZMmABw9epTo6GirYwYNGkRMTMx1ZT5//jweHh76aw8PD86fP2/R5sSJE6Snp7N582ZOnz5Nr1692L9/PydOnODMmTOsX7+eXbt28eyzz5KamsqJEye4++67+c9//sMXX3zB1KlTWbFixXVlUShAGV3FDZCWlsb3339PWFgYH330EWPHjrVq07BhQ7KysvTXOTk5FkbPFklJSWzfvp2YmBgGDhxIgwYN9PeaNm1KamrqTctcVp7s7GwaNmxo0aZBgwYEBwfj6urKQw89xD333ENmZiYNGjSgQ4cO1KhRg86dO3PixAm9fa9evQDo06cPr7/++k3Lp6h+qOkFRbkoLi5m3LhxLFq0iPnz5xMXF8fp06et2hmNRtatW4eIcPDgQRo2bMhdd137t/3NN9/k+++/p3bt2kRHR/PEE0/oi1NHjx7FaDRalYULF5ZLbi8vLzIyMsjKyiI/P59t27bh4+Nj0SY4OJjdu3cjImRnZ5OVlYWHhwehoaH8+OOPABw5ckT/MShdv2PHDlq2bFkuWRQKQC2kqWK7UGYhbeHChTJp0iT9dWJiogwZMkRELBfSRETmz58v/v7+EhAQIP/73//0+vIupJ08eVKWLFly3XZlSUhIkNDQUHFzc5PQ0FDZsmWLiIhs3LhRfH19xdfXV1auXKm3Hzp0qP7/ggULpFu3btKlSxdJTk4WEZHi4mKZMmWKGAwG8fPzkx07doiISH5+vjz99NNiNBrFYDDIwYMH7cqEWkhTpUxRYcAKm9xIGHCLFi1o1aoVycnJdtts3bqVyZMnYzQamT9/fkWJ6fSoMGBFWZTRVdhE7b1QMSijqyiLmtNVKBSKSkQZXcUdxYcffqgvprVu3ZqBAwcCcODAAYxGI8HBwUyZMkVvP3DgQAwGAz4+Prz77rtW5/vhhx/w9/fHYDAQEhLCkSNHAMjKyqJ79+4YDAb8/f3Zs2cPYF70CwoKIiAggOjoaAoKCirhrhVVCkdPKqvinAUbEWnOxjPPPCOrVq0SEZH+/fvLDz/8ICIio0ePlk2bNomIedFLRKSgoEC8vLysotFOnDghly5dEhGRdevWyVNPPSUiInFxcTJ79mwREfnuu+9kwIABFucTERk+fLi+6GYP1EKaKmWKGukqyk1GRgY+Pj5ER0fz2GOP8c477zBp0iT8/f2JiorS2wQEBBAcHIzRaCQrK4sLFy4wePBgQkJCCA4O5sCBA7csS15eHhs3bqRfv36AeaRb4grm4+NDSkoKALVq1QLgypUrNG3alDp16licx9PTk7p16wJQu3ZtatasCUCbNm3IyckBIDMzk/vuu8/ifMXFxRQWFlrt86BQXBdHW31VnLNgY6T766+/SuPGjeXy5cuSm5sr7u7usmfPHhERCQ0NlfT0dFm2bJnMmjVLP6a4uFimTZsmCQkJIiKyb98+6d+/v9W5x48fLwaDwaL07t3bql0Jq1evtnA/i4yMlK+++kqKi4slIiJCJk6cqL/Xr18/adSokcycOdPu+S5duiR+fn66i1tmZqb4+fmJt7e3PPDAA3L48GG97cyZM6V58+bSq1cvuXz5st1zioga6apiVRwugCrOWewZ3ZCQEP31ww8/rP8/YsQI2bp1q1y8eFGmT58uQ4cOlenTp0t+fr6Eh4eLr6+vbkxLb4Jzs/Tp00dSU1P110ePHpV+/fpJWFiYjB07Vt58802L9pcuXZKOHTtKWlqa1bny8vKkR48ekpSUpNe9/PLLEhsbKyIiP/zwg/Ts2dPimOLiYhk/frwsWrTomnIqo6tK2aLCgBU3hKZpNv8H8w94jRo1eOuttwAYNWoUGzZswNvbGz8/PyIiIgD07SFLM2HCBPbv329R5+7ubtP39+zZs6SnpxMUFKTXNWnShC+//BIRITo6moiICIqLiykqKsLFxQU3Nze9lKawsJAhQ4YQFRVF3759Ld4r2S+iUaNGZGdnA+ZpDVdXVzRNo169elbTFQrF9VBGV1GhJCcnExcXR82aNalduzaBgYEEBQUxbtw44uLiAOjRowfTpk2zOO6DDz4o9zU+++wzBg8ebGH0//Wvf/HRRx8BMHLkSNq0acPFixd1Q5qfn8+QIUP485//DMCwYcP49NNPWblyJd9++y1ZWVl8/PHHPPLII3zwwQc899xzDB8+nI8//pjc3Fz++te/AuYfhyNHjlBUVETLli3VvguKG0YFRyhsooIjKgYVHKEoi/JeUCgUikpEGV2FQqGoRJTRVTgNleXzunXrVtq1a4erqyvHjx/X62NiYjAYDHTp0oWpU6fq9UuWLKFr165069bNYm9fW7ngFIrr4mj3CVWcs+CAiLSyW0TeLrKzs+XixYtiMBjk2LFjen3paLOgoCDZt2+fnD59Wjp06CAmk0mys7OlU6dOUlRUdM1ccKVBuYypUqYo7wXFdcnIyGDYsGHUqlULESExMZG9e/cya9YsCgsL8fDwYNWqVbi5uWE0GunQoQP79+8nPz+fsWPHEh8fz+nTp1m9ejUtW7bEaDTi7e3NgQMHKC4uJiEhQY/4AigoKGDChAkcPnwYk8lEbGwsfn5+zJ07l6SkJNzd3enTpw8vvfTSTd1PvXr1bNaXRJuZTCbq1KmDp6cnBw8e5JFHHsHFxYV69epx1113kZGRYTcX3PU2bFco1PSC4rps2rSJxx9/nJSUFFJTU6lfvz6dOnUiJSWF7777jjZt2rB69Wq9vcFgYMOGDXh5ebFz5042bNjA5MmTWb58ud6ma9eu/N///R/Dhg0jNjbW4nrLli2jefPmbNq0icTERN24fvrpp6SkpLBp0yZeeOEFKzkHDBhglWHiRlO+P/vsszz88MN4enpSr149mjdvzp49e8jJyeH48eOkpaWRmZlplXutJBecQnE91M+y4roMHjyYt956i2HDhvHQQw8xe/Zs0tLSePXVV8nPz+f06dPcc889evtOnToB8OCDD9K8eXP9/82bN+tt/P399b+JiYkW19u7dy/btm1j/fr1AHpgwsKFC5k4cSKFhYWMGzfOKjnmmjVrbvlelyxZQkFBAQMGDGD9+vWEh4cze/Zs+vTpQ+PGjWnfvj2enp43lQtOoQBldBXlwFaU2dKlS5kzZw5+fn5MnToVkT98eu1FrZVus337dry8vNi+fTutWrWyuJ63tzdeXl68+OKLwB8RbH5+foSGhnL06FEiIiLYtWuXxXEDBgwgMzPTos7Ly4ulS5eW6z5Los1cXFxwd3fXo80iIyOJjIzk5MmTjBkzBk9PT4xGIxMnTuSFF17g0KFD5coFp1CAMrqKcmAryuzSpUuMHj2a1q1bc88991iMdMvD7t27iY+Pp6ioiISEBIv3nnnmGWJiYggODgagQ4cOLFiwgIiICPLy8sjLy2PixIlW5yzvSDc9PZ3nnnuOn376iaioKJ588kk9E/Hly5cxmUwEBQVhNBoBiI6O5tixY9StW1ePqmvVqhWPP/44gYGBaJrGokWLbuj+FdUXFZGmsMntjEgzGo2sXLmSBx988Lac35lQEWmKsqiFNIVCoahE1EhXYRO190LFoEa6irKoka5CoVBUIsroKhxGRkYGYWFhlXa97Oxs+vfvT7du3Rg5cqTNfX1//vlnAgMD8ff3529/+1ulyaaoPiijq6g2vP322/Tv35/vvvsOT09PPv30U6s2EydO5OOPP2br1q18+eWXHD582AGSKqoyyugqKpTJkyfz73//GzBnZXj00UcpKChgxowZhISE0LFjRxYvXmx13MiRI9m6dSsAqampeiTZvn37CAsLIyQkhMjISK5cuXLTsqWkpOjZK5544gk9eWUJ+fn55OTk0KJFC2rUqEGfPn0sAjoUiopA+ekqKpSRI0cyY8YMBg4cyIYNGwgJCcHFxYVXXnmFunXrkp+fT7t27codnjthwgRWrlxJ06ZNWbRoER9++KFFCLDJZKJ79+5WxwUGBjJ37lyLuszMTOrXrw+Ah4cH58+ft3i/bGivrTYKxa2ijK6iQmnbti1nz57lzJkzxMfHM336dAAWL17M2rVrqVmzJmfOnOHMmTMWx9mLXEtLSyM6Ohowj0RLAhZKqFWrlsV2i9eiQYMGZGdn4+HhQXZ2Ng0bNrT5fgm22igUt4oyuooKZ9iwYSxatIiMjAw6dOig5x/7+eefKSgooFWrVpR1R2vQoAFHjx4FYOfOnXp927ZtSUhIoHHjxoB1UssbGekajUaSkpIYMWIESUlJVgbc1dWVu+++myNHjvDnP/+Zr7/+Ws+7plBUFMroKiqcoUOH0qRJEz1pY/369XnkkUcIDAzkkUcesTl6HDNmDEOHDuVf//qXnjwSYNGiRYwcOZKCggIApk6dSs+ePfX3b2SkO3XqVEaMGMGyZcto1qwZr732GgDz58+nd+/etGvXjvfff5/o6GiKi4uJiIiotI3VFdUHFRyhsIkKjqgYVHCEoizKe0GhUCgqEWV0FQqFohJRRlehUCgqEbWQprCJq6vraU3T7ne0HHc6rq6upx0tg8K5UAtpiltC07QHgXXAD0CMiBQ6WKTbgqZpzwHTgSdE5L+Olkdx56KmFxQ3jaZp7TEb25XA+KpqcAFEJA4YB6zTNO0JR8ujuHNRI13FTaFpWi9gBTBBRD53tDyVhaZpnYAkIBb4u/KrU9woyugqbhhN054FZgMDRWSbg8WpdDRNewjzlMom4EURKXKwSIo7CGV0FeVG07QawDwgAggXkUMOFslhaJpWH/gCuAJEichlB4ukuENQc7qKcqFpmhvwGeAP+FVngwsgItlAOHAe2KxpWmMHi6S4Q1BGV3FdNE1rBHwLFAGPi4ja7xAQERMwClgL/KBpmreDRVLcASijq7gmmqa1xOyhkAIME5E8B4vkVIiZucArQIqmaZWXf0hxR6KMrsIumqYFAluA+SLyiogUO1omZ0VEPgUigU81TXva0fIonBe1kKawiaZpUcDfgadEZKOj5blT0DStNWbPhn8BM5VLmaIsyugqLNDMKRxexhwI0EdE9jpYpDsOTdPuw+zLewgYLSL5DhZJ4USo6QWFjqZpLsBHmB+T/ZTBvTlE5AwQDLgBGzVNa+BgkRROhDK6CgA0TbsH82Pxn4AgEfndwSLd0YhILuYfr53ANk3THnawSAonQRldBZqmNQG2Agcxb+hyycEiVQlEpFhEJgPvA99rmubraJkUjkcZ3WqOpmkdMbuE/ZMqvEuYIxGRD4AxwFeapg10tDwKx6IW0qoxmqb1xmxsx4nIvx0sTpXn6g9cEvAusEB5NlRPlNGtpmiaNgF4FRggItsdLU914epUzjrM0znPqyeL6oeaXqgmaJoWomlaE03TamiaFgs8DwQqg1u5iMgxIBDwAr7UNM1d0zRXTdOedLBoikpCGd1qgKZpdwHxmD0TVgNdAH8ROeJQwaopIpID9AZ+xxzx5wksvpqFQ1HFUUa3etAXOIl5FT0P6C4imY4VqXojIgXAWOBzzPtarAeedahQikpBzelWAzRN+x5ohXnT7dPACRGZ71ipFFc9GfoBl4GhmHdxa3x19zJFFUWNdKs4V7cb9AfqAu7AUcybbyscz2bM7nr3AgI0AGIcKpHitqNGulUcTdNqAmHApquPtAon5OqeFwHA/xORc46WR3H7UEZXoVAoKpG7HC1AReHm5nYqLy/vfkfLcafj6up6Ojc390+OlqOqoPSyYqkK+lllRrqapqkAnwpA0zRERHO0HFUFpZcVS1XQT7WQplAoFJWIMroKhUJRiSijq1AoFJWIMroKhUJRiVQbo3vo0CE6deqEyWQO9omNjWXmzJkAuLm50adPH71tbGwsAQEBdOvWjb17zRlrtm7dSvv27RkzZsxtk/GHH37A39+foKAgFixYYLNNWFgYjRo1Yu7cuXpdVlYW3bt3x2Aw4O/vz549ewCYPXs2Xbt2JSAggOeffx61oOM83An6GBcXR8uWLfHy8rKo9/f3x2Aw0LlzZxISEqyO+/DDDzEajRiNRlq3bs3AgeYthH/88Ud8fX0xGAz06tWLCxcuANCnTx8CAgLo2rUr8fHxt+1+nAYRqRLFfCvX5s0335TZs2fLkSNHpGPHjpKXlyciIs2bN9fb/PLLL2I0GqW4uFjS09PFaDTq76WkpMjo0aOvex0RkYsXL5arXWl8fHzkt99+k+LiYunevbscOnTIqs2xY8fk448/ljfeeEOvi4uLk9mzZ4uIyHfffScDBgzQ76WEyMhI+c9//nNdGa72o8M/z6pSrqWXzq6Pp06dEpPJZCGPiEh+fr6IiFy4cEGaNWt2zXM888wzsmrVKhERGThwoKSmpoqIyBtvvCHvv/++iPyhp7m5udK8eXPJzc21e76qoJ/VZqQLMGXKFJKTk4mKiuK9996jdu3aVm1SUlLo168fmqbRunVrzp49S2Fh+bY8NZlMJCYmEhkZybBhw25YvuzsbJo2bYqmaXTo0IHNmzdbtXnwQeuNqNq0aUNOTg4AmZmZ3HfffQC0bNlSb1O7dm1q1qx5wzIpbh/Oro/3338/Li4uVvW1atUC4OLFi3h7e9s9Pi8vj40bN9KvXz8AvL29yc7OBsxPZ2X1tFatWtSoUQNzcF7VpcoER5QHFxcXgoKCSE5OJjAw0Gab8+fP4+npqb+uV68eWVlZNGrUyO55d+zYwfLlyzl06BA9e/ZkwYIFNGnSBICjR48SHR1tdcygQYOIibEMs7/33nv56aefaNOmDSkpKdx7773luq+OHTvy2muv0bZtW7Kzs9myZYvF+6mpqRw/fpygoKBynU9ROTi7PtojNzeXHj16kJaWxvz59vdN+uqrrwgLC8PV1RWAiIgI+vXrxyuvvMLdd99tdey8efMYNGiQzR+fqkS1MrppaWl8//33hIWF8dFHHzF27FirNg0bNiQrK0t/nZOTg4eHxzXPm5SUxPbt24mJiWHgwIE0aPBHxu2mTZuSmppaLvk+/PBDJk+ejKZptGjRwuLLdi3efvttBgwYwOTJk9m+fTsTJ07km2++AWD37t1Mnz6d5ORkatSoVg82To+z66M93Nzc2LJlC+fOnaNz584MHjyYevXqWbVbsWIFkydP1l+PHz+eNWvW4OPjw/z581mwYAHTpk0DYOnSpaSlpbFy5cpbku1OoNp8C4uLixk3bhyLFi1i/vz5xMXFcfr0aat2RqORdevWISIcPHiQhg0bctdd1/5tevPNN/n++++pXbs20dHRPPHEE6xatQowjyxKFhVKl4ULF1qdp127dmzYsIGkpCSys7Pp0aNHue+vZOTTqFEj/REuPT2dsWPH8vnnn9OwYcNyn0tx+7kT9NEWJpOJ4uJiAOrWrYurq6s+ki3N2bNnSU9Pt3q6sqWnq1atIjExkfj4+OoxMHD0pHJFFa6zkLZw4UKZNGmS/joxMVGGDBkiImK1UDB//nzx9/eXgIAA+d///qfXl3fh4uTJk7JkyZLrtivL3/72NzEajRIcHCzffPONXj906FD9/6effloeeeQRad68ufTp00dERE6cOCEhISFiMBikS5cukpKSIiIiBoNBWrRoIQaDQQwGg3z55ZfXlYEqsFDhTMWeXt4J+piQkCChoaHi5uYmoaGhsmXLFjl48KB069ZNjEaj+Pn5SUJCgn6Nl156ST/2/fffl+nTp1ucLzU1Vbp27SoGg0GCg4PlxIkTkp+fLy4uLtK5c2ddT3/77Te7MlUF/VR7LwAtWrSgVatWJCcn222zdetWJk+ejNFovOY81p1OVYhtdyZuRi+VPtqnKuinMroKC6qCUjsTSi8rlqqgn9VgAsXxfPnll3Tt2hWDwUDv3r05f/48AEuWLKFLly4EBQURFRVFfn4+UL6ghqSkJLp27Uq3bt347LPP9PpZs2bh7++P0Whk3759gP3gCUX1ZtOmTWiaxvHjxwHz6Lldu3a4urrqdWA/aOfbb7/Fz88PPz8/m0ESIsIzzzxDUFAQPXv25MSJE/p77777LmFhYQQHB+vzzSXMnDnTKiCjSuHo+Y2KKpQjOMJR/Prrr2IymUREZNGiRfLqq6+KiMjBgwelqKhIRESmTJkiS5cuFZHrBzUUFRVJy5YtJScnR0wmk3Tu3FlycnJkz5490rNnT/2aISEhImI/eMIWVIE5M2cqzqqXRUVF0qtXL/Hx8ZFjx46JiEh2drZcvHhRDAaDXidiO2insLBQHn30UTl37pxcuXJFHnvsMcnJybG4RmJiojz77LMiYta76OhoERFZv369TJ061aZcx48flyFDhljNa5dQFfSzyo90MzIy8PHxITo6mscee4x33nmHSZMm4e/vT1RUlN4mICCA4OBgjEYjWVlZXLhwgcGDBxMSEkJwcDAHDhy4aRmaNWumO5mXDlLw8vLSV2tL118vqOHcuXM0atSIu+++GxcXFx5++GF27tzJgQMH6NSpk37N9PR0CgsL7QZPKCofZ9BHgJUrV9KvXz/q1q2r19WrVw93d3ertraCdg4dOkSzZs1o2LAhbm5u+Pv7s3PnTovjDhw4gI+PDwA+Pj6kpKQAZm+FoqIiwsLCiIyM5NSpU/oxs2fP5tVXX72le3N6HG31K6pgZ0Tx66+/SuPGjeXy5cuSm5sr7u7usmfPHhERCQ0NlfT0dFm2bJnMmjVLP6a4uFimTZumr8zu27dP+vfvb3Xu8ePH6yuuJaV379425RAxr/C2b99eTpw4YVG/f/9+6dSpk1y6dMmiPiUlRYxGoz4aLqFkpHv8+HHJzs6Wpk2byueffy779u0Tf39/yc/Pl127dommaXL27FnJzMwUPz8/8fb2lgceeEAOHz5sV0aqwEjCmUpZvXQGfbxy5YqEhoZKQUGB1ahWRKzqfH195X//+5/k5+dLly5dJDY2Vr7//nsZMWKE3mbGjBmyevVqi/OsW7dOBg0aJMXFxZKYmCh169YVEZHu3bvL888/LyIin3/+uQwfPlxERH766ScZM2aMiFh7cJRQFfSzWgRHtGnThjp16gBw33330b59e8AcUnv+/HkGDx7MW2+9xbBhw3jooYeYPXs2e/fuZfPmzfzjH/8AsBma+MEHH5RbhqysLAYOHMiHH35oEfSQkZHBiBEjWL16tcWo41pBDTVq1GDJkiU89dRT3H333bRv3x5PT0+8vb2JiooiLCyMli1b0q5dOxo2bMiMGTPsBk8oKh9H6+N7773HuHHjruvvW4KtoJ2yQRvZ2dlWvuDh4eFs27YNo9FI586dad26NQANGjSgV69egHmzm9dffx0wz+WW11/4TqZaGN3SClpWWUWEGjVq8NZbbwEwatQoNmzYgLe3N35+fkRERADou0GVZsKECezfv9+izt3d3crV59KlS/Tv3585c+bQuXNnvf7UqVNERkaybNkyHn74Yb2+JKhh7dq1doMaSpzaL168yMCBA/XzxsTEEBMTw759+4iNjdXv15ZTusIxOFof09LS2Lx5M0uXLuXnn39m+PDhJCcnW/zol6YkaMdkMjFgwAB69OhB/fr1ycjIICsrizp16rBt2zbmzZtndWzJbngbNmzQ92wIDQ3lxx9/pGfPnuzYsUOfTjt8+LC+a9rJkyeZOHEiixYtstOLdzCOHmpXVOEa0wuhoaH669KPLSNGjJDvvvtOVq1aJYGBgWIwGKR79+6SmZkp2dnZMmTIEAkODpbg4GCZP3++zfOXh1mzZkmjRo30R745c+bo12/SpIleX+LAbi+oYd68efLzzz+LiHnhzWg0SlhYmPz444/6tR5//HEJDg6WyMhIOXPmjIjYD56wBVXg8c2ZSlm9dAZ9LE3pqYT9+/dLaGio1K9fXwIDAyUuLk5E7CV2Wp4AACAASURBVAftbNy4UXx9fcXX11dWrlyp15cE82RmZorBYJCQkBB5+umn5fLlyyJi3qXs6aefFqPRKAaDQQ4ePGglV1WeXlB+ugoLqoIfpDOh9LJiqQr6WeW9FxQKhcKZUEZXoVAoKhFldBUKhaISUUb3FqmscEV7IZoxMTEYDAa6dOnC1KlTAbP7TmhoKIGBgfj6+lq4h9kKE1ZULyozxNZWuO8XX3xBmzZtbG4JWS1w9EpeRRUcFG5pb5W1orEXolmSr0pEJCgoSPbt2ydXrlzR25w9e1ZatmwpImI3TLg0VIHVYWcqjtLLa1FZOmsv3Pfs2bN6PrQbpSroZ5X1083IyGDYsGHUqlULESExMZG9e/cya9YsCgsL8fDwYNWqVbi5uWE0GunQoQP79+8nPz+fsWPHEh8fz+nTp1m9ejUtW7bEaDTi7e3NgQMHKC4uJiEhwSKctqCggAkTJnD48GFMJhOxsbH4+fkxd+5ckpKScHd3p0+fPrz00ks3dT+2duaHP/JVmUwm6tSpg6enJ25ubnouNTc3N90X1F6YcHmd5BW3l6qms6tWraJBgwaEhYXh4eFBXFwcf/rTn8qdhqrK4mirX1GFMiMKW6GUpcNsp06dKv/85z9FxOyrmJiYKCIio0ePlhdeeEFERD755BOZNm2a3iY+Pl4/9+TJk0Xkj1HD4sWLZd68eSIicubMGfH19RURkdatW+vXLRvOKyISERFhFbp5rY2pbYVtjh07Vh544AEZNWqU1TXGjBmj+//aCxMuDVVgJOFMpaxeXouqprP2wn1LUCPdKoatUMq0tDReffVV8vPzOX36NPfcc4/evmQE+OCDD9K8eXP9/9IZef39/fW/iYmJFtfbu3cv27ZtY/369QB61NfChQuZOHEihYWFjBs3zioB4Zo1a275XpcsWUJBQQEDBgxg/fr1hIeHA/Daa6/RoEEDPfeWvTBhhXNQ1XTWXrhvdafKGl1boZRLly5lzpw5+Pn5MXXq1JKRCGA/NLN0m+3bt+Pl5cX27dtp1aqVxfW8vb3x8vLixRdfBP4I0/Tz8yM0NJSjR48SERHBrl27LI4bMGAAmZmZFnVeXl4sXbq0XPeZl5eHq6srLi4uuLu76zH9sbGx/P777yxbtsyivb0wYYXjqWo6ay/ct7pTZY1ucnIycXFx1KxZk9q1axMYGMilS5cYPXo0rVu35p577rEYNZSH3bt3Ex8fT1FRkdWmzc888wwxMTEEBwcD0KFDBxYsWEBERAR5eXnk5eUxceJEq3OWd9SQnp7Oc889x08//URUVBRPPvmknu318uXLmEwmgoKCMBqN/Prrr0ybNo2AgACMRiMAGzdupFatWnTv3p3CwkLuvffeqhnXfgdT1XQ2OjqacePGERwcjIjoRjk1NZW5c+fy+++/ExYWxrPPPktkZOQN3dedjAoDLidGo5GVK1fqC1RVlaoQZulMODIMuCrqbFXQT+Wnq1AoFJWIGukqLKgKIwlnQullxVIV9FONdMuQkZFBWFiYQ2UomzCwNPaSVubm5jJu3DjCwsIwGo388ssvFscFBQXpe5Uq7jwqWy8PHDiA0WgkODiYKVOm6PV9+vQhICCArl27Eh8fb/NYW1GPIsJLL71Et27dCAsL03V7+fLlBAYGEhQURN++ffW0UlUaR/usVVShgiJ/yu53WtnYShhYGntJK19++WVZt26dzXOuWbNG+vbte03/3xKoAn6QzlTuVL3s37+//PDDDyJi9gPetGmTiPyhfyURZbm5uRbH2Yt63LBhg+6nu2HDBj1JZemIytdee00WLlx4Tbmqgn5Wi5Hu5MmT+fe//w1AYWEhjz76KAUFBcyYMYOQkBA6duzI4sWLrY4bOXIkW7duBcwrriUjxX379hEWFkZISAiRkZFcuXKlwmS1lTCwNPaSVm7cuJGUlBSMRiMvvfQShYWF+v0uXrzY5iq0wrE4s17aSypZon+1atWiRo0aVi6H9qIeU1JS9KwXjz/+OP/973/185Rw6dIlvL29b1rmO4VqYXRHjhypPwpt2LCBkJAQXFxceOWVV9i0aRM//PAD7777LgUFBeU634QJE1i+fDmbNm3CaDTy4YcfWrxvMpn0dDqly/WynObm5rJixYpyTQOkpqZy/PhxgoKCAPMXzt/fn9TUVAoKClixYgUA//jHP3jqqaeoXbt2ue5NUXk4s162bduW9evXIyJs3LjRyi933rx5DBo0yEqvvL29SUlJwWQysXv3bk6dOkV2djbnz5/Hw8MDMM/LFhUV6ccsXryYtm3bsnXr1mphdKusn25p2rZty9mzZzlz5gzx8fFMnz4dMH/Ya9eupWbNmpw5c4YzZ85YHGfP4TwtLY3o6GgA8vPzdV/YEmrVqkVqaup15crNzdUjdmbOnMmOHTvKlTDQVtLK0tE/vXv35uuvvyYnJ4e1a9eyceNGtmzZcl15FJWLs+olwN/+9jdiYmL4+9//zsMPP2yRTHXp0qWkpaWxcuVKq+PsRT2WTmQpIhY6Pn78eMaPH8/8+fOJjY3l7bffLpeMdyrVwugCDBs2jEWLFpGRkUGHDh3Iysri448/5ueff6agoIBWrVpZKDCYDdnRo0cB2Llzp17ftm1bEhISaNy4MWCdJNBkMtG9e3crGQIDA/VEfWDejKb0l2D58uXXTRhoL2llSfRPYGCgHv2Tnp5OTk4O4eHhZGZmcvLkSZYsWcKzzz57Ez2ouB04o14CNGnShC+//BIRITo6Wp8aWLVqFYmJiaxdu9YqS3UJtqIejUYjn332GREREWzatEmfuiiJqATw8PAgLy+v3H13x+LoSeWKKlxnweL8+fNSp04deeedd0TEvJnIoEGDxNfXV0aNGiUdOnSQY8eOWSxY7N+/X9q3by+9e/eWmJgYfSFq79690r17dz1JYOlkfRVF6Y1tvvnmG1mxYoVebytp5bFjx6RHjx5iMBgkMjLSaoEjJSVFLaQpvSw3n376qRiNRjEajfomO/n5+eLi4iKdO3fW9e+3334TkT+SUYrYTo5aXFwskyZNksDAQAkNDZWjR4+KiMj06dP1c0VEREhWVtY15aoK+qn8dBUWVAU/SGdC6WXFUhX0s1ospCkUCoWzoIyuQqFQVCLK6CoUCkUlooyuQqFQVCJVxmXM1dX1tKZp9ztajjsdV1fX046WoSqh9LJiqQr6WWW8FyoDTdOGAS8AXUWkuJKu2RBIB0JEROVMV9hE07R7gP8H9BeRnddrX4HX/QjIEZG/VNY173SU0S0nmqa5YzZ+Q0Tk+0q+9nNAf+Bx5X+ksIWmafOBP4nIyEq+7n3AfiBARH65XnuFMrrlRtO0N4CHRWSYA67tAvwPmCEiX1b29RXOjaZpXsB2oJ2InHTA9f+C+Umsd2Vf+05EGd1yoGlaM2AX8JiIWG9yWzkyPA78A3hERPIdIYPCOdE0bS3wg4j81UHXrwXsA14Qka8dIcOdhPJeKB+xwHuOMrgAIvJ/XFVsR8mgcD40TQsD2gLvOUoGETEBLwILrj6VKa6BGuleB03TDMAKoLWI5DpYlhbAD0BbETnlSFkUjkfTtLswTzu9KiJrHSyLBnwDrBcRh/0A3Akoo3sNNE2riXla4S0RWe1oeQA0TXsbuFdERjlaFoVj0TRtIhCBkyywaprWBtiCeQrsrKPlcVaU0b0GmqaNBZ4CDM6g1KC7Bv0C9KtM1yCFc+GsroSapr0HuIrIOEfL4qwoo2sHTdPqY/Z77CUiexwtT2k0TRsFjMHspqM+wGqIpmlxmL+/MY6WpTSapnlg/t70EJH/OVoeZ0QZXTtomrYAcBeRsY6WpSyaptUA/gssEJF/OVoeReWiaVpbYBPQRkTOO1qesmiaNg4YAgSrQYE1yujaQNO01sBWzHNTZ67X3hFomhYIJGBe4LvsaHkUlcPVBauNQJKIxDlaHltcXeDbDbwuIl84Wh5nQ7mM2eZvwDxnNbgAIrIV8w/DVEfLoqhU+gKemH22nRIRKcTs2hiraZqbo+VxNtRItwyapoVj9nlse9X/0GnRNK0psAfoKCK/OVoexe1F07TaQBow/qrftlOjadq/gd0i8qajZXEmlNEtxdXImp+Bv4jIOkfLUx40TZuFeRrkSUfLori9aJo2FfPiaX9Hy1IeNE17GNgJPCoiJxwtj7OgjG4pNE17EegOhN8pCwCaptXB7Do0XERUnvUqiqZpf8IckegrIoccLU950TTtTaCpiAx3tCzOgjK6V9E0rRHm3ZKCRCTd0fLcCJqmPQm8DPiISJGj5VFUPJqmLQfOisg0R8tyI1zdne8XYKCIbHe0PM6AMrpX0TTtH0CeiNxxextcXdHeAsSLyFJHy6OoWDRN8wGSMHuq5DhanhtF07RoYCLgV1n7UDszyugCmqa1BzZgVuosR8tzM2ia1glYB7QSkQuOlkdRMVz9Qd0KLBOR5Y6W52a46lf+A7BIRFY4Wh5HU+1dxq4q9XvArDvV4AKIyC4gGXjN0bIoKpQhQG3gnw6W46a5OrqdBMzTNO1uR8vjaKr9SFfTtEGYDVXHO30+9GourjTULv5VAk3T6mIOqa30bCW3A03TVgDHRWSGo2VxJNXa6F513E4HnhaRFEfLUxFomjYZMIpIH0fLorg1NE2bA7QQkaGOlqUi0DTtAcwumZ1F5Iij5XEU1dLoXs05thMIAzqIyEAHi1RhXPU1TgOeA+4DTonIRsdKpSgvV7OUjAE+whxK215EjjlSpopE07RXgE7AM8BMEZnkYJEqneo6pxuKebf9F4EpDpalQrkaRfcS8C7wCNDZsRIpbpA2gA/wV+D9qmRwr7IA6IDZHz7CwbI4hOpqdOsBg4FPgfevbpVYJbi6n2kv4ATmL3A9x0qkuEHqAS6AH2DSNM3pw33Ly1UPmy3AYuAVqqluVlej6wn4A1GYs6h+4lhxKpTZgCvQGugB/Mmh0ihulPqYn07yMY8Gq9Jm4Lsxewr9BbgbuPtqdpZqRXU1ug8CZzDv9zlXRAocLVBFISLZV1P5jAEKMM+fKe4cfIE6mB/DQ0XksIPlqTDEzKfAY8BBQAPqOlaqyqe6LqSFApuvbkFXZbka2txMpfW5c9A07SGgrojsd7QstxtN03oAG++UfU4qimppdBUKhcJRVNfpBYVCoXAId12vgZub26m8vLz7K0OYqoyrq+tpANWXFYOrq+vp3NxcfZFQ6emNU7oPVf9VLGX1szTXnV7QNK26TbncFsxbPIDqy4pB0zRERCv1WunpDVK6D1X/VSxl9bM0anpBoVAoKhFldBUKhaISqTJG98CBA3Ts2BF3d3e2bt2q12dnZ9O/f3+6devGyJEjMZnMuSYHDBiA0WjEaDRSv359vvrqK4vzHT58mKCgILp160ZgYCA//vgjAD/99BP+/v4YDAYCAgL46aefKu8mbwPr168nICAAo9FISEgIx46Zo04nTJig98+f/vQn4uLiMJlMep3RaMTV1ZW9e/ciIsTExODn50fnzp1ZuXKl1XVOnz5Nz549CQ4O5umnn9Y/h2PHjhEWFka3bt2YPHmy1XFBQUGMGTPm9nbCbWTTpk1omsbx48cB+/0QExODwWCgS5cuTJ36R4LnefPm0blzZ7p06UJsbGy5r1PCsmXLcHFxuQ13Vrl8+eWXdO3aFYPBQO/evTl//jwAW7dupV27dri6ulrce1xcHC1btsTLy8vqXCaTCS8vL+bOnWv1XlJSkq7f7du3p1OnP9zc3333XcLCwggODmbVqlU3fzMics2C7tPs3Fy+fFkyMzNlxIgR8t133+n106dPl2XLlun/L1++3OK43NxceeihhyQvL8+i/ty5c3Lu3DkREUlLS5PAwEARETGZTFJcXCwiIt9++60MGjSoXPIB4ox9mZ+fr/+/bNkymTx5slWbNm3ayO+//25Rd+zYMfH29hYRkb1794rRaBQRkUuXLsmf//xnq3NMmjRJEhISRETkjTfe0D+HqKgo+fbbb/X/N23apB+zZs0a6du3r4wePdrqfFf70qn1tKioSHr16iU+Pj5y7NgxEbHfD6U/h6CgINm3b5/k5OSIl5eXFBYWSmFhobRq1Uqys7PLdR0R83ciPDxcHn74YZvyle5DZ+y/0vz6669iMplERGTRokXy6quviohIdna2XLx4UQwGg8W9nzp1SkwmkzRv3tzqXAsWLJC+ffvKG2+8cc1rvvnmm/LXv/5VRETWr18vU6dOLbe8ZfWzdKmQkW5GRgY+Pj5ER0fz2GOP8c477zBp0iT8/f2JiorS2wQEBBAcHIzRaCQrK4sLFy4wePBgQkJCCA4O5sCBAzctQ506dfDw8LCqT0lJISLCvK/GE088QUqK5Q6Oa9eupUePHtSuXduivmHDhjRs2BCA2rVrU7OmOVrRxcVFXxTLzs7m0UcfvWmZnaHfatWqpf9v6362b99OkyZNaNy4sUX9ypUrGTZsGACenp7UqlWLgoICLl68SIMGDayuc+DAAXx8fADw8fHRP4fdu3cTEhICWH4+hYWFLF68mIkTJ97wPTlDv4K5j/r160fdun8EXdnrh5LPwWQyUadOHTw9PXFzc8PT05Pc3Fxyc3OpXbu2lZ7auw7AO++8w3PPPafr683iDP3ZrFkzfcRe+vtYr1493N3drdrff//9Nkf42dnZ/Oc//2HAgAHXveann36q6/iqVasoKioiLCyMyMhITp06ddP3UiEj3V9//VUaN24sly9fltzcXHF3d5c9e/aIiEhoaKikp6fLsmXLZNasWfoxxcXFMm3aNP1Xf9++fdK/f3+rc48fP14MBoNF6d27t11Zyo50W7ZsqY9MDxw4IOHh4Rbtw8PDLdqXpbCwUMLDw+Wbb77R67Zt2yZdu3YVT09P2b59+zV65g+wMdJ1ln5bs2aNdOrUSby8vOTgwYMW702YMEE++eQTq2Patm0rv/32my7T+PHjpVmzZtKoUSNZs2aNVfspU6ZIXFyciJhHeyWytGjRQm+zceNGmTBhgoiIxMXFSXx8vKSkpNzwSNcZ+vXKlSsSGhoqBQUFFqMwe/0gIjJ27Fh54IEHZNSoUVJUVCQiIm+99ZZ4enpK48aN5f333y/3dU6ePCn9+vUTEbE52ivbh9f6njtDf5Zw8uRJad++vZw4ccKivuxIt4Sy9z558mTZvHmzfPzxx9cc6f73v/+V0NBQ/XX37t3l+eefFxGRzz//XIYPH273WJFrj3Sv66dbXtq0aUOdOnUAuO+++2jfvj0ADz74IOfPn2fw4MG89dZbDBs2jIceeojZs2ezd+9eNm/ezD/+8Q8Am7/IH3zwwS3J1aBBA7Kzs/Hw8CA7O1sfvYJ5fu3gwYMEBATYPFZEGDVqFH369KFnz556vZ+fH9u3b2f79u0899xz/Pe//71p+Zyh3yIiIoiIiOCzzz5jxowZrF69GjCPutatW8fbb79t0X7Xrl3ce++9NG3aFICNGzdy4sQJDh06xIULFwgMDCQ8PNxiVDZ9+nRiYmJITEykXbt2eHp6AlCjxh8PWyWfT05ODmvXrmXjxo1s2XJzWeUd3a/vvfce48aN4667LL9i9voBYMmSJRQUFDBgwADWr19P8+bNWbNmDYcPH0ZECAoKYsCAATzwwAPXvc7s2bN59dVXyyVreXB0fwJkZWUxcOBAPvzwQ4t+Ky8ZGRn8+uuvBAUFceTItfdQX7FiBcOH/5E1vkGDBvTq1QuAPn368Prrr9/w9UuoMKNbukPLdq6IUKNGDd566y0ARo0axYYNG/D29sbPz09//C9ZVCjNhAkT2L/fMgzd3d2d5OTkcsllNBpJSkpixIgR+iR5CQkJCQwZMsTu49dzzz2Hl5cX48eP1+vy8vJwdXUFwMPDQ1fEm8XR/Xat+/n6668JCgqyemz95JNPLBQSzEpZs2ZN7r77bgoKCigqssx85OHhwaeffgqYDU94eDgAHTp0YPPmzRgMBpKSknj66adJT08nJyeH8PBwMjMzOXnyJEuWLOHZZ5+11YU2cXS/pqWlsXnzZpYuXcrPP//M8OHDSU5OttsPJZ+Di4sL7u7u+udw991365+Pq6srly5dKtd1Dh06xGuvmdPlnTx5kkGDBvHFF1+Uu//K4uj+vHTpEv3792fOnDl07nxzW0Tv3r2b33//nZ49e3LixAny8/Np27YtTzzxhEW7goICvvrqK+bNm6fXhYaG8uOPP9KzZ0927NhBy5Ytb0oGoOKmF0oPxUsP6Use91etWiWBgYFiMBike/fukpmZKdnZ2TJkyBAJDg6W4OBgmT9//nWvZY/MzEwJDQ2Vxo0bi4+Pj7zyyit6fd++faVbt24yfPhwiwWLjh07yi+//GJxnqFDh4qISEpKiri4uOiPPBERESIisnr1agkKChKj0ShGo1F/zLoe2JlecHS/vf/++2IwGMRoNEqPHj0kIyNDf2/AgAGyYcMGi/YFBQXy0EMPyYULF/S6wsJCGTFihPj7+4uPj4/8/e9/FxGRPXv2yNtvvy0i5kVHo9EowcHBMm/ePP3YjIwMCQkJkcDAQHnhhRf0qaASbnZ6wdH9WprSj772+iE8PFwMBoP4+fnJtGnT9PqXX35ZunbtKl26dNHrT548KS+99NI1r1OaiphecHR/zpo1Sxo1aqR/H+fMmSMiIvv375fQ0FCpX7++BAYG6lM3CQkJEhoaKm5ubhIaGipbtmyxOF/Z6YWS772ISFJSkkRFRVm0z8/Pl6efflqMRqMYDAarabiylNXP0kVFpFUSKiKtYlERabeOiki7faiINIVCoXASlNFVKBSKSkQZXYVCoahEnNbo2grfux3YCyO0F5b57bff4ufnh5+fHwkJCXp9WFgYjRo1shla6KxUVh8vWbKELl26EBQURFRUFPn5+QD8+OOP+Pr6YjAY6NWrFxcuXKgUeW4XldWfy5cvJzAwkKCgIPr27UtOTo7F+9HR0YSFhVWKLLcTR+unrVD4CsHeCpvcgPfC7cDeimtFYy+M0FZYZmFhoTz66KNy7tw5uXLlijz22GOSk5MjIuaw2Gs5XOOEYcCV1ccHDx7Unf2nTJkiS5cuFRGRgQMHSmpqqoiYQ2JtOf/bAycMA66s/iytm6+99posXLhQf71r1y7p37+/hbeBPSin94KjcLR+lsZWKPy1KKufpcsN++lmZGQwbNgwatWqhYiQmJjI3r17mTVrFoWFhXh4eLBq1Src3NwwGo106NCB/fv3k5+fz9ixY4mPj+f06dOsXr2ali1bYjQa8fb25sCBAxQXF5OQkMB9992nX6+goIAJEyZw+PBhTCYTsbGx+Pn5MXfuXJKSknB3d6dPnz689NJLN/WjU6+e7SzQtsIyDx06RLNmzfQAC39/f3bu3ElISAgPPvjgTV3fFlWtj0uPWEqHcHp7e5OdnQ2YHd9btGhxC71mn6rWn6VDty9duqSHUQO8/vrrvPLKK0yfPv3mO+w6VLX+tKefJdgLhb9p7FljsfMLaCvc79KlS/rrqVOnyj//+U8RMfsNJiYmiojI6NGj5YUXXhARkU8++UT3OTQYDBIfH6+fu2TDlZJfucWLF+v+jGfOnBFfX18REWndurV+3ZJfqdJERERYhRfa8vcswZaPY9mwzO+//15GjBihvz9jxgxZvXq1/rqiRrpVtY/3798vnTp10s+5Z88eadKkiXh7e4uvr6++oUl54AZGulWxPz/44APx9vaWzp07y5kzZ0RE5KuvvpI5c+ZY+dXag5sc6VbF/hSx1s8S7IXCX4uy+im3MtK1Fe6XlpbGq6++Sn5+PqdPn+aee+7R25dsjfbggw/SvHlz/f/Nmzfrbfz9/fW/iYmJFtfbu3cv27ZtY/369QD6yGjhwoVMnDiRwsJCxo0bR2BgoMVxa9asudFbs8JWWGZWVpb+ftmw4oqiKvZxRkYGI0aMYPXq1XqE2/jx41mzZg0+Pj7Mnz+fBQsWMG3atHKfs7xUxf4cP34848ePZ/78+cTGxjJv3jwWLFjAV199xdmzZ8t9npuhKvanLf0E+6Hwt8ING11b4X5Lly5lzpw5+Pn5MXXqVIsAAHvhg6XbbN++HS8vL7Zv306rVq0sruft7Y2Xlxcvvvgi8EcooZ+fH6GhoRw9epSIiAh27dplcdyAAQPIzMy0qPPy8mLp0qXluk9bYZleXl5kZGSQlZVFnTp12LZtm0WoYEVR1fr41KlTREZGsmzZMh5++GGL9xo1aqT/PXToUDl658apav1ZNnQ7Ly+PU6dO6XsT5ObmkpaWxuuvv87MmTNvrLPKQVXrz2vpp71Q+Fvhho1ucnIycXFx1KxZk9q1axMYGMilS5cYPXo0rVu35p577rH4lSsPu3fvJj4+nqKiIguPAIBnnnmGmJgYgoODAXOs/oIFC4iIiCAvL4+8vDyb2/+V91cuPT2d5557jp9++omoqCiefPJJYmJiGDhwIJcvX8ZkMhEUFKTv2fDOO+/o8fKTJ0/W73XUqFHs2LGD/Px8duzYYbUp+o1Q1fr45Zdf5vTp0zz//PMADB06lLFjxzJ//nyefPJJXF1dqVGjhs3NzyuCqtafr7/+Otu2bQPMe14sX76c+vXrs2fPHsA8ahszZsxtMbhQ9frTnn6CeZ+RG9nzozw4PAzYaDSycuXKCl2IckYcGQZcFfvYkWHAVaU/nSUMuKr0Z2lUGLBCoVA4CQ4f6VYX1IY3FYva8ObWcZaRblXEKUa6GRkZDouSSU1NpXHjxnp0ia1Nx20lsjt69KhFIkYXFxeysrIcnpyysvvyzTffJCgoiICAAKKjoykoKLCbpLI0IsIzzzxDUFCQvocpwJEjR/R5cqPRyG+//QbA2LFj8fX1xdfXl/nz51fa/ZVQ2f1qL+KpT58+BAQE0LVrV+Lj420eO2vWLPz9/TEajezbtw+wn2T0dlPZ/fbFF1/Qpk0bfTGxhBkzZvDQQw9ZyCJyLJt5MAAABxBJREFU/aSp+fn5REdH061bN5544gk9MnL27Nm0adNG/4zK7gd800lT7fmSlRQqKFKlvL6DtwN7e7KW5lqJ7EREvv/+e+nVq5eI3FxySiowIq2y+7J0BNTw4cMlOTnZ4v3SSSpLk5iYKM8++6yIiHz33XcSHR0tIiJ/+ctfdD/OTz75RN8btmRv46KiIvH19ZVDhw7ZlYnbEJHmSB0tHfFU0g+5ubnSvHlzyc3NtWi7Z88e6dmzp4iYZQ4JCRGR8iUZLQ0VFJFW2f129uxZvW9Kc+LECTl8+LCFLOVJmrpkyRJ57bXX9P9nzpwpIuY9fO35514raarItf10b2mkO3nyZP79738D5kSCjz76KAUFBcyYMYOQkBA6duzI4sWLrY4bOXKkniY9NTVV/7XYt28fYWFhhISEEBkZyZUrV25FPAs2btxIYGAgEyZMsHlee4nsSiidvqMik1OW4Mx9WRIBVVxcTGFhoVVMfOkklaWxl4SxdCRaZmamHn1Usht/jRo1uOuuu6wig24GZ+7XEspGPJX0Q61atahRo4ZVpoYDBw7ovq/NmjUjPT2dwsLC6yYZvRGcud/uvfdeq1EumBOklk7/VFJ3vaSp10pe+/bbbxMYGMi7776r191K0lTg1ka6e/fulb59+4qISHJyskyaNEn/RRERycvLkxYtWojJZLL4NSydPLL0KLRbt256ssOFCxfKu+++a3G9/Px8qwgTg8GgZ4mwR05Ojj5aeO211/RfMlvYGunm5+dL06ZN5cqVK3rdjSan5DojXWfvy5kzZ0rz5s2lV69ecvnyZYv3SiepLM26detk0KBBUlxcLImJiVK3bl0REfntt9+kdevW0q5dO2nRooVkZWVZHPfJJ59Y7dxfFso50nX2fhWxH/E0d+5cmT59ulX9vn37xN/fX/Lz82XXrl2iaZqcPXtWRK6dZLQsXGOkeyf0m63vatlRd3mSpj7++OPy66+/ioj5KfaRRx4REZFz585JcXGx5ObmSvfu3WXTpk0icv2kqSIVHJFWmrZt23L27FnOnDlDfHy8Hu+9ePFi1q5dS82aNTlz5gxnzpyxOM6eg3RaWhrR0dGAeZ6ldD4zMP/yp6amXleu3NxcPYnczJkzLWLTn3rqKd3Jurx89dVXhISE4ObmptdVZHJKcN6+LGHOnDnMnj2biRMn8s9//pMJEyYA1kkqSxMeHs62bdswGo107tyZ1q1bAzBt2jTeeOMNBg0axGeffcb06dP1UdM333zDihUrSEpKKrds18LZ+9VexNPSpUtJS0uzOQfp7e1NVFQUYWFhtGzZknbt2umRkfaSjN4ozt5v5aU8SVMbNmxIVlYWzZo1s4gyLfnr6urKwIED+fHHH+nUqdMtJ0295cSUw4YNY9GiRWRkZNChQweysrL4+OOP+fnnnykoKKBVq1ZWK/YNGjTg6NGjAOzcuVOvb9u2LQkJCfpjVtmJa5PJRPfu3a1kCAwMtNhS0c3NzeIDvHDhgr6xzaZNm6wiXq7HJ598ojtOQ8UnpyzBGfsS/rhfTdOoV6+exf3aSlJZmpJzbdiwweLxt3QkWslUw5YtW5g7dy5ff/21zcfHm8VZ+xVsRzytWrWKxMRE1q5da/W4XEJMTAwxMTHs27eP2NhYNE2rcL105n67Ea6XNLUkeW2HDh0sktdmZ2dTv359RISUlBSioqIqJGnqLS+knT9/XurUqSPvvPOOPpwfNGiQ+Pr6yqhRo6RDh//f3h2rKA5FYRyPhZDaZkvxAQQXFMUYLiraCFrZCRJrH8bGZxFsrK0jgrXVgFpr9W2xmNVdZ3Rm3YMs/1+XynCIH8m9h3u+a7vdXr32r9drFQoFdTodjcfj5BU9jmO12+1kkN1sNvvwtx81nU5VLBYVhqF6vZ72+72knwfUzOdzSe8PstvtdsrlclcHanxlOKX3wEbaq9YyiiI551Sr1TQajZKDaW4NqZR+Dfk7HA5yzqnRaCiKomRZYrVaKQgCOedUrVYVx7EkKZvNKp/PJ5+Wy+Xy3XvyPrGR9qp1lf4c/nk6nZROp1UqlZI6nD/LL4cntlot1et19fv95MCbj4aM3uLd2Uh71botFour/+r50KnJZKIgCJTJZNRsNrXZbB4amno8HjUYDBSGobrdbrLcNRwOValUVC6Xb25KfnV5gT5dI/TpPhd9un+PPt1/5yX6dAEAhC4AmCJ0AcAQoQsAhu62jPm+/5ZKpb5Z3Mz/zPf9N8/zPGr5HOd6Xl5T28+5rCH1e67fn89Ld7sXAADPw/ICABgidAHAEKELAIYIXQAwROgCgCFCFwAMEboAYIjQBQBDhC4AGCJ0AcAQoQsAhghdADBE6AKAIUIXAAwRugBgiNAFAEOELgAYInQBwBChCwCGCF0AMEToAoAhQhcADBG6AGDoB3FQQKn5zU05AAAAAElFTkSuQmCC\n",
>>>>>>> 4ef4e05 (Save lambda mart)
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two - Compute the swaps but _scaled_ to current model's error\n",
    "\n",
    "LambdaMART is an _ensemble_ model. It's not just about the first model, but collecting a series of models where each model makes a gradual improvement on the current model. The technique used is known as [Gradient Boosting]()\n",
    "\n",
    "To build a model that compensates for the current model's error, we scale the next set of dependent vars to predict based on the correctness of the existing model in ranking. In this way, we eliminate where the model currently does a good job (no need to learn these) and leave in places where the model isn't doing a good job (this is where. we want ot learn)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 91,
=======
   "execution_count": 381,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 528,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
<<<<<<< HEAD
       "      <th>delta</th>\n",
=======
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
<<<<<<< HEAD
       "      <th></th>\n",
=======
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
<<<<<<< HEAD
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>427.553645</td>\n",
=======
       "      <td>7.292951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.100504</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
<<<<<<< HEAD
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>97.974272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>63.670676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>13.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>83.896012</td>\n",
=======
       "      <td>3.798236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>3.094822</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.697540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>1_32221</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>32221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>1_801</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>801</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>2.992312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1_70</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.022346</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
<<<<<<< HEAD
       "      <td>...</td>\n",
=======
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
<<<<<<< HEAD
       "      <td>1385</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>-19.692155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1386</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>-19.806921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1387</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>-19.915309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1388</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>-20.017899</td>\n",
=======
       "      <td>1366</td>\n",
       "      <td>40_1891</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1891</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1365</td>\n",
       "      <td>40_1892</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1892</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 2.4547963]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1364</td>\n",
       "      <td>40_1895</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.487482, 2.062405]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.682829</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1363</td>\n",
       "      <td>40_330459</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>330459</td>\n",
       "      <td>3</td>\n",
       "      <td>[7.2694716, 4.237955]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>1.354350</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002145</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
<<<<<<< HEAD
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>-20.115197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
=======
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.013697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 13 columns</p>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
<<<<<<< HEAD
       "    2       2     1_1369    1      rambo    1369      3   \n",
       "    3       3    1_13258    1      rambo   13258      2   \n",
       "    4       4     1_1368    1      rambo    1368      4   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1385   40_37079   40  star wars   37079      0   \n",
       "    26   1386  40_126757   40  star wars  126757      0   \n",
       "    27   1387   40_39797   40  star wars   39797      0   \n",
       "    28   1388   40_18112   40  star wars   18112      0   \n",
=======
       "    2      39    1_32221    1      rambo   32221      0   \n",
       "    3      29      1_801    1      rambo     801      1   \n",
       "    4      22       1_70    1      rambo      70      0   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1366    40_1891   40  star wars    1891      3   \n",
       "    26   1365    40_1892   40  star wars    1892      3   \n",
       "    27   1364    40_1895   40  star wars    1895      2   \n",
       "    28   1363  40_330459   40  star wars  330459      3   \n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
<<<<<<< HEAD
       "1   0   [11.657399, 10.083591]              0.0             0  1.000000   \n",
       "    1    [9.456276, 13.265001]              0.0             1  0.630930   \n",
       "    2    [6.036743, 11.113943]              0.0             2  0.500000   \n",
       "    3          [0.0, 6.869545]              0.0             3  0.430677   \n",
       "    4         [0.0, 11.113943]              0.0             4  0.386853   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]              0.0            25  0.210310   \n",
       "    26              [0.0, 0.0]              0.0            26  0.208015   \n",
       "    27              [0.0, 0.0]              0.0            27  0.205847   \n",
       "    28              [0.0, 0.0]              0.0            28  0.203795   \n",
       "    29              [0.0, 0.0]              0.0             9  0.289065   \n",
       "\n",
       "             gain        dcg      lambda       delta  \n",
       "qid                                                   \n",
       "1   0   15.000000  30.700871  213.776822  427.553645  \n",
       "    1    4.416508  30.700871   48.987136   97.974272  \n",
       "    2    3.500000  30.700871   31.835338   63.670676  \n",
       "    3    1.292030  30.700871    6.723500   13.447000  \n",
       "    4    5.802792  30.700871   41.948006   83.896012  \n",
       "...           ...        ...         ...         ...  \n",
       "40  25   0.000000  30.207651   -9.846078  -19.692155  \n",
       "    26   0.000000  30.207651   -9.903461  -19.806921  \n",
       "    27   0.000000  30.207651   -9.957655  -19.915309  \n",
       "    28   0.000000  30.207651  -10.008949  -20.017899  \n",
       "    29   0.000000  30.207651  -10.057598  -20.115197  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 91,
=======
       "1   0   [11.657399, 10.083591]         7.292951             0  0.500000   \n",
       "    1    [9.456276, 13.265001]         3.798236             1  0.386853   \n",
       "    2               [0.0, 0.0]        -0.517338             2  0.333333   \n",
       "    3               [0.0, 0.0]        -0.517338             3  0.301030   \n",
       "    4               [0.0, 0.0]        -0.517338             4  0.278943   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]        -0.517338            25  0.173765   \n",
       "    26        [0.0, 2.4547963]        -0.517338            26  0.172195   \n",
       "    27    [6.487482, 2.062405]        -0.517338            27  0.170707   \n",
       "    28   [7.2694716, 4.237955]        -0.517338            28  0.169294   \n",
       "    29              [0.0, 0.0]        -0.517338             1  0.386853   \n",
       "\n",
       "            gain        dcg    lambda  \n",
       "qid                                    \n",
       "1   0   8.000000  13.741487  0.100504  \n",
       "    1   3.094822  13.741487  0.697540  \n",
       "    2   0.333333  13.741487 -0.005952  \n",
       "    3   0.602060  13.741487  2.992312  \n",
       "    4   0.278943  13.741487 -0.022346  \n",
       "...          ...        ...       ...  \n",
       "40  25  1.390123  10.793185 -0.002116  \n",
       "    26  1.377563  10.793185 -0.002126  \n",
       "    27  0.682829  10.793185 -0.002136  \n",
       "    28  1.354350  10.793185 -0.002145  \n",
       "    29  0.386853  10.793185 -0.013697  \n",
       "\n",
       "[1390 rows x 13 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 381,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 528,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "judgments['last_prediction'] = tree.predict(judgments['features'].tolist()) * learning_rate\n",
    "\n",
    "def compute_swaps_scaled(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
<<<<<<< HEAD
    "    # Important - stable sort. Otherwise DCG swaps get kind of wonky due to position discounts\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
=======
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    best_dcg = query_judgments.loc[0, 'dcg']\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
<<<<<<< HEAD
    "    query_judgments['delta'] = 0.0\n",
=======
>>>>>>> 4ef4e05 (Save lambda mart)
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
<<<<<<< HEAD
    "                swap_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                dcg_after_swap = metric(swap_judgments, at=at)\n",
    "\n",
    "                delta = abs(best_dcg - dcg_after_swap)\n",
=======
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
    "                delta = best_dcg - dcg_after_swap\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "\n",
    "                if delta > 0.0:\n",
    "                    \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(model_score_diff))    \n",
    "                    # --------------\n",
    "                    # rho works as follows\n",
    "                    # \n",
    "                    # model ranks                    rho\n",
    "                    # better higher than worse       approaches 0      <-- model currently doing well!\n",
    "                    # better same as worse.          0.5  \n",
    "                    # worse higher than better       approaches 1      <-- model currently doing poorly!\n",
    "                    # \n",
<<<<<<< HEAD
    "                    query_judgments.loc[better, 'delta'] += delta\n",
    "                    \n",
=======
    "\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "                    # Use rho to scale the lambdas\n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "        \n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
<<<<<<< HEAD
    "                    query_judgments.loc[worse, 'delta'] -= delta\n",
    "\n",
    "    return query_judgments\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0.0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "#\n",
=======
    "\n",
    "    return query_judgments\n",
    "\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled, axis=1)\n",
    "\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in on 2 swapped by each result worse than it in query `ramba`\n",
    "\n",
    "```\n",
    "better_grade worse_grade, model_score_diffs, rho,                 dcg_delta\n",
    "2 1                       0.758706128029972  0.31892724571177816  0.02502724555344038\n",
    "2 1                       0.981162516523929  0.2726611758805124   0.04377062727053094\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.11698017724693699\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.14090604137532914\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.08043118677314176\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.17784892734690594\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.19254512210920538\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.20543079947538878\n",
    "2 1                       1.142439045359345  0.24187283082372052  0.21685570619866112\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.22708156316293504\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.23630863576764227\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.24469312448475122\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2523588995024131\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.25940563061320177\n",
    "2 0                       1.142439045359345  0.24187283082372052  0.2659145510893115\n",
    "...\n",
    "2 0                       1.142439045359345  0.24187283082372052 0.34214193097965406\n",
    "```\n",
    "\n",
    "Summing all the model score diffs, we see those are rather high. This results in a high-ish rho between (for each value here 0.25-0.31). So each dcg_delta is added to the model.\n",
    "\n",
    "What's the intuition here? The model hasn't entirely nailed this example, the model feels there's more 'dcg_delta' to learn to push it away from those less relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming out to more of `rambo`\n",
    "\n",
    "We see a similar pattern in results with mediocre grades (2 and 3) where the resulting rho-scaled lambda's are higher than you might expect. The model's happy with the position of 0, but the ranking of other results could be separated more. The model diff should be higher when compared to the dcg diff to push the middling results away from the irrelevant result.\n",
    "\n",
    "So the next tree learns these lambdas using the resulting features moreso than other results."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 92,
=======
   "execution_count": 382,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 529,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
<<<<<<< HEAD
       "      <th>display_rank</th>\n",
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>delta</th>\n",
=======
       "      <th>grade</th>\n",
       "      <th>last_prediction</th>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <th>lambda</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rambo</td>\n",
<<<<<<< HEAD
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.553645</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.868699</td>\n",
       "      <td>-9.934349</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.149295</td>\n",
       "      <td>-10.074647</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.276314</td>\n",
       "      <td>-10.138157</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.395685</td>\n",
       "      <td>-10.197842</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.508155</td>\n",
       "      <td>-10.254078</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.336529</td>\n",
       "      <td>-10.168264</td>\n",
=======
       "      <td>4</td>\n",
       "      <td>7.292951</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.002105</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>0.697540</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.024589</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.019625</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rambo</td>\n",
<<<<<<< HEAD
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.432963</td>\n",
       "      <td>-10.216481</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.524423</td>\n",
       "      <td>-10.262211</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.611330</td>\n",
       "      <td>-10.305665</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.694056</td>\n",
       "      <td>-10.347028</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.069351</td>\n",
       "      <td>-10.534675</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.147879</td>\n",
       "      <td>-10.573939</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.222977</td>\n",
       "      <td>-10.611488</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.294893</td>\n",
       "      <td>-10.647447</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.363851</td>\n",
       "      <td>-10.681926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.430053</td>\n",
       "      <td>-10.715026</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.493681</td>\n",
       "      <td>-10.746841</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.554902</td>\n",
       "      <td>-10.777451</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.013760</td>\n",
       "      <td>-10.006880</td>\n",
=======
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.025089</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.024931</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.020225</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rambo</td>\n",
<<<<<<< HEAD
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.712923</td>\n",
       "      <td>-9.856462</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.974272</td>\n",
       "      <td>48.987136</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.545028</td>\n",
       "      <td>-9.772514</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.670676</td>\n",
       "      <td>31.835338</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.447000</td>\n",
       "      <td>6.723500</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.896012</td>\n",
       "      <td>41.948006</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.400912</td>\n",
       "      <td>-4.200456</td>\n",
       "      <td>[0.0, 7.8627386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.024956</td>\n",
       "      <td>-5.012478</td>\n",
       "      <td>[0.0, 4.563677]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.243092</td>\n",
       "      <td>-7.621546</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.950401</td>\n",
       "      <td>-7.975200</td>\n",
=======
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.023536</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rambo</td>\n",
<<<<<<< HEAD
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.536694</td>\n",
       "      <td>-8.268347</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rambo</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.032666</td>\n",
       "      <td>-8.516333</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rambo</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.459201</td>\n",
       "      <td>-8.729601</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.831043</td>\n",
       "      <td>-8.915522</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.158927</td>\n",
       "      <td>-9.079464</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.450871</td>\n",
       "      <td>-9.225435</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.712994</td>\n",
       "      <td>-9.356497</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.950060</td>\n",
       "      <td>-9.475030</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.165834</td>\n",
       "      <td>-9.582917</td>\n",
=======
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1.098537</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rambo</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2.992312</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.022346</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.237120</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.235248</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.233307</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.231291</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.229197</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.227018</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.224748</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.222381</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.219909</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.217324</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.214616</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.032095</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.050806</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.039664</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.195112</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.191166</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.186963</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.177656</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rambo</td>\n",
<<<<<<< HEAD
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.363338</td>\n",
       "      <td>-9.681669</td>\n",
=======
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.172476</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.166880</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.160809</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.154187</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.146926</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.138911</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rambo</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.129999</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rambo</td>\n",
<<<<<<< HEAD
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.613868</td>\n",
       "      <td>-10.806934</td>\n",
=======
       "      <td>0</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>-0.238927</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>[0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "   keywords  display_rank  grade  last_prediction       delta      lambda  \\\n",
       "0     rambo             0      4              0.0  427.553645  213.776822   \n",
       "21    rambo            21      0              0.0  -19.868699   -9.934349   \n",
       "23    rambo            23      0              0.0  -20.149295  -10.074647   \n",
       "24    rambo            24      0              0.0  -20.276314  -10.138157   \n",
       "25    rambo            25      0              0.0  -20.395685  -10.197842   \n",
       "26    rambo            26      0              0.0  -20.508155  -10.254078   \n",
       "27    rambo            27      1              0.0  -20.336529  -10.168264   \n",
       "28    rambo            28      1              0.0  -20.432963  -10.216481   \n",
       "29    rambo            29      1              0.0  -20.524423  -10.262211   \n",
       "30    rambo            30      1              0.0  -20.611330  -10.305665   \n",
       "31    rambo            31      1              0.0  -20.694056  -10.347028   \n",
       "32    rambo            32      0              0.0  -21.069351  -10.534675   \n",
       "33    rambo            33      0              0.0  -21.147879  -10.573939   \n",
       "34    rambo            34      0              0.0  -21.222977  -10.611488   \n",
       "35    rambo            35      0              0.0  -21.294893  -10.647447   \n",
       "36    rambo            36      0              0.0  -21.363851  -10.681926   \n",
       "37    rambo            37      0              0.0  -21.430053  -10.715026   \n",
       "38    rambo            38      0              0.0  -21.493681  -10.746841   \n",
       "39    rambo            39      0              0.0  -21.554902  -10.777451   \n",
       "22    rambo            22      0              0.0  -20.013760  -10.006880   \n",
       "20    rambo            20      0              0.0  -19.712923   -9.856462   \n",
       "1     rambo             1      3              0.0   97.974272   48.987136   \n",
       "19    rambo            19      0              0.0  -19.545028   -9.772514   \n",
       "2     rambo             2      3              0.0   63.670676   31.835338   \n",
       "3     rambo             3      2              0.0   13.447000    6.723500   \n",
       "4     rambo             4      4              0.0   83.896012   41.948006   \n",
       "5     rambo             5      1              0.0   -8.400912   -4.200456   \n",
       "6     rambo            40      1              0.0  -10.024956   -5.012478   \n",
       "7     rambo             7      0              0.0  -15.243092   -7.621546   \n",
       "8     rambo             8      0              0.0  -15.950401   -7.975200   \n",
       "9     rambo             9      0              0.0  -16.536694   -8.268347   \n",
       "10    rambo            10      0              0.0  -17.032666   -8.516333   \n",
       "11    rambo            11      0              0.0  -17.459201   -8.729601   \n",
       "12    rambo            12      0              0.0  -17.831043   -8.915522   \n",
       "13    rambo            13      0              0.0  -18.158927   -9.079464   \n",
       "14    rambo            14      0              0.0  -18.450871   -9.225435   \n",
       "15    rambo            15      0              0.0  -18.712994   -9.356497   \n",
       "16    rambo            16      0              0.0  -18.950060   -9.475030   \n",
       "17    rambo            17      0              0.0  -19.165834   -9.582917   \n",
       "18    rambo            18      0              0.0  -19.363338   -9.681669   \n",
       "40    rambo             6      0              0.0  -21.613868  -10.806934   \n",
       "\n",
       "                  features  \n",
       "0   [11.657399, 10.083591]  \n",
       "21              [0.0, 0.0]  \n",
       "23              [0.0, 0.0]  \n",
       "24              [0.0, 0.0]  \n",
       "25              [0.0, 0.0]  \n",
       "26              [0.0, 0.0]  \n",
       "27              [0.0, 0.0]  \n",
       "28              [0.0, 0.0]  \n",
       "29              [0.0, 0.0]  \n",
       "30              [0.0, 0.0]  \n",
       "31              [0.0, 0.0]  \n",
       "32              [0.0, 0.0]  \n",
       "33              [0.0, 0.0]  \n",
       "34              [0.0, 0.0]  \n",
       "35              [0.0, 0.0]  \n",
       "36              [0.0, 0.0]  \n",
       "37              [0.0, 0.0]  \n",
       "38              [0.0, 0.0]  \n",
       "39              [0.0, 0.0]  \n",
       "22              [0.0, 0.0]  \n",
       "20              [0.0, 0.0]  \n",
       "1    [9.456276, 13.265001]  \n",
       "19              [0.0, 0.0]  \n",
       "2    [6.036743, 11.113943]  \n",
       "3          [0.0, 6.869545]  \n",
       "4         [0.0, 11.113943]  \n",
       "5         [0.0, 7.8627386]  \n",
       "6          [0.0, 4.563677]  \n",
       "7               [0.0, 0.0]  \n",
       "8               [0.0, 0.0]  \n",
       "9               [0.0, 0.0]  \n",
       "10              [0.0, 0.0]  \n",
       "11              [0.0, 0.0]  \n",
       "12              [0.0, 0.0]  \n",
       "13              [0.0, 0.0]  \n",
       "14              [0.0, 0.0]  \n",
       "15              [0.0, 0.0]  \n",
       "16              [0.0, 0.0]  \n",
       "17              [0.0, 0.0]  \n",
       "18              [0.0, 0.0]  \n",
       "40              [0.0, 0.0]  "
      ]
     },
     "execution_count": 92,
=======
       "   keywords  grade  last_prediction    lambda                features\n",
       "0     rambo      4         7.292951  0.100504  [11.657399, 10.083591]\n",
       "26    rambo      4        -0.517338  0.000000        [0.0, 11.113943]\n",
       "24    rambo      3        -0.517338 -0.002105   [6.036743, 11.113943]\n",
       "1     rambo      3         3.798236  0.697540   [9.456276, 13.265001]\n",
       "25    rambo      2        -0.517338 -0.024589         [0.0, 6.869545]\n",
       "10    rambo      1        -0.517338 -0.019625              [0.0, 0.0]\n",
       "28    rambo      1        -0.517338 -0.025089         [0.0, 4.563677]\n",
       "27    rambo      1        -0.517338 -0.024931        [0.0, 7.8627386]\n",
       "11    rambo      1        -0.517338 -0.020225              [0.0, 0.0]\n",
       "20    rambo      1        -0.517338 -0.023536              [0.0, 0.0]\n",
       "9     rambo      1        -0.517338  1.098537              [0.0, 0.0]\n",
       "3     rambo      1        -0.517338  2.992312              [0.0, 0.0]\n",
       "4     rambo      0        -0.517338 -0.022346              [0.0, 0.0]\n",
       "39    rambo      0        -0.517338 -0.237120              [0.0, 0.0]\n",
       "38    rambo      0        -0.517338 -0.235248              [0.0, 0.0]\n",
       "37    rambo      0        -0.517338 -0.233307              [0.0, 0.0]\n",
       "36    rambo      0        -0.517338 -0.231291              [0.0, 0.0]\n",
       "35    rambo      0        -0.517338 -0.229197              [0.0, 0.0]\n",
       "34    rambo      0        -0.517338 -0.227018              [0.0, 0.0]\n",
       "33    rambo      0        -0.517338 -0.224748              [0.0, 0.0]\n",
       "32    rambo      0        -0.517338 -0.222381              [0.0, 0.0]\n",
       "31    rambo      0        -0.517338 -0.219909              [0.0, 0.0]\n",
       "30    rambo      0        -0.517338 -0.217324              [0.0, 0.0]\n",
       "29    rambo      0        -0.517338 -0.214616              [0.0, 0.0]\n",
       "2     rambo      0        -0.517338 -0.005952              [0.0, 0.0]\n",
       "5     rambo      0        -0.517338 -0.032095              [0.0, 0.0]\n",
       "8     rambo      0        -0.517338 -0.050806              [0.0, 0.0]\n",
       "6     rambo      0        -0.517338 -0.039664              [0.0, 0.0]\n",
       "23    rambo      0        -0.517338 -0.195112              [0.0, 0.0]\n",
       "22    rambo      0        -0.517338 -0.191166              [0.0, 0.0]\n",
       "21    rambo      0        -0.517338 -0.186963              [0.0, 0.0]\n",
       "7     rambo      0        -0.517338 -0.045760              [0.0, 0.0]\n",
       "19    rambo      0        -0.517338 -0.177656              [0.0, 0.0]\n",
       "18    rambo      0        -0.517338 -0.172476              [0.0, 0.0]\n",
       "17    rambo      0        -0.517338 -0.166880              [0.0, 0.0]\n",
       "16    rambo      0        -0.517338 -0.160809              [0.0, 0.0]\n",
       "15    rambo      0        -0.517338 -0.154187              [0.0, 0.0]\n",
       "14    rambo      0        -0.517338 -0.146926              [0.0, 0.0]\n",
       "13    rambo      0        -0.517338 -0.138911              [0.0, 0.0]\n",
       "12    rambo      0        -0.517338 -0.129999              [0.0, 0.0]\n",
       "40    rambo      0        -0.517338 -0.238927              [0.0, 0.0]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 382,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 529,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "lambdas_per_query.loc[1, :][['keywords', 'display_rank',  'grade', 'last_prediction', 'delta', 'lambda', 'features']].sort_values('last_prediction', ascending=False)"
=======
    "lambdas_per_query.loc[1, :][['keywords', 'grade', 'last_prediction', 'lambda', 'features']].sort_values('grade', ascending=False)"
>>>>>>> 4ef4e05 (Save lambda mart)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 93,
=======
   "execution_count": 383,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 530,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 93,
=======
     "execution_count": 383,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 530,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree2 = DecisionTreeRegressor()\n",
    "tree2.fit(train_set['features'].tolist(), train_set['lambda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More 'oomph' in second tree for the last tree's error cases\n",
    "\n",
    "We see in the following lambdas our next tree learns more about the areas the last model seemed to need correction. \n",
    "\n",
    "The first example is well covered by the first tree."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 94,
=======
   "execution_count": 384,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 531,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([173.48027244])"
      ]
     },
     "execution_count": 94,
=======
       "array([0.10050401])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 384,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 531,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[11.6, 10.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example reflects some of the middling ranked results"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 95,
=======
   "execution_count": 385,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 532,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([6.72350002])"
      ]
     },
     "execution_count": 95,
=======
       "array([-0.02458865])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 385,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 532,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2.predict([[0.0, 6.869545]])"
   ]
  },
  {
<<<<<<< HEAD
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.75718641e-16]],\n",
       "\n",
       "       [[-4.26415296e+00]],\n",
       "\n",
       "       [[ 9.29026112e+01]],\n",
       "\n",
       "       [[-5.17337837e+00]],\n",
       "\n",
       "       [[ 3.79823562e+01]],\n",
       "\n",
       "       [[ 7.29295071e+01]],\n",
       "\n",
       "       [[ 1.18053928e+02]]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree_.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'apply',\n",
       " 'capacity',\n",
       " 'children_left',\n",
       " 'children_right',\n",
       " 'compute_feature_importances',\n",
       " 'compute_partial_dependence',\n",
       " 'decision_path',\n",
       " 'feature',\n",
       " 'impurity',\n",
       " 'max_depth',\n",
       " 'max_n_classes',\n",
       " 'n_classes',\n",
       " 'n_features',\n",
       " 'n_leaves',\n",
       " 'n_node_samples',\n",
       " 'n_outputs',\n",
       " 'node_count',\n",
       " 'predict',\n",
       " 'threshold',\n",
       " 'value',\n",
       " 'weighted_n_node_samples']"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree.tree_.decision_path(np.array([[0.0, 6.869545]], dtype='float32')).toarray()\n",
    "dir(tree.tree_)"
   ]
  },
  {
>>>>>>> 4ef4e05 (Save lambda mart)
=======
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three - Weigh each leaf's predictions\n",
    "\n",
    "Because we're dealing with trees, each leaf corresponds to a set of examples that have been grouped to this node. In addition to per-swap 'rho' we also care about a per-swap 'weight', referred to in gradient boosting as 'gamma'. \n",
    "\n",
    "Gamma means picking a weight for this sub-model that best predicts the final function.\n",
    "\n",
    "First we group by the paths in the tree to uniquely identify each leaf"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 96,
=======
   "execution_count": 414,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 534,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
<<<<<<< HEAD
       "      <th>qid</th>\n",
       "      <th>uid</th>\n",
=======
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
<<<<<<< HEAD
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7555</td>\n",
=======
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>1</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
<<<<<<< HEAD
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>106.888411</td>\n",
=======
       "      <td>7.292951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>0.099688</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
<<<<<<< HEAD
=======
       "      <td>1</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
<<<<<<< HEAD
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>48.010828</td>\n",
       "      <td>26.458003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>31.382749</td>\n",
       "      <td>18.143963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>6.460558</td>\n",
       "      <td>7.448315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>43.639845</td>\n",
       "      <td>21.819923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
=======
       "      <td>3.798236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>3.094822</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>0.697540</td>\n",
       "      <td>0.740546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>1_32221</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>32221</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>0.005887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>1_801</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>801</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>2.992312</td>\n",
       "      <td>1.507942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1_70</td>\n",
       "      <td>1</td>\n",
       "      <td>rambo</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>13.741487</td>\n",
       "      <td>-0.022346</td>\n",
       "      <td>0.016692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>4.923039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>4.951730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>4.978827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>5.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>40_43052</td>\n",
=======
       "      <th rowspan=\"5\" valign=\"top\">40</th>\n",
       "      <th>25</th>\n",
       "      <td>1366</td>\n",
       "      <td>40_1891</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1891</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1365</td>\n",
       "      <td>40_1892</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1892</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 2.4547963]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002126</td>\n",
       "      <td>0.002125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1364</td>\n",
       "      <td>40_1895</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>1895</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.487482, 2.062405]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>0.682829</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002136</td>\n",
       "      <td>0.002135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1363</td>\n",
       "      <td>40_330459</td>\n",
       "      <td>40</td>\n",
       "      <td>star wars</td>\n",
       "      <td>330459</td>\n",
       "      <td>3</td>\n",
       "      <td>[7.2694716, 4.237955]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>1.354350</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.002145</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1389</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>40</td>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
<<<<<<< HEAD
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>5.028799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid        uid   keywords   docId  grade                features  \\\n",
       "0       1     1_7555      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1       1     1_1370      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2       1     1_1369      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1    1_13258      rambo   13258      2         [0.0, 6.869545]   \n",
       "4       1     1_1368      rambo    1368      4        [0.0, 11.113943]   \n",
       "...   ...        ...        ...     ...    ...                     ...   \n",
       "1385   40   40_37079  star wars   37079      0              [0.0, 0.0]   \n",
       "1386   40  40_126757  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40   40_39797  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40   40_18112  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40   40_43052  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount       gain  train_dcg  \\\n",
       "0                   0             0  1.000000  15.000000  30.700871   \n",
       "1                   0             1  0.630930   4.416508  30.700871   \n",
       "2                   0             2  0.500000   3.500000  30.700871   \n",
       "3                   0             3  0.430677   1.292030  30.700871   \n",
       "4                   0             4  0.386853   5.802792  30.700871   \n",
       "...               ...           ...       ...        ...        ...   \n",
       "1385                0            25  0.210310   0.000000  30.207651   \n",
       "1386                0            26  0.208015   0.000000  30.207651   \n",
       "1387                0            27  0.205847   0.000000  30.207651   \n",
       "1388                0            28  0.203795   0.000000  30.207651   \n",
       "1389                0             9  0.289065   0.000000  30.207651   \n",
       "\n",
       "            dcg      lambda      weight  \n",
       "0     30.552986  213.776822  106.888411  \n",
       "1     30.552986   48.010828   26.458003  \n",
       "2     30.552986   31.382749   18.143963  \n",
       "3     30.552986    6.460558    7.448315  \n",
       "4     30.552986   43.639845   21.819923  \n",
       "...         ...         ...         ...  \n",
       "1385  30.120435   -9.846078    4.923039  \n",
       "1386  30.120435   -9.903461    4.951730  \n",
       "1387  30.120435   -9.957655    4.978827  \n",
       "1388  30.120435  -10.008949    5.004475  \n",
       "1389  30.120435  -10.057598    5.028799  \n",
       "\n",
       "[1390 rows x 14 columns]"
      ]
     },
     "execution_count": 96,
=======
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>11.668802</td>\n",
       "      <td>10.793185</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>0.013544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        uid  qid   keywords   docId  grade  \\\n",
       "qid                                                       \n",
       "1   0       0     1_7555    1      rambo    7555      4   \n",
       "    1       1     1_1370    1      rambo    1370      3   \n",
       "    2      39    1_32221    1      rambo   32221      0   \n",
       "    3      29      1_801    1      rambo     801      1   \n",
       "    4      22       1_70    1      rambo      70      0   \n",
       "...       ...        ...  ...        ...     ...    ...   \n",
       "40  25   1366    40_1891   40  star wars    1891      3   \n",
       "    26   1365    40_1892   40  star wars    1892      3   \n",
       "    27   1364    40_1895   40  star wars    1895      2   \n",
       "    28   1363  40_330459   40  star wars  330459      3   \n",
       "    29   1389   40_43052   40  star wars   43052      0   \n",
       "\n",
       "                      features  last_prediction  display_rank  discount  \\\n",
       "qid                                                                       \n",
       "1   0   [11.657399, 10.083591]         7.292951             0  0.500000   \n",
       "    1    [9.456276, 13.265001]         3.798236             1  0.386853   \n",
       "    2               [0.0, 0.0]        -0.517338             2  0.333333   \n",
       "    3               [0.0, 0.0]        -0.517338             3  0.301030   \n",
       "    4               [0.0, 0.0]        -0.517338             4  0.278943   \n",
       "...                        ...              ...           ...       ...   \n",
       "40  25              [0.0, 0.0]        -0.517338            25  0.173765   \n",
       "    26        [0.0, 2.4547963]        -0.517338            26  0.172195   \n",
       "    27    [6.487482, 2.062405]        -0.517338            27  0.170707   \n",
       "    28   [7.2694716, 4.237955]        -0.517338            28  0.169294   \n",
       "    29              [0.0, 0.0]        -0.517338             1  0.386853   \n",
       "\n",
       "            gain  train_dcg        dcg    lambda    weight  \n",
       "qid                                                         \n",
       "1   0   8.000000  13.741487  13.741487  0.100504  0.099688  \n",
       "    1   3.094822  13.741487  13.741487  0.697540  0.740546  \n",
       "    2   0.333333  13.741487  13.741487 -0.005952  0.005887  \n",
       "    3   0.602060  13.741487  13.741487  2.992312  1.507942  \n",
       "    4   0.278943  13.741487  13.741487 -0.022346  0.016692  \n",
       "...          ...        ...        ...       ...       ...  \n",
       "40  25  1.390123  11.668802  10.793185 -0.002116  0.002115  \n",
       "    26  1.377563  11.668802  10.793185 -0.002126  0.002125  \n",
       "    27  0.682829  11.668802  10.793185 -0.002136  0.002135  \n",
       "    28  1.354350  11.668802  10.793185 -0.002145  0.002144  \n",
       "    29  0.386853  11.668802  10.793185 -0.013697  0.013544  \n",
       "\n",
       "[1390 rows x 15 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 414,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 534,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_swaps_scaled_with_weights(query_judgments, axis, metric=dcg, at=10):\n",
    "    \"\"\"Compute the 'lambda' the DCG impact of every query result swapped with every-other query result\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort to see ideal ordering\n",
    "    # This isn't strictly nescesarry, but it's helpful to understand the algorithm\n",
<<<<<<< HEAD
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False, kind='stable').reset_index()\n",
=======
    "    query_judgments = query_judgments.sort_values('last_prediction', ascending=False).reset_index()\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "\n",
    "    # Instead of explicitly 'swapping' we just swap the 'display_rank' - where \n",
    "    # in the final ranking this would be placed. We can easily use that to compute DCG\n",
    "    query_judgments['display_rank'] = query_judgments.index.to_series()\n",
    "    query_judgments['train_dcg'] = query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "    train_dcg = query_judgments.loc[0, 'dcg']\n",
<<<<<<< HEAD
    " \n",
    "    qid = query_judgments.loc[0, 'qid']\n",
    "    keywords = query_judgments.loc[0, 'keywords']\n",
    "\n",
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "\n",
    "    for better in range(0,len(query_judgments)):\n",
    "         for worse in range(0,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                return query_judgments\n",
=======
    "\n",
    "    query_judgments['lambda'] = 0.0\n",
    "    query_judgments['weight'] = 0.0\n",
    "    \n",
    "    for better in range(0,len(query_judgments)):\n",
    "        for worse in range(better+1,len(query_judgments)):\n",
    "            if better > at and worse > at:\n",
    "                break\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "                \n",
    "            if query_judgments.loc[better, 'grade'] > query_judgments.loc[worse, 'grade']:\n",
    "                query_judgments = rank_with_swap(query_judgments, better, worse)\n",
    "                query_judgments['dcg'] = metric(query_judgments, at=at)\n",
    "\n",
    "                dcg_after_swap = query_judgments.loc[0, 'dcg']\n",
<<<<<<< HEAD
    "                delta = abs(train_dcg - dcg_after_swap)\n",
    "\n",
    "                if delta != 0.0:\n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff)) \n",
=======
    "                delta = train_dcg - dcg_after_swap\n",
    "\n",
    "                if delta > 0.0:\n",
    "                    assert(not math.isnan(query_judgments.loc[better, 'last_prediction']))\n",
    "                    assert(not math.isnan(query_judgments.loc[worse, 'last_prediction']))\n",
    "                    \n",
    "                    last_model_score_diff = query_judgments.loc[better, 'last_prediction'] - query_judgments.loc[worse, 'last_prediction']\n",
    "                    rho = 1.0 / (1.0 + exp(last_model_score_diff))   \n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "\n",
    "                    assert(delta >= 0.0)\n",
    "                    assert(rho >= 0.0)\n",
    "                   \n",
    "                    query_judgments.loc[better, 'lambda'] += delta * rho\n",
    "                    query_judgments.loc[worse, 'lambda'] -= delta * rho\n",
    "            \n",
    "                    # --------------\n",
    "                    # NEW!\n",
    "                    #  last_model_score_diff        rho         weight\n",
    "                    #      0.0                      0.5         0.25 (max possible value)\n",
    "                    #      100.0                    0.0000      0.0  (max possible value)\n",
    "                    # \n",
    "                    # If the current model has an ambiguous prediction, we include more of the delta in the weight\n",
    "                    # If the current model has a strong prediction, weight approaches 0\n",
    "                    query_judgments.loc[better, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    query_judgments.loc[worse, 'weight'] += rho * (1.0 - rho) * delta;\n",
    "                    #\n",
    "                    # These will be used to rescale each decision tree node's predictions\n",
    "                    # If many results in a leaf node have last model score ~ ambiguous\n",
    "                    #     the resulting model will have a high denominator ~ (1 / deltaDCG)\n",
    "                    # If many results in a leaf node have last model score - not ambiguous, positive\n",
    "                    #     the resulting model will have a low denominator\n",
    "                    #\n",
    "                    # Apparently we want to cancel out the deltas if last model was ambiguous?\n",
    "                    # ---------------\n",
    "\n",
    "                    \n",
    "\n",
    "    return query_judgments\n",
    "\n",
<<<<<<< HEAD
    "# Convert to Pandas Dataframe\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "judgments['last_prediction'] = 0\n",
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
    "lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
=======
    "lambdas_per_query = judgments.groupby('qid').apply(compute_swaps_scaled_with_weights, axis=1)\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "\n",
    "lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 97,
=======
   "execution_count": 389,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 535,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=4)"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 97,
=======
     "execution_count": 389,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 535,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "train_set = lambdas_per_query[['lambda', 'features']]\n",
    "train_set\n",
    "\n",
    "tree3 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "tree3.fit(train_set['features'].tolist(), train_set['lambda'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
    "### Label each row with its unique prediction (ie tree path)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 98,
=======
   "execution_count": 542,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
<<<<<<< HEAD
    "lambdas_per_query['path'] = tree_paths(tree3, train_set['features'].tolist())"
=======
    "lambdas_per_query['path'] = tree_paths(tree3, features)"
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
>>>>>>> 4ef4e05 (Save lambda mart)
=======
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
    "### Override outputs using our own weighted average\n",
    "\n",
    "The typical decision tree uses either the [median or mean of the target values](https://scikit-learn.org/stable/modules/tree.html#regression-criteria) classified to a given leaf node as the prediction. However, in the case of lambdaMART, we want to use a weighted average that accounts for how much of the DCG error out there has been accounted for. Thus the psuedoresponses are summed and divided by the remaining error DCG.\n",
    "\n",
    "rho=0, then an example is weighed by `1/0.25*deltaNDCG` as there's a lot of outstanding DCG error left."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001    1673.720249\n",
       "1010010    1467.050422\n",
       "1100100     538.598514\n",
       "1101000    4655.114588\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-474-421c2af06928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambdas_per_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'groupby'"
     ]
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010101     41.871274\n",
       "1010110     17.304968\n",
       "1011000     39.105907\n",
       "1100000    129.959528\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 100,
=======
   "execution_count": 393,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 544,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
<<<<<<< HEAD
<<<<<<< HEAD
       "1010001    3334.073882\n",
       "1010010    2787.629391\n",
       "1100100     893.120752\n",
       "1101000   -7014.824025\n",
       "Name: lambda, dtype: float64"
      ]
     },
     "execution_count": 100,
=======
       "1010101     21.084755\n",
       "1010110     26.392483\n",
=======
       "1010101     50.899615\n",
       "1010110     34.429523\n",
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
       "1011000     78.211815\n",
       "1100000   -163.540952\n",
       "Name: lambda, dtype: float64"
      ]
     },
<<<<<<< HEAD
     "execution_count": 393,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 544,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_per_query.groupby('path')['lambda'].sum()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 101,
=======
   "execution_count": 394,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 545,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
<<<<<<< HEAD
       "{'1010001': 1.9920138295288714,\n",
       " '1010010': 1.9001592234365985,\n",
       " '1100100': 1.658230999946508,\n",
       " '1101000': -1.5069068425436136}"
      ]
     },
     "execution_count": 101,
=======
       "{'1010101': 0.7250098367343205,\n",
       " '1010110': 1.9965848868529685,\n",
=======
       "{'1010101': 1.2156213554683677,\n",
       " '1010110': 1.989574489468554,\n",
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
       " '1011000': 2.0,\n",
       " '1100000': -1.25839909859753}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 394,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 545,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "round_predictions.to_dict()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 102,
=======
   "execution_count": 395,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": 546,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[Text(167.4, 181.2, 'X[0] <= 10.328\\nmse = 883.211\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(83.7, 108.72, 'X[0] <= 9.182\\nmse = 179.235\\nsamples = 1324\\nvalue = -4.624'),\n",
       " Text(41.85, 36.23999999999998, 'mse = 62.9\\nsamples = 1301\\nvalue = -5.392'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'mse = 4838.036\\nsamples = 23\\nvalue = 38.831'),\n",
       " Text(251.10000000000002, 108.72, 'X[0] <= 13.782\\nmse = 5973.405\\nsamples = 66\\nvalue = 92.753'),\n",
       " Text(209.25, 36.23999999999998, 'mse = 5828.45\\nsamples = 39\\nvalue = 71.478'),\n",
       " Text(292.95, 36.23999999999998, 'mse = 4584.565\\nsamples = 27\\nvalue = 123.484')]"
      ]
     },
     "execution_count": 102,
=======
       "[Text(111.60000000000001, 190.26, 'X[0] <= 8.205\\nmse = 5.779\\nsamples = 1390\\nvalue = -0.0'),\n",
       " Text(55.800000000000004, 135.9, 'mse = 0.912\\nsamples = 1246\\nvalue = -0.101'),\n",
       " Text(167.4, 135.9, 'X[0] <= 8.242\\nmse = 47.043\\nsamples = 144\\nvalue = 0.873'),\n",
       " Text(111.60000000000001, 81.53999999999999, 'mse = 0.0\\nsamples = 1\\nvalue = 78.212'),\n",
       " Text(223.20000000000002, 81.53999999999999, 'X[0] <= 8.4\\nmse = 5.252\\nsamples = 143\\nvalue = 0.332'),\n",
       " Text(167.4, 27.180000000000007, 'mse = 53.276\\nsamples = 12\\nvalue = 2.199'),\n",
       " Text(279.0, 27.180000000000007, 'mse = 0.504\\nsamples = 131\\nvalue = 0.161')]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 395,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
     "execution_count": 546,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
<<<<<<< HEAD
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydeVzU1f7/X0dFQVFDs5I0vYK4oJWKsjPDEi65BIqJC3rdUrTNUENLrdy+UthN/XktXDC7qHXFS5rLLSEl9zIX5Kpk5JI7IC7AsLx+f4x8YpgZBMWZcTjPx+M8mPnM+XzO+/Oew3vO55z3+7wFSUgkEonENNQytwASiURSk5BGVyKRSEyINLoSiURiQqTRlUgkEhMija5EIpGYEGl0JRKJxIRIoyuRSCQmRBpdiUQiMSHS6EokEokJkUZXIpFITIg0uhKJRGJCpNGVSCQSEyKNrkQikZgQaXQlEonEhEijK5FIJCZEGl2JRCIxIdLoSiQSiQmRRlcikUhMSB1zCyCxTOzs7C7n5+c/bW45HndsbW2v5OXlPWNuOSSWg5A50iSGEEJQ9o2HRwgBksLcckgsBzm9IJFIJCZEGl2JRCIxIdLoSiQSiQmRRldSKTIyMtCtWzdoNBoAQExMDGbNmgUAsLOzQ9++fZW6MTEx8Pb2hq+vL44fPw4ASE1NxYsvvoixY8c+MhmXLFkCFxcXODs76xz/4Ycf4OnpCU9PTyQkJOidd/ToUXh5eUGlUsHb2xtHjx4FAKxYsQI9evSAn58fwsPDUVBQAAA4fPgwPDw8oFKp0Lt3b9y8efOR3ZPECiEpiyx6Rds1dJk3bx7nzJnDs2fPsmvXrszPzydJOjk5KXVOnTpFtVrNkpISpqenU61WK58lJydzzJgxetc1xK1btypVryyXL1+mRqPRkaeoqIjPP/88r1+/zrt37/KFF15gbm6uznkajYYlJSUkyR9++IGDBg0iSZ45c4bFxcUkyalTpzIuLo4kOXDgQKakpJAkP/roI3722WdGZbqnR7N/n7JYTpEjXUmlmTp1KrZs2YLw8HB8+umnqFevnl6d5ORk9O/fH0IItG/fHteuXUNRUVGlrq/RaJCYmIiwsDAMGzasyvI9/fTTsLGx0TmWkZGB1q1bo2nTprCzs4OXlxcOHTqkU8fGxgZCaB0McnJy8PzzzwMAnJ2dUauW9l+kXr16qF27NgDA1dUVOTk5AIDs7Gw89dRTVZZVUnORfrqSSmNjYwM/Pz9s2bIFPj4+BuvcuHEDjo6OyvvGjRsjOzsbzZo1M3rdAwcOYNWqVcjIyECvXr0QGxuLli1bAgDOnTuHiIgIvXMGDRqEyZMn31fmGzduwMHBQXnv4OCAGzdu6NXbt28f3n77bZw/fx6bNm3S+Sw9PR3btm3Djz/+CAAICQlB//79MXPmTDRs2BALFy68rxwSSSnS6EoqTVpaGn766ScEBQXhiy++wPjx4/XqNG3aFNnZ2cr73NxcHaNniKSkJOzfvx+TJ0/GwIED0aRJE+Wz5557DikpKQ8sc3l5cnJy0LRpU716np6e2L9/P/bv34/XX38dBw8eBABkZmZi5MiR2LhxIxo0aAAAmDhxIjZt2gQ3NzcsXLgQsbGxmD59+gPLKKlZyOkFSaUoKSnBhAkTsGzZMixcuBBLlizBlStX9Oqp1Wps3boVJHHmzBk0bdoUdepU/Ns+b948/PTTT6hXrx4iIiLwyiuvYMOGDQC0I121Wq1Xli5dWim5nZ2dkZmZiezsbBQUFGDv3r1wc3PTqZOfn6+8dnBwQP369QEAly9fRlhYGOLi4tCmTRudc0pH7s2aNVOmGiSSSmHuSWVZLLOg3ELa0qVL+eabbyrvExMTOWTIEJK6C2kkuXDhQnp5edHb25u//vqrcryyC2mXLl3iihUr7luvPAkJCQwMDKSdnR0DAwO5e/dukuTOnTvp4eFBDw8Prlu3Tqk/dOhQkuTGjRvp5+dHtVpNtVrNI0eOkCRHjhzJli1bUqVSUaVSKTKlpKTQ3d2dKpWK/v7+vHjxolGZIBfSZClXZBiwxCBVCQNu27Yt2rVrhy1bthitk5qaiqioKKjV6ho1ByrDgCXlkUZXYhC590L1II2upDxyTlcikUhMiDS6kseK7OxsBAcHQ6VSwcvLC0eOHAFgPEps4MCBUKlUcHNzw+LFi/Wut2/fPiUaLSAgAGfPngUAnD59Gl27doW9vT1SU1OV+t988w06dOgAW1tbE9ytxCox96SyLJZZYCAizRJYsmQJ58yZQ5Lcs2cPQ0NDSRqPEisoKCBJFhYW0tnZWS8a7eLFi7x9+zZJcuvWrRw+fDhJ8s6dO8zKyuLIkSO5Z88epf61a9eYl5ent3hoDMiFNFnKFemnK6k0mZmZGDRoEDp27IijR49ixIgROH/+PA4dOoRWrVohISEBmZmZGDZsGOrWrQuSSExMRK1atTBu3Dhcv34dJLFixQq4uLg8kAwdOnTAd999BwDIyspSosHKR4m1bdsWAFC3bl0AwN27d/Hcc88p7mCllA3kKBt1Vr9+fb26APDkk08+kNwSiYK5rb4slllgYKT7+++/s3nz5rxz5w7z8vJob2+vuFcFBgYyPT2dK1eu5OzZs5VzSkpKOH36dCYkJJAkT5w4wQEDBuhde+LEiYprVml5+eWX9eplZWXR09OTrq6ufPbZZ/nbb7+RJI8cOcKWLVvS1dWVHh4e1Gg0yjn9+/dns2bNOGvWLL3rlXL79m16enrquLiR1BvpliJHurI8aDG7ALJYZjFmdAMCApT3bdq0UV6PHDmSqampvHXrFqOjozl06FBGR0ezoKCAffr0oYeHh2JMy26CU1XeffddxsTEkCT37dvHXr16kSQ9PDx46NAhkuSCBQu4cOFCnfNu377Nrl27Mi0tTe+a+fn57NmzJ5OSkvQ+k0ZXluoucnpBUiVKN4Yp/xrQ/oDXqlUL8+fPBwCMHj0aO3bsgKurKzw9PRESEgIAyvaQZYmMjMTJkyd1jtnb2xv0/TUWDVb2eEZGBkpKSlBcXAwbGxvY2dkppSxFRUUYMmQIwsPD0a9fv0rrQSJ5YMxt9WWxzAIjI93AwEDlfdnRXumIcMOGDfTx8aFKpWJwcDCzsrKYk5PDIUOG0N/fn/7+/nqj0Kpw8eJFBgQEUKVSsUePHkxOTiZpOEosNzdXGV17eHjw008/Va5TGo22evVqNmzYUKk3ceJEktppjMDAQDZv3pxubm6cOXMmSW1UXdmot40bN1YoL+RIV5ZyRQZHSAwigyOqBxkcISmP9NOVSCQSEyKNrkQikZgQaXQlFkP53GaPitTUVHTu3Bm2tra4cOGCcnzy5MlQqVTo0aMHpk2bphxfsWIF3N3d4evrq7O3r6FccBLJfTH3pLIslllghoi0yrphPSw5OTm8desWVSoVz58/rxwvjV4jST8/P544cYJXrlxhly5dqNFomJOTw27durG4uLjCXHBlgVxIk6VckS5jkvtiKMrs+PHjmD17NoqKiuDg4IANGzbAzs4OarUaXbp0wcmTJ1FQUIDx48cjPj4eV65cwcaNG+Hi4gK1Wg1XV1ecPn0aJSUlSEhI0MkzVlhYiMjISPz222/QaDSIiYmBp6cn5s6di6SkJNjb26Nv376YMmXKA91P48aNDR4vjV7TaDSoX78+HB0dcebMGXTs2BE2NjZo3Lgx6tSpg8zMTKO54O63YbtEIqcXJPdl165deOmll5CcnIyUlBQ88cQT6NatG5KTk7Fnzx506NABGzduVOqrVCrs2LEDzs7OOHToEHbs2IGoqCisWrVKqePu7o7//ve/GDZsGGJiYnTaW7lyJZycnLBr1y4kJiYqxvWrr75CcnIydu3ahbfeektPztDQUL0ME1VN+f7aa6+hTZs2cHR0ROPGjeHk5IQjR44gNzcXFy5cQFpaGrKysvRyr5XmgpNI7of8WZbcl8GDB2P+/PkYNmwYWrVqhTlz5iAtLQ3vvfceCgoKcOXKFTRq1Eip361bNwBAixYt4OTkpLwuTewIAF5eXsrfxMREnfaOHz+OvXv3Yvv27QCgBEAsXboUkyZNQlFRESZMmKCXHLN8QskHYcWKFSgsLERoaCi2b9+OPn36YM6cOejbty+aN2+OF198EY6Ojg+UC04iAaTRlVQCQ1FmcXFx+OCDD+Dp6Ylp06aB/Mun11jUWtk6+/fvh7OzM/bv34927drptOfq6gpnZ2e8/fbbAP6KYPP09ERgYCDOnTuHkJAQ/PzzzzrnhYaGIisrS+eYs7Mz4uLiKnWf+fn5sLW1hY2NDezt7ZUNb8LCwhAWFoZLly5h7NixcHR0hFqtxqRJk/DWW28hIyOjUrngJBJAGl1JJdiyZQuWLFmC2rVro169evDx8cHt27cxZswYtG/fHo0aNdIZ6VaGX375BfHx8SguLkZCQoLOZ+PGjcPkyZPh7+8PAOjSpQtiY2MREhKC/Px85OfnY9KkSXrXrOxINz09Ha+//jqOHj2K8PBwvPrqq0om4jt37kCj0cDPzw9qtRoAEBERgfPnz6NBgwZYsmQJAKBdu3Z46aWX4OPjAyEEli1bVqX7l9RcZESaxCCPMiJNrVZj3bp1aNGixSO5viUhI9Ik5ZELaRKJRGJC5EhXYhC590L1IEe6kvLIka5EIpGYEGl0JSYnMzMTQUFBZmmbJKZMmQJfX18EBQXphAGXkpOTgwEDBsDX1xejRo0yuP+vRPKgSKMrqVH897//xfXr17Fnzx5MmzYNM2fO1KuzaNEiDBgwAHv27IGjoyO++uorM0gqsVak0ZVUC1FRUfj3v/8NQJuN4fnnn0dhYSFmzJiBgIAAdO3aFcuXL9c7b9SoUUqK85SUFCWC7MSJEwgKCkJAQADCwsJw9+7dapEzOTlZyWDx0ksv4eDBgxXWeeWVV5CcnFwtbUskgPTTlVQTo0aNwowZMzBw4EDs2LEDAQEBsLGxwcyZM9GgQQMUFBSgc+fOlQ7LjYyMxLp16/Dcc89h2bJl+Pzzz3VCfzUaDYKDg/XO8/Hxwdy5c41et2z4rhACxcXFenWysrLwxBNPAAAcHBxw48aNSskskVQGaXQl1UKnTp1w7do1XL16FfHx8YiOjgYALF++HJs3b0bt2rVx9epVXL16Vec8YxFraWlpiIiIAAAUFBQogQql1K1bV2ebRWPk5eWhd+/eAIBZs2bphO+SNBhF1qRJE+Tk5MDBwQE5OTlo2rTp/RUgkVQSaXQl1cawYcOwbNkyZGZmokuXLsjOzsbq1atx7NgxFBYWol27dijvhtakSROcO3cOAHDo0CHleKdOnZCQkIDmzZsD0E9mWdmRrp2dnY5xLiwsxPr16xESEoJdu3bBzc1N7xpqtRpJSUkYOXIkkpKS9Ay+RPIwSD9diUEexE83KysLLVu2xIcffoh33nkHJDF48GBcuHABHTt2xJEjR5CUlISioiKMHTsW33//PdLT0zF06FA8++yz+Nvf/oa8vDzExcXhxIkTeOedd1BYWAgAmDZtGnr16vXQ90USb7/9Nn7++WfUq1cPq1evRsuWLbF9+3Zcu3YNI0aMQHZ2NkaOHImcnBy0bt0acXFxyraPVUX66UrKI42uxCAyOKJ6kEZXUh7pvSCRSCQmRBpdiUQiMSHS6EokEokJkd4LEoPY2tpeEUI8bW45HndsbW2vmFsGiWUhF9IkD4UQogWArQD2AZhMssjMIj0ShBCvA4gG8ApJ/TA2iaSSyOkFyQMjhHgRWmO7DsBEazW4AEByCYAJALYKIV4xtzySxxc50pU8EEKI3gDWAogk+bW55TEVQohuAJIAxAD4h/Srk1QVaXQlVUYI8RqAOQAGktxrZnFMjhCiFbRTKrsAvE1SfwMHicQI0uhKKo0QohaABQBCAPQhmWFmkcyGEOIJAN8AuAsgnOQdM4skeUyQc7qSSiGEsAOwHoAXAM+abHABgGQOgD4AbgD4UQjR3MwiSR4TpNGV3BchRDMAPwAoBvASSbnXIQCSGgCjAWwGsE8I4WpmkSSPAdLoSipECOECrYdCMoBhJPPNLJJFQS1zAcwEkCyEME8eIsljgzS6EqMIIXwA7AawkORMkiXmlslSIfkVgDAAXwkh/m5ueSSWi1xIkxhECBEO4B8AhpPcaW55HheEEO2h9Wz4F4BZ0qVMUh5pdCU6CG0qh3ehDQToS/K4mUV67BBCPAWtL28GgDEkC8wsksSCkNMLEgUhhA2AL6B9TPaUBvfBIHkVgD8AOwA7hRBNzCySxIKQRlcCABBCNIL2sfgZAH4k/zSzSI81JPOg/fE6BGCvEKKNmUWSWAjS6EoghGgJIBXAGWg3dLltZpGsApIlJKMAfAbgJyGEh7llkpgfaXRrOEKIrtC6hK2BFe8SZk5I/j8AYwF8K4QYaG55JOZFLqTVYIQQL0NrbCeQ/LeZxbF67v3AJQFYDCBWejbUTKTRraEIISIBvAcglOR+c8tTU7g3lbMV2umcN+STRc1DTi/UEIQQAUKIlkKIWkKIGABvAPCRBte0kDwPwAeAM4D/CCHshRC2QohXzSyaxERIo1sDEELUARAPrWfCRgA9AHiRPGtWwWooJHMBvAzgT2gj/hwBLL+XhUNi5UijWzPoB+AStKvo+QCCSWaZV6SaDclCAOMBfA3tvhbbAbxmVqEkJkHO6dYAhBA/AWgH7abbVwBcJLnQvFJJ7nky9AdwB8BQaHdxa35v9zKJlSJHulbOve0GvQA0AGAP4By0m29LzM+P0LrrPQmAAJoAmGxWiSSPHDnStXKEELUBBAHYde+RVmKB3NvzwhvA/0heN7c8kkeHNLoSiURiQuqYW4Dqws7O7nJ+fv7T5pbjccfW1vZKXl7eM+aWw5qQfbP6sIb+aTUjXSGEDPCpBoQQICnMLYc1Iftm9WEN/VMupEkkEokJkUZXIpFITIg0uhKJRGJCpNGVSCQSE1JjjG5GRga6desGjUYb7BMTE4NZs2YBAOzs7NC3b1+lbkxMDLy9veHr64vjx7UZa1JTU/Hiiy9i7Nixj0zGffv2wcvLC35+foiNjTVYJygoCM2aNcPcuXOVY9nZ2QgODoZKpYKXlxeOHDkCAJgzZw7c3d3h7e2NN954A3Ixx3J4HPrjkiVL4OLiAmdnZ53jXl5eUKlU6N69OxISEvTO+/zzz6FWq6FWq9G+fXsMHKjdQvjw4cPw8PCASqVC7969cfPmTQBA37594e3tDXd3d8THxz+y+7EYSFpF0d5KxcybN49z5szh2bNn2bVrV+bn55MknZyclDqnTp2iWq1mSUkJ09PTqVarlc+Sk5M5ZsyY+7ZDkrdu3apUvbK4ubnxjz/+YElJCYODg5mRkaFX5/z581y9ejU/+ugj5diSJUs4Z84ckuSePXsYGhqq3EspYWFh/P777+8rwz09mv37tKZirG9aen+8fPkyNRqNjjwkWVBQQJK8efMmW7duXeE1xo0bxw0bNpAkBw4cyJSUFJLkRx99xM8++4zkX/00Ly+PTk5OzMvLM3o9a+ifNWakCwBTp07Fli1bEB4ejk8//RT16tXTq5OcnIz+/ftDCIH27dvj2rVrKCqq3JanGo0GiYmJCAsLw7Bhw6osX05ODp577jkIIdClSxf8+OOPenVatNDfiKpDhw7Izc0FAGRlZeGpp54CALi4uCh16tWrh9q1a1dZJsmjw9L749NPPw0bGxu943Xr1gUA3Lp1C66urkbPz8/Px86dO9G/f38AgKurK3JycgBon87K99O6deuiVq1a0AbnWS9WExxRGWxsbODn54ctW7bAx8fHYJ0bN27A0dFRed+4cWNkZ2ejWbNmRq974MABrFq1ChkZGejVqxdiY2PRsmVLAMC5c+cQERGhd86gQYMwebJumP2TTz6Jo0ePokOHDkhOTsaTTz5Zqfvq2rUr3n//fXTq1Ak5OTnYvXu3zucpKSm4cOEC/Pz8KnU9iWmw9P5ojLy8PPTs2RNpaWlYuND4vknffvstgoKCYGtrCwAICQlB//79MXPmTDRs2FDv3AULFmDQoEEGf3ysiRpldNPS0vDTTz8hKCgIX3zxBcaPH69Xp2nTpsjOzlbe5+bmwsHBocLrJiUlYf/+/Zg8eTIGDhyIJk3+yrj93HPPISUlpVLyff7554iKioIQAm3bttX5Z6uIRYsWITQ0FFFRUdi/fz8mTZqEbdu2AQB++eUXREdHY8uWLahVq0Y92Fg8lt4fjWFnZ4fdu3fj+vXr6N69OwYPHozGjRvr1Vu7di2ioqKU9xMnTsSmTZvg5uaGhQsXIjY2FtOnTwcAxMXFIS0tDevWrXso2R4Hasx/YUlJCSZMmIBly5Zh4cKFWLJkCa5cuaJXT61WY+vWrSCJM2fOoGnTpqhTp+Lfpnnz5uGnn35CvXr1EBERgVdeeQUbNmwAoB1ZlC4qlC1Lly7Vu07nzp2xY8cOJCUlIScnBz179qz0/ZWOfJo1a6Y8wqWnp2P8+PH4+uuv0bRp00pfS/LoeRz6oyE0Gg1KSkoAAA0aNICtra0yki3LtWvXkJ6ervd0ZaifbtiwAYmJiYiPj68ZAwNzTypXV8F9FtKWLl3KN998U3mfmJjIIUOGkKTeQsHChQvp5eVFb29v/vrrr8rxyi5cXLp0iStWrLhvvfJ88sknVKvV9Pf357Zt25TjQ4cOVV7//e9/Z8eOHenk5MS+ffuSJC9evMiAgACqVCr26NGDycnJJEmVSsW2bdtSpVJRpVLxP//5z31lgBUsVFhaMdQ3H4f+mJCQwMDAQNrZ2TEwMJC7d+/mmTNn6OvrS7VaTU9PTyYkJChtTJkyRTn3s88+Y3R0tM71UlJS6O7uTpVKRX9/f168eJEFBQW0sbFh9+7dlX76xx9/GJXJGvqn3HsBQNu2bdGuXTts2bLFaJ3U1FRERUVBrVZXOI/1uGMNse2WRlX7puyPxrGG/imNrkQHa+jUlobsm9WHNfTPGjCBYn5Onz6Nrl27wt7eHqmpqcrxyMhIZU7tmWeewZIlSwBoF0Lc3d3h6+uL9evX612vNIhCpVIhICAAZ89q80tu374d3t7eUKvVCAgIwPnz5wFogyQ6dOigtFXqkC+pmaSkpKB58+ZKfzh48CAAbT9Vq9Xw9/fH1KlTAWjncMvO/dra2uL48eO4cuUKvLy8oFar4e7ujh9++MFoeytXrtRxPcvJycGAAQPg6+uLUaNGKf1x1KhR6NKlC9RqNUJDQx+hBsyMuec3qqugEsER5uLOnTvMysriyJEjuWfPHoN1OnTowD///JPFxcV0cXFhbm4uNRoNu3fvztzcXJ26Fy9e5O3bt0mSW7du5fDhw0n+5bROkitXrmRUVBRJcvbs2fzyyy8rJSusYM7M0oql9U1jc8EDBgzgvn37SJJjxozhrl27dD4/f/48XV1dSZJFRUUsKioiSf722290c3Mz2NadO3fYp08ftmnTRjkWHR3NlStXKq9XrVpFkhX+f5RiDf3T6ke6mZmZcHNzQ0REBF544QV8/PHHePPNN+Hl5YXw8HCljre3N/z9/aFWq5GdnY2bN29i8ODBCAgIgL+/P06fPv3AMtSvX79CN5/9+/ejZcuWaN68Oa5fv45mzZqhYcOGsLGxQZs2bXDo0CGd+o6OjmjQoAEA3aCHUqd1QDuaeP7555X3ixYtgo+PDxYvXvzA9yF5eCyhPwLAzp074ePjg8jISNy9exeAdqTr5uYGAHBzc0NycrLOOevWrVOCLGrXrq30u/J9rSwff/wxXn/9dZ2Ah+TkZISEhAAAXnnlFZ12pkyZAl9fX/zrX/96qPuzaMxt9aurwMho4vfff2fz5s15584d5uXl0d7enkeOHCFJBgYGMj09nStXruTs2bOVc0pKSjh9+nRlZfbEiRMcMGCA3rUnTpyorLiWlpdfftmgHKTxX/LIyEhlJFo60r1w4QJzcnL43HPP8euvvzZ4vdu3b9PT01NnRXvTpk3s1q0bnZ2deebMGZLk9evXWVJSwry8PAYHB+uNYMoCKxhJWFop2zctoT/m5uYqobbvv/8+Z82aRVIbKv7tt9+ypKSEISEhnDRpks55nTp10vEsOHv2LL29vfnkk0/y22+/1Wvn0qVL7N+/P0ldjwwXFxeWlJSQJE+fPs0+ffqQJK9du0aSzM7OZteuXXn69Gm9a1pD/zS7ANV2IxUY3YCAAOV92ceckSNHMjU1lbdu3WJ0dDSHDh3K6OhoFhQUsE+fPvTw8FA6b9mY9wfFkNEtKChgq1atlOkCUvv4p1ar2a9fP/bv358//fST3rXy8/PZs2dPJiUlGWwrISGBYWFhesdXrFjBRYsWGZXRGjq1pZXyRtdS+iOp3feg1OidO3eO/fv3Z1BQEMePH8958+Yp9Q4fPmy0zd9++42tWrXSO/7aa6/x4MGDJHWNroeHB7OyskiSBw8e5IgRI/TOjY6O5saNG/WOW0P/rBERaWUfbcrHdZNErVq1MH/+fADA6NGjsWPHDri6usLT01N5DDK0+BQZGYmTJ0/qHLO3t6/Q1ac83333Hfz8/JTpAgDKosWtW7cwcOBAdO/eXeecoqIiDBkyBOHh4ejXr59yPD8/X3FUd3BwQP369QFoH/+eeOIJkERycrLyGCsxD+bujzdv3lQiyHbt2oV27doBAFq2bIn//Oc/IImIiAilLQD48ssvMWLECOV9QUGBEq7bqFEj2Nvb68mTkZGB999/HwBw6dIlDBo0CN988w3UajWSkpIwcuRIJCUlQa1WA/irnxYWFiI1NRWvvvqqMRU+3pjb6ldXQQUj3cDAQOV92V/c0pHnhg0b6OPjQ5VKxeDgYGZlZTEnJ4dDhgyhv78//f39uXDhQoPXrwxZWVkMDAxk8+bN6ebmxpkzZyqfhYaGcseOHTr1p06dSrVazaCgIB4+fFg5XhoksXr1ajZs2FAZ9UycOJGk1iG9dBTUs2dPZmZmKvfp4eFBd3d3ZXHNGLCCkYSlFZQb6Zq7Py5btoxubm709fXlgAEDeOPGDZLkV199RbVaTbVazTVr1ij1CwsL2apVK968eVM5tmfPHiVIwtvbW9nB7siRIwafpMreZ1ZWFvv160dfXzTF/J0AACAASURBVF+OGDFCWQAODg6ml5cXu3fvzk8++cSg7NbQP6WfrkQHa/CDtDRk36w+rKF/Wr33gkQikVgS0uhKJBKJCZFGVyKRSEyINLoPQfncUY+K1NRUdO7cGba2trhw4YJyfPLkyVCpVOjRowemTZsGQLsCHBgYCB8fH3h4eCj76pZy48YNODg41Ih9SyWGMVW/BYDFixcjKCgI/v7+yvaSxcXFiI6ORlBQENRqtcEMKdZMjXAZe9zp3Lkz9u3bp5OsEABiY2OVKDSVSoW0tDS0adMG8fHxaNGiBa5fvw5vb2/07t1bOWfu3LlGsxRIJNXJjh07cPnyZXz//fc6x+Pi4tCyZUssWLDATJKZF6sc6RoKo9y9ezf8/f3h6+uL/v37Iy8vD4DWJ/btt99Gz549oVar8a9//Qs9e/bEiy++qIRaqtVqTJo0CS+99BICAwNx9epVnfYKCwsxbtw4BAQEwMfHB/v27QOgNXA9evRAQECA0ey+laFx48YG/SBLDa5Go0H9+vXh6OgIOzs7JY+anZ2djh9oRkYGbty4gW7duj2wLJJHh7X12w0bNqC4uBhBQUEICwvD5cuXleOXLl1CQEAARo8ejVu3bj1wG48l5vZZq66CMr6QhsIoy0Z8TZs2TfFDVKlUTExMJKnd5OOtt94iSX755ZecPn26Uic+Pl65dqmva6nv4fLly7lgwQKS5NWrV+nh4UGSbN++vdJucXExyxMSEqIXtlnRptQqlYrnz5/XOTZ+/Hg+++yzHD16tF4bY8eO1dm8Ojw8nBkZGRVugAMr8IO0tIJKbnhjbf02ODiYb7zxBkny66+/ViLPXFxcGBsbS5KMiYnh+++/Xyn9kNbRP61yemHw4MGYP38+hg0bhlatWmHOnDlIS0vDe++9h4KCAly5cgWNGjVS6peO/Fq0aAEnJyflddm5Ji8vL+VvYmKiTnvHjx/H3r17sX37dgBQ0pAsXboUkyZNQlFRESZMmKD3WL9p06aHvtcVK1agsLAQoaGh2L59O/r06QMAeP/999GkSRMl79bevXvRtGlT5f4kloe19dsmTZooU1t9+/bFhx9+aPB46XpETcEqja6hMMq4uDh88MEH8PT0xLRp00pHIACMh2WWrbN//344Oztj//79SthkKa6urnB2dsbbb78N4K8QTU9PTwQGBuLcuXMICQnBzz//rHNeaGgosrKydI45OzsjLi6uUvdZGvZrY2MDe3t7Jew3JiYGf/75J1auXKnUPXz4MI4dO4ZevXohIyMDDRo0gJOTEzw9PSvVluTRY239NjAwEIcPH0avXr1w4MABJdV66fH27dvrHK8pWKXR3bJlC5YsWYLatWujXr168PHxwe3btzFmzBi0b98ejRo10hkxVIZffvkF8fHxKC4uRkJCgs5n48aNw+TJk+Hv7w8A6NKlC2JjYxESEoL8/Hzk5+dj0qRJetes7IghPT0dr7/+Oo4ePYrw8HC8+uqrSqbXO3fuQKPRwM/PD2q1Gr///jumT5+ubGYOaLfxe+ONN/DGG28A0G5q7uzsLA2uhWFt/TYiIgITJkyAv78/SCpGOSoqCmPGjMHKlStha2uLtWvXVumeHndkGHAlUKvVWLdunbJAZc1YQ5ilpWGuMGBr7LfW0D+t0ntBIpFILBU50pXoYA0jCUtD9s3qwxr6pxzpliMzMxNBQUFmlWHXrl0QQuhEn5ViLJpn/Pjx8PDwgIeHh8GU3H5+fhg7duwjl13yaDB1vzSUpBLQeht4e3vD3d0d8fHxeucdPHhQ2Q/aw8MDTZs2BWA8Gea8efPg5+cHb29vREREoLCw0DQ3aE7M7bNWXQXVlPyv/H6npqa4uJi9e/emm5ubnk8uSf7zn//ksmXL9I6fOnVKOd/Dw4MZGRnKZ5s2bWK/fv0q9AEuBVbgB2lppTr6pqn7pbEklaX9LC8vj05OTkraH0N89dVXyl7PxpJhlk2mOmLECG7ZsqVCuayhf9aIkW5UVBT+/e9/A9BmXXj++edRWFiIGTNmICAgAF27dsXy5cv1zhs1apSSMj0lJUUZKZ44cQJBQUEICAhAWFiYktivOli3bh369++vk0miLMaieUrdbmrVqoU6deooSQOLioqwfPlyg6vQEvNiyf3SWJLK0n5Wt25d1KpVSy/zRVnWrl2rk23CUDLM0qjKkpISFBUVmXRfCHNRI4zuqFGjlEehHTt2ICAgADY2Npg5cyZ27dqFffv2YfHixZV+tImMjMSqVauwa9cuqNVqfP755zqfazQa5TGqbHnvvfcqvG5eXh7Wrl1b4TTAxYsX0aRJE+zatQsdO3ZETEyMzufr1q1Dy5Yt0bp1awDAP//5TwwfPlxJrSKxHCy5X3bq1Anbt28HSezcuVPPL3fBggUYNGiQ0X51+fJlZGZmKm6J3bp1w+nTp5Gamoonn3wS//d//6fUnT17NlxcXJCTk4OWLVtW6l4fZ6zST7c8nTp1wrVr13D16lXEx8cjOjoaALB8+XJs3rwZtWvXxtWrV/Vi0405nKelpSEiIgKANldUqT9sKXXr1kVKSsp95crLy1Mic2bNmoUDBw5gwoQJqFPH+NdSUTTPtm3bsHbtWiQlJQEAcnNzsXnzZuzcuRO7d+++rzwS02Kp/RIAPvnkE0yePBn/+Mc/0KZNGzg6OiqfxcXFIS0trcKd6r766iudXHwNGzZUXg8fPlwJyACADz74AHPmzMGkSZOwZs0aREZGVkrGx5UaYXQBYNiwYVi2bBkyMzPRpUsXZGdnY/Xq1Th27BgKCwvRrl07nQ4MaA3cuXPnAACHDh1Sjnfq1AkJCQlo3rw5AP0kgRqNBsHBwXoy+Pj4YO7cucp7Ozs7nX+CVatW4ccff0RcXByOHTuGESNGYMuWLTpTDcaieXbv3o25c+fiu+++U5JTpqenIzc3F3369EFWVhYuXbqEFStW4LXXXnsQFUoeAZbYLwHjSSo3bNiAxMREbN68GbVqGX9QXrduHb755hvlvbFkmKVRlUIING7cWImqtGrMPalcXQX3Way4ceMG69evz48//pikdjORQYMG0cPDg6NHj2aXLl14/vx5nQWLkydP8sUXX+TLL7/MyZMnKwsBx48fZ3BwsJIkcNu2bRW2/SCU3dxm27ZtXLt2LUkyOzuboaGhVKvV7NWrF69evUqSbNWqFTt37qxsQHLgwAGd6xlbyCgPrGChwtJKRX3TUvuloSSVBQUFtLGxYffu3ZV+9scff5D8K2kqSR47doxeXl461zOWDPPvf/87VSoVfXx8OHr0aGo0mgrlsob+Kf10JTpYgx+kpSH7ZvVhDf2zRiykSSQSiaUgja5EIpGYEGl0JRKJxIRIoyuRSCQmxGpcxmxtba8IIZ42txyPO7a2tlfMLYO1Iftm9WEN/dNqvBdMgRBiGIC3ALiTLDFRm00BpAMIIHnCFG1KHj+EEI0A/A/AAJKH7le/Gtv9AkAuyXdM1ebjjjS6lUQIYQ+t8RtC8icTt/06gAEAXpK+RxJDCCEWAniG5CgTt/sUgJMAvEmeMmXbjyvS6FYSIcRHANqQHGaGtm0A/ApgBsn/mLp9iWUjhHAGsB9AZ5KXzND+O9A+ib1s6rYfR6TRrQRCiNYAfgbwAkn9TW5NI8NLAP4JoCPJAnPIILFMhBCbAewj+X/3rfxo2q8L4ASAt0h+Zw4ZHiek90LliAHwqbkMLgCQ/C/udWxzySCxPIQQQQA6AfjUXDKQ1AB4G0DsvacySQXIke59EEKoAKwF0J5knpllaQtgH4BOJC+bUxaJ+RFC1IF22uk9kpvNLIsAsA3AdpJm+wF4HJBGtwKEELWhnVaYT3KjueUBACHEIgBPkhxtblkk5kUIMQlACCxkgVUI0QHAbminwK6ZWx5LRRrdChBCjAcwHIDKEjo1oLgGnQLQ35SuQRLLwlJdCYUQnwKwJTnB3LJYKtLoGkEI8QS0fo+9SR4xtzxlEUKMBjAWWjcd+QXWQIQQS6D9/51sblnKIoRwgPb/pifJX80tjyUija4RhBCxAOxJjje3LOURQtQCcBBALMl/mVseiWkRQnQCsAtAB5I3zC1PeYQQEwAMAeAvBwX6SKNrACFEewCp0M5NXb1ffXMghPABkADtAt8dc8sjMQ33Fqx2AkgiucTc8hji3gLfLwA+JPnN/erXNKTLmGE+AbDAUg0uAJBMhfaHYdr96kqsin4AHKH12bZISBZB69oYI4SwM7c8loYc6ZZDCNEHWp/HTvf8Dy0WIcRzAI4A6EryD3PLI3m0CCHqAUgDMPGe37ZFI4T4N4BfSM4ztyyWhDS6ZbgXWXMMwDskt5pbnsoghJgN7TTIq+aWRfJoEUJMg3bxdIC5ZakMQog2AA4BeJ7kRXPLYylIo1sGIcTbAIIB9HlcFgCEEPWhdR0aQVLmWbdShBDPQBuR6EEyw9zyVBYhxDwAz5EcYW5ZLAVpdO8hhGgG7W5JfiTTzS1PVRBCvArgXQBuJIvNLY+k+hFCrAJwjeR0c8tSFe7tzncKwECS+80tjyUgje49hBD/BJBP8rHb2+DeivZuAPEk48wtj6R6EUK4AUiC1lMl19zyVBUhRASASQA8TbUPtSUjjS4AIcSLAHZA26mzzS3PgyCE6AZgK4B2JG+aWx5J9XDvBzUVwEqSq8wtz4Nwz698H4BlJNeaWx5zU+Ndxu516k8BzH5cDS4AkPwZwBYA75tbFkm1MgRAPQBrzCzHA3NvdPsmgAVCiIbmlsfc1PiRrhBiELSGquvjPh96Lw9XGuQu/laBEKIBtCG1Js9W8igQQqwFcIHkDHPLYk5qtNG957idDuDvJJPNLU91IISIAqAm2dfcskgeDiHEBwDakhxqblmqAyHEs9C6ZHYnedbc8piLGml07+UcOwQgCEAXkgPNLFK1cc/XOA3A6wCeAnCZ5E7zSiWpLPeylIwF8AW0obQvkjxvTpmqEyHETADdAIwDMIvkm2YWyeTU1DndQGh3238bwFQzy1Kt3IuimwJgMYCOALqbVyJJFekAwA3A/wH4zJoM7j1iAXSB1h8+xMyymIWaanQbAxgM4CsAn93bKtEquLefaW8AF6H9B25sXokkVaQxABsAngA0QgiLD/etLPc8bHYDWA5gJmpo36ypRtcRgBeAcGizqH5pXnGqlTkAbAG0B9ATwDNmlUZSVZ6A9umkANrRoDVtBv4LtJ5C7wBoCKDhvewsNYqaanRbALgK7X6fc0kWmlug6oJkzr1UPmMBFEI7fyZ5fPAAUB/ax/BAkr+ZWZ5qg1q+AvACgDMABIAG5pXK9NTUhbRAAD/e24LOarkX2txapvV5fBBCtALQgORJc8vyqBFC9ASw83HZ56S6qJFGVyKRSMxFTZ1ekEgkErNQ534V7OzsLufn5z9tCmGsGVtb2ysAIHVZfdja2l7Jy8t7BpD99EGQ+nt0lNVtee47vSCEqGlTLo8E7RYPgNRl9SGEAElx77Xsp1VE6u/RUVa35ZHTCxKJRGJCpNGVSCQSE2L1Rnf9+vUIDAyEv78//vGPfwAAVqxYgR49esDPzw/h4eEoKCjQO2/fvn3w8vKCn58fYmNjTS22ydm1axeEELhw4QIA4OzZs/Dz84NarYZarcYff2jzXk6ePBkqlQo9evTAtGl/JSJesGABunfvjh49eiAmJkbv+iQxZcoU+Pr6IigoSGln1apV8PHxgZ+fH/r164fcXO0e3Xl5eZgwYQKCgoKgVqtx6tTjtWlaSkoKmjdvrujv4MGDAIDDhw/Dw8MDKpUKvXv3xs2b2q2P+/btC29vb7i7uyM+Pl65zv30Wkr572/NmjX429/+prR/7ty5R3i3j5bK3tu2bdvQvXt3+Pr6Ijw8HIWFf7nfazQaODs7Y+7cuXrXz8zMhIODg3K9pKQk5bPFixcjKCgI/v7+2LBhAwBgzpw56NChg1Jfo6li/lqSFRYoPs2PHydPnuTQoUNZXFysc/zMmTPKsalTpzIuLk7vXDc3N/7xxx8sKSlhcHAwMzIyHkoWALRUXRYXF7N37950c3Pj+fPnSZLvvPMO16xZQ5L88ssvOWXKFJJkQUGBcp6fnx9PnDjB3NxcOjs7s6ioiEVFRWzXrh1zcnJ02tixYwdHjBihvI6IiNC73vvvv8+lS5eSJN99911u3bq1Qrnv6dMi+2lycjLHjBmjd3zgwIFMSUkhSX700Uf87LPPSJKnTp0iSebl5dHJyYl5eXmV0itp+PtbvXo1P/roowpltGT9lVKVe+vWrRszMzNJkmPGjGFSUpLyWWxsLPv162fwvN9//52BgYF6x7dv385p06bpHZ89eza//PLLCuUuq9vy5ZGNdDMzM+Hm5oaIiAi88MIL+Pjjj/Hmm2/Cy8sL4eHhSh1vb2/4+/tDrVYjOzsbN2/exODBgxEQEAB/f3+cPn36gWX4+uuv0bhxY/Tq1Qsvv/yyMlpydnZGrVraW69Xrx5q19aPRMzJycFzzz0HIQS6dOmCH3/88YHlMIYl6AgA1q1bh/79+6NBg7+Cg1xdXZGTkwMAyMrKwlNPPQUAqFu3LgDtyKF+/fpwdHSEnZ0dHB0dkZeXh7y8PNSrVw/16tXTaSM5ORkhIdr9TV566SVl5Fd6PQC4ffs2XF1dAQA7d+5EcnIy1Go1pkyZgqKiysexWIped+7cCR8fH0RGRuLu3bsAdPWanZ2t6NXFxUXRR61atSCEqJReAcPfHwCsXbsWPj4+mDlzJkpKqpYlx1J0WJV7K9UtSeTk5KBZs2YAtP/L33//PUJDQ422c/ToUfj6+mL48OG4du0aAGDDhg0oLi5GUFAQwsLCcPnyZaX+okWL4OPjg8WLF1f9poxZYz7kL+Dvv//O5s2b886dO8zLy6O9vT2PHDlCkgwMDGR6ejpXrlzJ2bNnK+eUlJRw+vTpTEhIIEmeOHGCAwYM0Lv2xIkTqVKpdMrLL7+sV2/8+PF85ZVXWFxczIMHD1KlUul8fvLkSXbr1o23b9/WO9fDw4O//vorCwoK2KNHD8bExDyQHkqBgZGuJejo7t27DAwMZGFhIVUqlTKa+OOPP9i+fXt27tyZbdu2ZXZ2tnLO+PHj+eyzz3L06NHKE8P8+fPp6OjI5s2bK6O3sowbN47JycnK+7Zt2yqv/9//+390dXVl9+7defXqVZJk3bp1uWnTJpLk5MmTuXLlSoM6pYF+agl6zc3NZV5eHkntCH7WrFkkySNHjrBly5Z0dXWlh4cHNRqNznlz585ldHS08v5+ejX2/WVlZSkj5FGjRnHVqlWV1p+l6LCq97Zz504+88wzdHFx4cCBA5XrREVF8ccffzQ6Qs7Pz2dubi5JcuXKlcpTWHBwMN944w2S5Ndff608qV2/fp0lJSXMy8tjcHAwd+3aVaFuy5dHanQDAgKU923atFFejxw5kqmpqbx16xajo6M5dOhQRkdHs6CggH369KGHh4fyZajV6gdqn9Q+oi5fvlx57+zsrCNf9+7d+dtvvxk899ixYwwODmbPnj05bNgwfvXVVw8sB2nc6JpbR/Pnz+fXX39Nkjode8iQIcrxhIQETpgwQec8jUbDvn37cuvWrfzf//5HNzc35uXl8e7du3Rzc+OFCxd06r/77ruKES0pKWGHDh30ZFmwYAGnTp1KknzmmWcUo7Vt2za+/vrrevUrMrrm1mtZTp06xT59+pDU/pgfOnRIud+FCxcq9b744guGh4crP2SV0aux768sO3bsYGRkpN7x+xldc+uwqvfWpk0bZXrhtdde4/r16/n7778rBrgyUy75+fl0dXUlqf0f2LZtG0nttE/nzp316q9YsYKLFi3SO16R0b1vcMTDUOqbWv71vW8YtWrVwvz58wEAo0ePxo4dO+Dq6gpPT0/lUdTQJHVkZCROntQNTbe3t8eWLVt0jgUGBmL9+vUAtAtDTZo0AQBcvnwZYWFhWLlyJdq0aWNQ9s6dO2PHjh3QaDQIDQ1Fz549q3LrlcbcOkpLS8OPP/6IuLg4HDt2DCNGjFDqlD6eNWvWTHkkzs/Ph62tLWxsbGBvb4/69esDABo2bAhbW1sAgK2tLW7fvq3Tjlqtxvr16xESEoJdu3bBzc1N53oA4ODggPz8fADa7+7w4cPw8fHBgQMHlMfvymJuvd68eRONG2t3Lty1axfatWunfFZWrxkZGQC0j7KJiYnYvHmzMvUF3F+vxr6/wsJCPPHEEwbbryzm1mFV761OnTpwcHAA8Fef/eWXX/Dnn3+iV69euHjxIgoKCtCpUye88sorSjtlv6vk5GSlr5X2wV69eun0wZycHDzxxBMgieTkZGW6pdIYs8ashpFu2clpJycn5fXIkSO5Z88ebtiwgT4+PlSpVAwODmZWVhZzcnI4ZMgQ+vv709/fX2ckUFVKSko4depUqlQqenp68sCBA0r7LVu2VH6NV6xYQVL7S7hz506S5CeffEK1Wk1/f3/l1+5hgJGRrrl1VJayo4kTJ07Q29ubKpWKXl5ePH78OEmyT58+ij6nT5+unPvuu+/S3d2dPXr0UI5funRJWYArKSnhm2++SR8fHwYGBvLcuXMkyejoaOV7CAkJUaYxzp8/z549e1KlUjEsLEwZ9ZYFFYx0za3XZcuW0c3Njb6+vhwwYABv3LhBkkxJSaG7uztVKhX9/f158eJFFhQU0MbGht27d1d08ccff1RKr2Up+/3NmDGDPXr0oJeXF0eOHKmzYHk//VmKDqt6bxs3blR03rdvX966dUvnGuVHukOHDiVJJiYmskuXLvTz8+NLL73E33//naR2kffvf/871Wo1VSoVz5w5o9y/h4cH3d3dGRUVZVBeVDDSlRFpJkJGpFU/MqLq4ZD6e3TIiDSJRCKxEKTRlUgkEhMija5EIpGYEIs1us7OziZpJzU1FZ07d4atra0SZggYD3f94Ycf4OnpCU9PTyQkJCjHg4KC0KxZM4NhhpaKqXRsLOx63rx58PPzg7e3NyIiInTCNh9HTKVPY6HT1ha6bu7+GRkZqYT6PvPMM1iyZEn1NGhshY0P6b3wsJRdLX2U5OTk8NatW3p+gIbCXYuKivj888/z+vXrvHv3Ll944QXFqfr8+fMV+gHCAsOATaVjY2HXZXU8YsQIbtmypUrXhYWFsZpKn8ZCp6saum5p+iuPuftnWTp06MA///yz0tdEdfrpZmZmYtiwYahbty5IIjExEcePH8fs2bNRVFQEBwcHbNiwAXZ2dlCr1ejSpQtOnjyJgoICjB8/HvHx8bhy5Qo2btwIFxcXqNVquLq64vTp0ygpKUFCQoISGgkAhYWFiIyMxG+//QaNRoOYmBh4enpi7ty5SEpKgr29Pfr27YspU6Y80I9OqX9eeQyFu2ZkZKB169Zo2rQpAMDLywuHDh1CQEAAWrRo8UDtG8LadFx2xFI27LpUxyUlJSgqKnpkIxtr02f50OmAgAAAf4WuA1BC152cnB5Cc4axNn0a65+l7N+/Hy1btkTz5s0fTGHlMWaNaeQX0FDoX9kw2mnTpikbpahUKiYmJpLUbkDx1ltvkdRuoFLqc6hSqRgfH69cu9TvrfRXbvny5VywYAFJ8urVq/Tw8CBJtm/fXmm3/IY2JBkSEqIXamhoA5JSDEW8lA93/emnnzhy5Ejl8xkzZnDjxo3K++oa6Vqrjg2FXc+aNYtOTk7s3bs379y5Uyn9lIJKjtSsUZ+GQqerGrpeWf2Vxxr1SRrfFiAyMvK+G9yUB9U50h08eDDmz5+PYcOGoVWrVpgzZw7S0tLw3nvvoaCgAFeuXEGjRo2U+t26aTOAt2jRQvnVbdGihc4GMl5eXsrfxMREnfaOHz+OvXv3Yvv27QCgREYtXboUkyZNQlFRESZMmAAfHx+d8zZt2lTVW9NjxYoVKCwsRGhoKLZv3w4nJydkZ2crn+fk5Cij3urEGnWcmZmJkSNHYuPGjTqbl3zwwQeYM2cOJk2ahDVr1iAyMrLS16ws1qjPiRMnYuLEiVi4cCFiYmKwaNEifP7554iKioIQAm3btoWjo2Olr1cVrFGfxvqnRqPB1q1bsWjRokpf635U2egaCv2Li4vDBx98AE9PT0ybNk0nAMBYKGHZOvv374ezszP279+vF67o6uoKZ2dnvP322wD+Civ09PREYGAgzp07h5CQEPz8888654WGhiIrK0vnmLOzM+Li4ip1n4bCXZ2dnZGZmYns7GzUr18fe/fuxYIFCyp1vapgbTo2FnZdqmMhBBo3bqyEFFc31qZPY6HTpgpdtzZ9VrQtwHfffQc/Pz+9Xc4ehiob3S1btmDJkiWoXbs26tWrBx8fH9y+fRtjxoxB+/bt0ahRI51fucrwyy+/ID4+HsXFxToeAQAwbtw4TJ48Gf7+/gC0c1WxsbEICQlBfn4+8vPzMWnSJL1rVvZXLj09Ha+//jqOHj2K8PBwvPrqq5g8eTIGDhyIO3fuQKPRKJt5A8DHH3+MPn36AACioqKUex09ejQOHDiAgoICHDhwAN9++22VdFAWa9Pxu+++iytXruCNN94AAAwdOhTjx49HZGQkzp49i+LiYri4uODDDz+s0j1VFmvT54cffoi9e/cCAJo0aYJVq1YBAGJjY/Htt99CCIFp06Y9kqcwwPr0aax/AsCXX36J1157rUr3cj/MHgasVquxbt26al2IskTMGQZsrTo2VxirtejTUsKArUWfZZFhwBKJRGIhmH2kW1OQG95UP5YyUntckfp7dFjESDczMxNBQUGmak4HY0kCy2IsAm3FihVwd3eHr68vUlJSAPwV+aNSqRAQEICzZ8+a6lYAmF6XxiLH1qxZAzc3N3h6euKtt97SO0+j0WDw4MHw9fVFjx498N///heA8YiqJUuWwMXFxWSRSIDpdWksymnGjBlo1arVfWU5deoUbGxskJqaCgD4/PPPleu1b98eAwcOBGA8AeajwNQ6/Oab4tJnQwAAB0tJREFUb9ChQwdlMbEUY8k9S/9Xu3fvrjdfXJbKJmctJSIi4sHu25gvWWlBNUWqGEv+ZgqMJQksi6EItCtXrrBLly7UaDTMyclht27dWFxczIsXLyq+fFu3buXw4cPvKwOqMSLN1Lo0FjnWqlUrZc/SwMBAHjt2TOe8b7/9lqNGjSJJnjt3jl27dtW7XtmIqsuXL1Oj0VQ6EgnVEFFlzn5ZNsrp4sWL/O233+4ry5AhQxgUFMQ9e/bofTZu3Dhu2LCBpPEEmGWpDv2RptfhtWvXlASeZTGU3JP8q7/dvHmTrVu3NnjNqiRnJcmff/6ZAwYMMHrfqMBP96FGulFRUfj3v/8NACgqKsLzzz+PwsJCzJgxAwEBAejatSuWL1+ud96oUaOUX+qUlBSMHTsWAHDixAkEBQUhICAAYWFhSjK/6sBQksCyGIpAy8zMRMeOHWFjY4PGjRujTp06yMzMhKOjo+JCYiyxZVWxZF0aixxr3749bt26hcLCQhQUFCi79pfi5OSEgoICkNRJwmgsGeXTTz8NGxubB5azFEvWZSnlo5wcHR11MkYYYvfu3WjdujWeffZZvc/y8/Oxc+dO9O/fH4DxBJiVxZJ1+OSTT+qNcgHDyT1L3wPArVu3lL5WnqokZwW0HiQzZ858sBswZo1ZiV/A48ePs1+/fiTJLVu28M033yRJZRSYn5/Ptm3bUqPR6Pwalu48T+qOQn19fZUd85cuXcrFixfrtFdQUKAXYaJSqThz5kyjMpLGkwSWp3wE2vXr19mxY0fevHmT58+fp729vZLfqvQ+PT09+euvv1bYPnn/ka6l69JQ5Fh8fDyffvpptm7d2mAmg/z8fA4YMIAuLi586qmnuG/fPuUzQxFVpTzsSNfSdUkajnKqaMRYUlLCnj17Mjs7W0fOUjZu3KjzNHe/BJgV6Y98PHRorJ+UT+559+5d+vr6skmTJvz888/16lc1Oeu3337LDz74oMLvC9UZkVaWTp064dq1a7h69Sri4+MRHR0NAFi+fDk2b96M2rVr4+rVq7h69arOecYcpNPS0hAREQEAKCgoUHxjS6lbt64yr1oReXl56N27NwBg1qxZSmw6AAwfPlxxsi5P+Qi0Pn36YM6cOejbty+aN2+OF198UYnyKSgowMCBAxEdHY0XXnjhvjLdD0vVZSnlI8dGjBiBDz74AOnp6WjUqBEGDBiAAwcOwN3dXTlnzZo1aNGiBTZv3ozMzEyEhITgyJEjAAxHVFUXlq7LB4lyWr9+PYKCgpTcYOVZu3YtoqKilPcTJ07Epk2b4ObmhoULFyI2NhbTp0+vdHuWrkNjxMXFIS0tDevWrVOO2dnZYffu3bh+/Tq6d++OwYMH6+y58umnn2LChAmoU0fXHE6fPh0fffQRBg0ahPXr1yM6OhpLly5V/KFLU7VXlYdOTDls2DAsW7YMmZmZ6NKlC7Kzs7F69WocO3YMhYWFaNeund6KfZMmTXDu3DkAwKFDh5TjnTp1QkJCgvLIVT6pnUajQXBwsJ4MPj4+Olsq2tnZ6XyBFSUJLMVYwsWwsDCEhYXh0qVLGDt2LBwdHVFUVIQhQ4YgPDwc/fr1q4q6KsQSdQkYjhyrVasW6tati4YNG6J27dpwcHBQHsXKUpqE0cHBQUmqaCyiqjqxVF0CDxbl9Ouvv+Lw4cP4/vvvcfz4cfzvf//Dv/71L7Rp0wbXrl1Deno6/Pz8dM4xlACzKliyDg1hKLmnRqNBnTp1UOv/t3fvqolFYRTHt6mOik2aeQ0DChHjBQ7YSBq1tdBOyDNYWFuIL5DygG9gJfgKYhpLG1OIjWATWFMMnlFnvGRiPmT4/yqx2nzIwn2BdXfn4vG48zzvj6OJz5SzLhYLt1qtXLVadZvNxk2nU9fpdFy73b5ojc65r1+kLZdLxWIxdbtdSb+2QbVaTY+Pj2o2m3p4eNB8Pt/7K/729qZkMqlyuayXl5dwCzKZTFQqlcJSu2sUQkrHSwJ3iyiPFS7W63UVi0WVy+Wwrv319VWJRCLcArVarbNrcBdcpN3qLBuNhgqFgp6entRsNsOtar/fVzqdDgsCPz4+JP0u/Fuv13p+flY+n1cqlQoveY6VUQZBIN/3FY1G5fu+xuPxyXW5E9vjW52lJFUqFQ2Hw73ver2estms7u/v5ft+eCm0neWuw+OFfr+/t52W/l6AeejU/KTbneFoNNr7nQwGg6PlnrPZTLlcTsViUZlMRkEQSLqs3PNYOevWvx4v8E7XCO90r493pl/D/L7PTbzTBQAQugBgitAFAEOELgAYOvtkzPO890gk8sNiMf8zz/PenXOOWV7Pdqbbz8z2c5jf99md7aGzrxcAANfD8QIAGCJ0AcAQoQsAhghdADBE6AKAIUIXAAwRugBgiNAFAEOELgAYInQBwBChCwCGCF0AMEToAoAhQhcADBG6AGCI0AUAQ4QuABgidAHAEKELAIYIXQAwROgCgCFCFwAMEboAYOgnVBE8invLQqYAAAAASUVORK5CYII=\n",
=======
      "image/png": "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\n",
>>>>>>> 4ef4e05 (Save lambda mart)
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(tree3)"
   ]
  },
  {
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the original tree, and provide new value\n",
    "\n",
    "Instead of directly using the provided decision tree, we want to use our prediction for each leaf. This `OverridenDecisionTree` takes the original tree, looks up the prediction path in the original tree, but uses our values for the predicted variable instead of what's here."
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 103,
=======
   "cell_type": "code",
   "execution_count": 473,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "cell_type": "code",
   "execution_count": 547,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
<<<<<<< HEAD
       "array([-1.50690684,  1.658231  ])"
      ]
     },
     "execution_count": 103,
=======
       "array([-0.85598879,  0.72500984])"
      ]
     },
     "execution_count": 473,
>>>>>>> 4ef4e05 (Save lambda mart)
=======
       "array([-1.2583991 ,  1.21562136])"
      ]
     },
     "execution_count": 547,
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OverridenRegressionTree:\n",
    "    def __init__(self, predictions, tree):\n",
    "        self.predictions = predictions\n",
    "        self.tree = tree\n",
    "        \n",
<<<<<<< HEAD
    "    def predict(self, X, use_original=False):\n",
    "        if use_original:\n",
    "            return self.predict(X)\n",
=======
    "    def predict(self, X):\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "        path = self.tree.decision_path(X).toarray().astype(str)\n",
    "        path = \"\".join(path[0])\n",
    "        \n",
    "        paths_as_array = self.tree.decision_path(X).toarray()\n",
    "        paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "        \n",
    "        predictions = self.predictions[paths]\n",
    "        \n",
    "        # Any NaN predictions is a red flag, debug\n",
    "        if np.any(predictions.isnull()):\n",
    "            print(predictions[predictions.isnull()])\n",
    "            print(pd.DataFrame(X)[predictions.isnull().reset_index(drop=True)])\n",
    "            raise AssertionError(\"No prediction should be NaN\")\n",
    "        return np.array(self.predictions[paths].tolist())\n",
    "        \n",
    "override_tree = OverridenRegressionTree(predictions = round_predictions, tree=tree3)\n",
    "override_tree.predict([[0.0, 6.869545], [10.0, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four - Putting it all together, from the top!\n",
    "\n",
    "Now we can put together the full lambdamart algorithm that \n",
    "\n",
    "1. Uses pair-wise swaps on our our metric (ie DCG) to generate decision tree predictors (the 'lambdas')\n",
    "2. Focuses in on predicting where current model makes the wrong call when ranking by DCG\n",
    "3. Predicts using a weighted average, weighed by 1 / (remaining DCG)\n",
    "\n",
    "TODOs / known issues\n",
<<<<<<< HEAD
    "* While DCG converges, it does sometimes wander a tad, so there might be more room for improvement\n",
=======
    "* Why is DCG performance so different from Ranklib, and it seems to wander around more (precision does this)\n",
>>>>>>> 4ef4e05 (Save lambda mart)
    "* Speeding up the inner loop of the `compute_swaps_scaled_with_weights` that must run for ever query's swaps"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 104,
=======
   "execution_count": 470,
>>>>>>> 4ef4e05 (Save lambda mart)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "Train DCGs\n",
<<<<<<< HEAD
      "mean    20.03521601393222\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    20.572514984109958\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    20.558839639948378\n",
      "median  18.543559338088343\n",
      "----------\n",
      "round 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-285d00895833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mjudgments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftr_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mlambdas_per_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_mart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudgments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjudgments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-285d00895833>\u001b[0m in \u001b[0;36mlambda_mart\u001b[0;34m(judgments, rounds, learning_rate, max_leaf_nodes, metric)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# ------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#1. Build pair-wise predictors for this round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n\u001b[0m\u001b[1;32m     32\u001b[0m                                                                    axis=1, metric=dcg)\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \"\"\"\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                 \u001b[0msdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                 \u001b[0mresult_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mfast_apply\u001b[0;34m(self, f, sdata, names)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;31m# must return keys::list, values::list, mutated::bool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlibreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_frame_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.apply_frame_axis0\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nan\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-10840cc48db9>\u001b[0m in \u001b[0;36mcompute_swaps_scaled_with_weights\u001b[0;34m(query_judgments, axis, metric, at)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0mquery_judgments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0mquery_judgments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0;31m# We have to operate column-wise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1815\u001b[0m             \u001b[0;31m# scalar value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0milocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_with_indexer_2d_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m             \u001b[0;31m# set the item, possibly having a dtype change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1919\u001b[0;31m             \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1920\u001b[0m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m             \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5931\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5932\u001b[0m         \"\"\"\n\u001b[0;32m-> 5933\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5934\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5935\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;31m# ---------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ws/hello-ltr/venv/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block_same_class\u001b[0;34m(self, values, placement)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# We assume maybe_coerce_values has already been called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
      "mean    10.61593108150268\n",
      "median  10.739621334025406\n",
      "----------\n",
      "round 1\n",
      "Train DCGs\n",
      "mean    14.34966497096901\n",
      "median  13.216212771507438\n",
      "----------\n",
      "round 2\n",
      "Train DCGs\n",
      "mean    12.357610149958038\n",
      "median  11.567827400752915\n",
      "----------\n",
      "round 3\n",
      "Train DCGs\n",
      "mean    14.24721713178369\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 4\n",
      "Train DCGs\n",
      "mean    13.441858214366144\n",
      "median  12.506165082920402\n",
      "----------\n",
      "round 5\n",
      "Train DCGs\n",
      "mean    14.091782646778626\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 6\n",
      "Train DCGs\n",
      "mean    13.426222703083178\n",
      "median  12.460843740423556\n",
      "----------\n",
      "round 7\n",
      "Train DCGs\n",
      "mean    14.008223263605812\n",
      "median  13.016486241731291\n",
      "----------\n",
      "round 8\n",
      "Train DCGs\n",
      "mean    13.434549080998451\n",
      "median  12.346789866126002\n",
      "----------\n",
      "round 9\n",
      "Train DCGs\n",
      "mean    13.862740868310802\n",
      "median  12.615453090513515\n",
      "----------\n"
>>>>>>> 4ef4e05 (Save lambda mart)
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def predict(ensemble, X, learning_rate=0.1):\n",
    "    prediction = 0\n",
    "    for tree in ensemble:\n",
    "        prediction += tree.predict(X) * learning_rate\n",
    "    return prediction.rename('prediction')\n",
    "\n",
    "\n",
    "def tree_paths(tree, X):\n",
    "    paths_as_array = tree.decision_path(X).toarray()\n",
    "    paths = [\"\".join(item) for item in paths_as_array.astype(str)]\n",
    "    return paths\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = lambdas_per_query.groupby('qid').apply(compute_swaps_scaled_with_weights, \n",
    "                                                                   axis=1, metric=dcg)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        print(\"mean   \", lambdas_per_query['train_dcg'].mean())\n",
    "        print(\"median \", lambdas_per_query['train_dcg'].median())\n",
    "        print(\"----------\")\n",
    "\n",
    "        \n",
    "        # Reset the dataframe for further processing\n",
    "\n",
    "        lambdas_per_query = lambdas_per_query.drop('qid', axis=1).reset_index().drop(['level_1', 'index'], axis=1)\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
<<<<<<< HEAD
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.1, metric=dcg)"
=======
    "lambdas_per_query = lambda_mart(judgments=judgments, rounds=10, max_leaf_nodes=10, metric=dcg)"
>>>>>>> 4ef4e05 (Save lambda mart)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                 262\n",
       "uid                               8_69315\n",
       "qid                                     8\n",
       "keywords             battlestar galactica\n",
       "docId                               69315\n",
       "grade                                   3\n",
       "features           [15.908707, 31.961138]\n",
       "last_prediction                       NaN\n",
       "display_rank                           26\n",
       "train_dcg                               0\n",
       "dcg                                     0\n",
       "lambda                                  0\n",
       "weight                                  0\n",
       "path                  1010001000000000001\n",
       "Name: (8, 26), dtype: object"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "source": [
    "lambdas_per_query.iloc[282]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path\n",
       "1010001000000000001         NaN\n",
       "1010001000000000010    1.995794\n",
       "1010010000100000000   -2.000000\n",
       "1010010001001010000    2.000000\n",
       "1010010001001100000    1.846154\n",
       "1010010001010000000    2.000000\n",
       "1100100010000000000    1.415385\n",
       "1100100100000000100    2.000000\n",
       "1100100100000001000   -2.000000\n",
       "1101000000000000000   -1.297171\n",
       "dtype: float64"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "source": [
    "ensemble[0].predictions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(141.64615384615385, 199.32, 'X[0] <= 10.666\\nmse = 0.044\\nsamples = 1390\\nvalue = 0.0'),\n",
       " Text(51.50769230769231, 163.07999999999998, 'X[0] <= 9.182\\nmse = 0.014\\nsamples = 1329\\nvalue = -0.029'),\n",
       " Text(25.753846153846155, 126.83999999999999, 'mse = 0.007\\nsamples = 1301\\nvalue = -0.035'),\n",
       " Text(77.26153846153846, 126.83999999999999, 'X[0] <= 9.451\\nmse = 0.252\\nsamples = 28\\nvalue = 0.244'),\n",
       " Text(51.50769230769231, 90.6, 'X[1] <= 4.527\\nmse = 0.302\\nsamples = 4\\nvalue = 0.847'),\n",
       " Text(25.753846153846155, 54.359999999999985, 'mse = 0.0\\nsamples = 1\\nvalue = -0.05'),\n",
       " Text(77.26153846153846, 54.359999999999985, 'mse = 0.045\\nsamples = 3\\nvalue = 1.146'),\n",
       " Text(103.01538461538462, 90.6, 'mse = 0.173\\nsamples = 24\\nvalue = 0.144'),\n",
       " Text(231.7846153846154, 163.07999999999998, 'X[0] <= 13.528\\nmse = 0.297\\nsamples = 61\\nvalue = 0.625'),\n",
       " Text(180.27692307692308, 126.83999999999999, 'X[1] <= 10.412\\nmse = 0.312\\nsamples = 32\\nvalue = 0.451'),\n",
       " Text(154.52307692307693, 90.6, 'X[0] <= 10.761\\nmse = 0.322\\nsamples = 27\\nvalue = 0.538'),\n",
       " Text(128.76923076923077, 54.359999999999985, 'mse = 0.003\\nsamples = 2\\nvalue = 1.331'),\n",
       " Text(180.27692307692308, 54.359999999999985, 'X[1] <= 9.943\\nmse = 0.293\\nsamples = 25\\nvalue = 0.474'),\n",
       " Text(154.52307692307693, 18.119999999999976, 'mse = 0.259\\nsamples = 22\\nvalue = 0.382'),\n",
       " Text(206.03076923076924, 18.119999999999976, 'mse = 0.023\\nsamples = 3\\nvalue = 1.15'),\n",
       " Text(206.03076923076924, 90.6, 'mse = 0.0\\nsamples = 5\\nvalue = -0.018'),\n",
       " Text(283.2923076923077, 126.83999999999999, 'X[1] <= 25.987\\nmse = 0.209\\nsamples = 29\\nvalue = 0.818'),\n",
       " Text(257.53846153846155, 90.6, 'mse = 0.192\\nsamples = 28\\nvalue = 0.847'),\n",
       " Text(309.04615384615386, 90.6, 'mse = -0.0\\nsamples = 1\\nvalue = 0.0')]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydeVyWVdr4vzc7jIiiL26Nw2suGTI4hv3SRkZpTHObMEpfc4MmyyVDRFQsJVQUWVNcU0nTYt7UitQWzbWURoYJE0YtlVfFgGJ9MGQ9vz8eeWKX5VnhfD+f+4Peyznnuq9zX88517nOOYoQAolEIpHoBzNDF0AikUjaE9LoSiQSiR6RRlcikUj0iDS6EolEokek0ZVIJBI9Io2uRCKR6BFpdCUSiUSPSKMrkUgkekQaXYlEItEj0uhKJBKJHrEwdAEk+sPW1jbz3r173QxdDm1hY2OTVVxc3N3Q5ZBImoMi115oPyiKItqSvhVFQQihGLocEklzkO4FiUQi0SPS6EokEokekT7ddsrRo0cpKSnB2tqakpISUlJScHd3Z8SIEYSEhGBhYcH8+fPJzs5m586dbNu2rd50ysvLsbBouBrt3r2bo0ePcuDAAQoKCmqk3bt3bwA+//xzUlJS6NOnD88//zxhYWFYWloyduxYrK2t2bdvH/b29vj7++vkXUgk+kS2dNsp48aNIykpidOnT+Pl5QXAyJEjOXHiBN7e3vj5+XHw4EHc3d3p0KFDjWdTUlKIjo4mLCyM3Nxczp8/T0xMjOa4d++e5l5fX1+cnZ0B6qRdxf79++nQoQNCCFJTU0lLSwPAysqKXbt24ejoiIWFBW3JHy1pv0ij206pqKhApVJRXFzcrOeSkpJ466236NevH/7+/jg5ObW6LAUFBcyfP59Tp05RWlqKs7Mz8+fPZ/Pmzdy9e5cJEybQoUMHkpOTW52XRGJoZPRCO6J69EJ0dDRjx45FpVLx3XffcefOHQICAqioqCAkJAQrKyvmzp1L7969CQgIICIiokZaqampfPnll0yZMoWePXs2mOeRI0eIjIxk4cKFjBo1qkba586dY/z48Xz44YcUFBSQm5vLW2+9xZIlS3B0dGTw4MF0796dQ4cOUVRUxJo1a7C3t68uj4xekJgc0ui2IxoLGYuPj6dTp06MHTu2xvmkpCQSExNZsGCBPorYLKTRlZgi0ui2I/QdpxsXF0deXh5mZmb4+fkB1DuYtmPHDq5evUpERASFhYVMmTKF8PBwBg0a1Gj60uhKTBEZvSABIDg4GAcHBy5dusTQoUNJTEwkNjaWwMBAXF1d8fHxYfXq1Tg5OWFra8ucOXMAtZvh2LFjmnSmTZum8fOmpaURHh5OYGCg5nrVYJqzszPx8fEMGDCAoUOHcvXqVYQQbN++ncmTJ+tXeIlEj8iBNIkGHx8fevXqxdSpU+nduzeKotCvXz/y8vLIysri8uXLODg4kJeX16T0FOXBjdBvvvmG06dPk5yczMWLF8nLy+PkyZOcOHGiteJIJEaJbOlKNFhYWGBmZqb5m5ubi4WFBT///DMALi4uFBYW4urqqnnGxcUFFxeXetMbOHAgUVFR9OrVi8rKSuLi4vD29q4zUAdw+/Zt3NzccHNz491338Xd3V33AkskBkD6dNsRcu0FicTwSPeCRCKR6BHpXpA0m/ridptL9enBOTk5vPfee9y4cYMFCxZw9uxZ8vPzyc/PJyQkhFWrVmFra8sf//hHxo0bpyUpJBLDII1uO2Tr1q2Ul5fj5uZGp06dOH36NNeuXSMmJoZJkybh6enJlStXGDBgAObm5tjb23Pjxg0qKyvx9fUFoKSkhKCgIHr27ImzszPZ2dmaND08PAA4f/483377rSbfV199FRsbG0A9Pbhqum+XLl1wdXXl7NmzWFpakpSUxJYtW5gxYwZ5eXnY2NiwbNkyAgMDpdGVmDzSvdAOGTx4MKWlpahUKlQqFebm5mRkZKBSqejTpw9+fn5YWlri5+dHeno6AKNHj8bLy4uTJ08CcPHiRYqKiujSpQuZmZk10mwJTz31FKtWreLq1avMnDmTjRs3kpubS1FRUZOiICQSU0G2dNsh+fn52NjYkJqaio2NDY6OjlRWVlJRUaFZMczKygr4LezryJEjFBcXs3DhQtLS0nB1dcXBwYGioiLc3NxqpDl+/HgAhg0bxrBhw+otw5EjR0hOTubjjz+mf//+JCQkkJGRwdy5cykoKKCsrIwJEybw+9//nnv37hEWFsbIkSN1/3IkEh0joxfaES2NXqgK4XrQDDF9I6MXJKaINLrtCBkyJpEYHunTlTSZ4OBgioqKWp1OZGQkUVFR7NmzR3MuPT2dwMBAAgICWuwXlkhMAWl0JTVYvnw5FRUVrFu3jpycHN5++20CAwP57rvvNPcEBARo/paUlLB48WIiIyNrLEze2MLmmZmZ+Pv7k5qaqjn30UcfsWjRIp577jmOHz+uB0klEsMgB9LaAYqidAN8m3LvM888Q0JCAoWFhVhbW1NeXk7Pnj05e/ZsnXuFEHWiGJpYnmaV/wFpWQshSrSWoESiY6TRbaMoimIGeAKvAKOBg40/ocbDwwNPT0+Cg4PJzs4mJyeHbt26UVFRobnH2tqa/fv3k5GRUSeKoYrGIhe6d+9OdHQ0gwYNIjMzkwsXLuDl5UVMTAyVlZWsXLmyOaLeUhRlD7BDCPFDcx6USAyBHEhrYyiK4gTMBl4GfgW2A/uFEAVtcSAN6I9a1tnA96jl/VgIUWq4kkkkDSONbhtAUVufkahbtWOBj1Abn2+rW9m2aHSrohcURbEGvFC/g4HAu8A7QohrhiuhRFIXaXRNGEVRugKzgDlAGWpD+54QIr+++21tbTPv3bvXTY9F1Ck2NjZZxcXF3WufVxRlAOp3MhP4N+r3kiCEKNNzESWSOkija2Lcb9V6oG7RjQM+AXYA59pUM1YLKIpiAzyH+l31A+JQt35vGLRgknaNNLomgqIojvzWqgXYhrpVm2u4UpkOiqI8itr3OwNIQt36/VQIUW7QgknaHdLoGjH3W7VPom6pTQQOozYWX8tWbctQFMUW8Eb9Tv8b2A3sFEL8n0ELJmk3SKNrhCiK0hl1i+wV1GF924E9QogcgxasjaEoyiDUPYcXgUTU7/mobP1KdIk0ukbC/VbtE6gN7bPAZ6iNwGnZqtUtiqLYAS+gfvcPAbtQt35vG7RgkjaJNLoGRlEUB2A66g/eBvWg2B4hxM8GLVg7RVGUP6LWxf8AX6P+4ftcCFHR6IMSSRORRtcA3G/VPo764/YCvkT9cZ8SQlQasmwSNYqi/A6YglpHPYCdwC4hRIZBCyYxeaTR1QOKokwCvgCsUfsPXwHsUbdq44QQ2QYsnuQBKIoyGLXOpgBnUP9AfimEqLg/MHdPuoAkTcUojG5bDtpXFGUeEAicQN2q/Qr1R/uVbNWaFoqidEDtdngF6Aq8AzgBnYDZ1Q1vW6nTDU1AkbQcozC6bXV6qqIo84FYoBD11NxlQoimLcUlMWoURXkMtfF9ATBHvd7DjGrX20SdlgvFax+5nq5uuQ7su38USYPbdhBC/As4jTqkrwR4SpE7aEqagGzp6gDZOpC0lTot67L2Mdr1dI8ePUpJSQnW1taUlJSQkpKCu7s7I0aMICQkBAsLC+bPn092djY7d+5k27Zt9aZTXl6u2eG2Pt588006d+6Mo6Mjs2fPBuD69euEhITg7e3NhAkTWL9+PZaWlty+fRs/Pz8++OADrl+/TkBAAP3799eF+JI2gr7q8e7duzl69CgHDhwgJyeH9957jxs3brBgwQL69esHwNy5cxkwYABPPPEEtra2fP7551y5coXw8HAOHDhAYWEhly9fZteuXTp5FxI1Rmt0x40bx4oVKygvLycsLIyUlBRGjhzJsWPH8Pb2xtnZmfj4eBYtWkR8fHyNZ1NSUjhx4gSlpaX4+Phw7do1vv32W831V199FRsbGwBycnJYvXo1EydO1BjdPn36MHv2bM1+YJWVlRQWFuLo6Mgf/vAHli1bRnx8PBkZGdLoShpFX/XY19eXtLQ0ALp06YKrqytnz57F0tJSc3+3bt002ya5ubnh5uZGeHg4eXl5WFlZ8csvv9CxY0ddv5J2j9H6dCsqKlCpVBQXFzfruaSkJN566y369euHv78/Tk5Ojd7v4eHBpk2bcHBwaPAeMzMz3nrrLXJy1LNwz58/z82bNxk1alSzyiZpf+irHtfmqaeeYtWqVVy9elVzLjg4WNNgAEhISKBr16707duXrKwswsLC6NixIwUFBc3KS9I8jLalu3HjRubOnYtKpWLHjh2a856enoSEhGBlZcXcuXPrPOfu7s6hQ4dITU0lNjaWKVOmNLp1jKIoFBcXM336dM3WMcOHD+fAgQOUlpbi7u5OUVER0dHR2NnZcfXqVV5//XWmTZtGamoqLi4uOnsHEtNHX/X4yJEjJCcn8/HHH9O/f38SEhLIyMhg7ty5xMfHM378ePbu3Ut2djaDBg3izJkzREVF8eyzz3Lr1i2srKw0rV57e3udvQ+JCQ2kxcfH06lTJ8aOHVvjfFJSEomJiSxYsECXRWwWcvBB0lCdNqV6DLIu6wKTMbraJC4ujry8PMzMzPDz8wOgoKCgxsDGjz/+SEhICIcPH6ZDhw4UFhYyZcoUwsPDGTRoUKPpy4oq0Wedrq8+p6Sk1Bgo++STT8jPzyc/P58VK1awdetWfvrpJ2xtbQkODm5MDlmXtYzRuheqCA4OxsHBgUuXLjF06FASExOJjY0lMDAQV1dXfHx8WL16NU5OTtja2jJnjnqN79TUVI4dO6ZJZ9q0aRq/WFpaGuHh4QQGBmqunzhxos7AxpkzZwD1VuPbt29n8uTJepRc0hbRV32uPVCWlJTEli1bmDFjBsXFxfj5+REaGsr06dP1+wIkxjuQVh0fHx969erF1KlT6d27N4qi0K9fP/Ly8sjKyuLy5cs4ODiQl5fXpPSaG8N++fJl8vLyOHnyJCdOnGiJCBKJBn3V5+oDZTNnzmTjxo3k5uZibm5OZWUlt2/fpnfv3toUTdIEjL6lC2BhYYGZmZnmb25uLhYWFvz8s3r1QxcXFwoLC3F1ddU84+Li0uAg18CBA4mKiqJXr15UVlYSFxeHt7d3jYGNlJQUEhMT2b59O4sXLyY0NJR3330Xd3d3vcgsabvooz7369evxkCZEIKysjImTJiAvb09n332GWPGjNGLvJKatEufrq6RfjBJW6nTsi5rH5NwL0gkEklboU0Z3YCAgFansXv3bry9vQH1bLWYmBhef/11fvjhB/bt28eaNWt46623qKysxN/fn9WrV/P555+3Ol+J5EFoo37Hx8cTHh7OF198QW5uLn5+fsTGxmqhdJKmYlQ+3a1bt1JeXo6bmxudOnXi9OnTXLt2jZiYGCZNmoSnpydXrlxhwIABmJubY29vz40bN6isrMTX1xeAkpISgoKC6NmzJ87OzmRnZ2vS9PDwANQzyloynXL69OlUVFSwcOFCcnNzsbe3Z8mSJcyYMaNO3KVEUhtjqN/vv/++pq46Ojri5+fH4cOH9fwm2jdG1dIdPHgwpaWlqFQqVCoV5ubmZGRkoFKp6NOnD35+flhaWuLn50d6ejoAo0ePxsvLi5MnTwJw8eJFioqK6NKlC5mZmTXSbAnVp1OWl5fzxhtv4O/vT9euXXnooYfYvn073bqZ/FrVEj1gLPV73rx5fPLJJ7oQUdIEjKqlm5+fj42NDampqdjY2ODo6EhlZSUVFRWaFZasrKyA38Jkjhw5QnFxMQsXLiQtLQ1XV1ccHBwoKirCzc2tRprjx48HaPF0yvnz52NnZ8epU6d4+OGHAbh79y4zZ87U9auRtAGMoX7/5S9/4e233+bhhx+mrKyM3bt3k5aWxtNPPy0Xb9ITJh29UBXC9aAZYvpGjvhKtBG9YAz1W9Zl7WPSRtdYkRVV0lbqtKzL2seo3AtNITg4mICAADp06KCV9JqyDsOZM2dIT0+noqKCN998k4CAADp37szQoUPlAJpEK2irXkdGRqIoCl26dGHWrFkA5Obm8vbbb9OxY0fmzJnD3r17+eWXX3B0dGTGjBnMmjWLUaNG4evrK9fT1QNGNZBWxfLly6moqGDdunXk5OTw9ttvExgYyHfffae5pyp8JiAggJKSEhYvXkxkZCQHDx7U3HP+/HliYmI0R9UCztVJS0vD39+fO3fuaM5VrcPg5+fHwYMHmT59OsuXLyc7O7tG1IJcYV/SHPRRrzMzM/H39yc1NVVz7oMPPsDa2hpA05BYvHgx6enpmJmZ4eTkxN27dzEzM0pz0OYwyrf8zDPPkJCQQGFhIdbW1pSXl9OzZ0/Onj1b514hRJ0R3Qfx66+/EhMTQ0pKSpPWYZBRCxJtoOt6DfWvw1BaWsqwYcP405/+xNGjR7l79y4rV65k+fLldOzYkXfeeYdx48Zx6NChVssoeTBG6V7w8PDA09OT4OBgsrOzycnJoVu3blRUVGjusba2Zv/+/WRkZNQZ0a2ioVFcOzs7jSshOTn5geswyKgFiTbQdb0G6N69O9HR0QwaNEizKP9zzz3Hxo0bEULw+uuv88ILL/DYY49x8uRJHn/8ceLj47l58yb+/v46fwcSOZCmE+Tgg6St1GlZl7WPUboXJBKJpK1iFO4FGxubLEVR2oyD1MbGJsvQZZAYlrZSp2Vd1j5G4V5oKYqizAVeAoYLIUq1lOarwMvAMG2lKWm/KIry38B7wD1gthDitoGLVC+KojgDe4EKYJYQ4qZBC9SGMVn3gqIoA4EQ4EUtG8ftQMb9tCWSFqGomQX8EzgIPG2sBhdACJEOjAK+AJIURfkfw5ao7WKSLV1FUayBRGCLEOIdHaT/X8B3wCvAQ0KIbdrOQ9J2URTFEfWP9yOoGwUXDVykZqEoyhBgP/AvYIEQIt/ARWpTmKrRDQf6ApN1NUR833XxBmAphHDSRR6StsP9nlcfoASIAz4EgoQQdWfkmACKotgB4cB4YBbgAPxHCPGDQQvWBjA5o6soyl+Bd4HBQohfdJSHOfAN4Ax0AxyFEE3bJVDS7lDUMxJOo/bbDgR8hBDHDVsq7aAoyjhgJ3AJqBBCPGPgIpk8JuPTVRTFU1GUaagNro+uDC6AEKICGA4EAAXA73WVl6RN8BIwAngY+KKtGFwAIcRR4DjQHxh7/xuUtAKjCBlrIl6AB+oWRYquMxNCVAL77h8SSWN0A74CPkU9cNbW2AEkA5OAHgYui8ljMu4FRVFSUftxfwImmdrghEQikYBptXRLgAggRAhR0pqEbG1tM+/du2eSges2NjZZxcXF3Q1dDm1jyjqpTVvTkdSNdjGZlq42MeV58W11Lrwp66Q2bU1HUjfaxWQG0iQSiaQt0Kh7oS11K8A4uhYSiaR906h7oS11K+C3rkV1uY4ePUpJSQnW1taUlJSQkpKCu7s7I0aMqLFlT3Z2Njt37mTbtvonp5WXl2t2dK2PN998k86dO+Po6Mjs2bM157/66is2b97MoUOHmDt3LgMGDOCJJ56gf//+hISE0LdvXxYsWFBHBu28EeOhKToZPnx4jXeSlJTUKp3s3r2bo0ePcuDAgTpbNPXu3Vtz34oVK+jRowfz5s0jJCSEoqIiIiIi+PTTT7l06RKZmZm8/fbb1WVpUzoyhG5CQ0MpLCxkyJAh9O/fn7fffhs3NzfNOthQ85vy9PRkw4YN2NnZMWnSJPLy8rh27RoHDx6ssUi8Meim3bsXxo0bR1JSEqdPn8bLywuAkSNH1tmyx93dvc7+VSkpKURHRxMWFkZubm6j26jk5OTg7+9fY9uV69evk52dTZ8+fQDo1q2b5hlHR8caFaw90ZBOar+T1urE19cXZ2dnoO4WTVXs3buXMWPGAGBmZkZwcLDm2sSJE1m+fHm920C1VfSlm6CgIF577TUuX76MlZUVnTt3pqysrMaC79W/KXNzc/Lz88nPz+ehhx5i4sSJeHl5MW7cOB2/keZjVNELTdkkMiUlhdTUVEpLS1m6dClbt27lp59+wtbWtsYH0VQqKipQqVTNfi4pKYnQ0FB8fX0ZM2YMlpaWXLt2rcH7PTw82LRpEw4ODppzX3zxBSUlJSQnJ5Oamqopv5+fH0888USzy9RW0JdOmsKFCxf48ccfSUtLY/78+XW2w1m/fj0vvfRSq/IwJfSlm6ysLCIjI1m7di22trZERUXx0Ucf8c033+Dh4QHU/KZu3rzJnDlz6NWrF4cPH2bBggXs2bOHl19+ucWy6ooWG93g4GAcHBy4dOkSQ4cOJTExkdjYWAIDA3F1dcXHx4fVq1fj5OSEra0tc+bMASA1NZVjx45p0pk2bRpOTuqlDdLS0ggPDycwMFBzvaoF4uzsTHx8PHfu3CE8PJx169bx66+/4ufnR2hoKNOnT2+RHBs3bmTu3LmoVCp27NihOe/p6Vljy57auLu7c+jQIVJTU4mNjWXKlCmNbqOiKArFxcVMnz5ds41KVbq3b9/GxcWFzZs3k52dzaBBgygrK2P37t2kpaXx9NNP079//xbJZ4o0pJMHvZPm6uTIkSMkJyfz8ccf19F3fHw848ePZ9OmTaSnp3P48GEURWH79u0kJyeTlJTEl19+ydWrV7GxsWHo0KFN2m/P1NGHboQQTJw4kcmTJ3PmzBm6d+/O559/zo0bN1i3bh07duxgzpw5Nb6pzp07s3HjRjp27Mhzzz1HRUUF2dnZ9OhhhHM5hBANHurL9bNq1SqRl5cn3nzzTc3foqIiERUVJdauXSvS09PF5MmTRVxcnFi/fr3muUuXLono6GjNkZWVpbm2ZMmSGn+FEOLQoUPi3Llz4s6dOyIqKkpzbd26dSI3N1dUVFSIuXPnNljO6tyXp1G5PvjgA/HZZ5/VOX/hwgWxadOmJuWjS6pkaGuHKeukNm1NR1I32j1a5V6wsLDAzMxM8zc3NxcLCwt+/vlnAFxcXCgsLMTV1VXzjIuLCy4uLvWmN3DgwAduEpmSkkJYWBhlZWV07tyZzz77TONz0wZTp06t97y7uzvu7u7NSqs+d0lKSgqff/45V65cITw8nA0bNtCjRw8effRRBgwYwAcffMD169cJCAhoV63bxtCmTqqoTzc5OTm899573LhxgwULFnDr1i1CQkI4fPgwlZWVxMbGkpmZyfPPP8+IESNaLE9bQl+6qf3dnDt3TuNmfOONNwgICKBz584MHTqUsWPHtlgefdDuoxeq0IW7ZMmSJRp3yYYNG2rkFx4ejpeXFwkJCVRWVjJw4EDGjx8PQHx8PN26dWPUqFENyqDVF2MENFbX9Kmbr776ii1bthAZGYmzszPBwcEEBARoBoUSExP5z3/+g4+PT2OytCkdGYtu4LfvZvv27Ro349///ndiY2NZvnw5M2bM4MMPP2xMFoPrpt1HL1THx8eHXr16MXXqVHr37o2iKPTr14+8vDyysrK4fPkyDg4O5OU1bZXHhnx8CQkJdO3alb59++Lv709AQICmAp4/f56bN2/Wa3DbM/rSzVNPPcWqVau4evVqnWtXr14lISGBWbNmtUqWtoYhvpuqexRFwcLCgoceeojt27fTrZvxTyvQS/RCQEAAERERrUqjekxl7W7g5cuXNV2NlStXEhUVRVpaGjt37mxWHvpwl/Tr14+oqCieffZZbt26xenTp7l+/To9e/bk6tWrvP7660ybNo3U1NQG022P6EM3w4YNIyEhgYyMDI0rKzExke3bt+Pj48PUqVOZPn063377bYODc+0RQ3w3I0aMqOFmBLh79y4zZ87UvcCtpEnuha1bt1JeXo6bmxudOnXi9OnTXLt2jZiYGCZNmoSnpydXrlxhwIABmJubY29vz40bN6isrMTX15fNmzezdu1agoKC6NmzJ87OzmRnZ2vSrAoBOX/+PN9++60m/1dffRUbGxvN/6sb7+rdwM2bN2u6Gq+++iqdO3eu19A35l4wFYyhe6QLTFkntWlrOpK60S5Nci8MHjyY0tJSVCoVKpUKc3NzMjIyUKlU9OnTBz8/PywtLfHz8yM9PR2A0aNH4+XlxcmTJwG4ePEiRUVFdOnShczMzBpptoTq3cDqXQ2JRCIxZprkXsjPz8fGxobU1FRsbGxwdHSksrKSiooKzVQ+Kysr4DfDd+TIEYqLi1m4cCFpaWm4urri4OBAUVERbm5uNdKsGkBqakxl//79a3QDa3c1PvjgA5KTkzlx4gSenp6tfkkPQhvuk+rTHp944okaUxr//Oc/a6mk7Q9t6CY+Pp709HQ6duzIk08+WWMUvUuXLloqaftDF99NTEwMAP7+/jz00EPaKKbW0Un0wrvvvou7uzuDBg1qTdm0TkPuBWNxn2RkZLBr1y5eeuklli5dip2dHUFBQZqpqtVl0P3b0i8N1TVj0E1mZibr169n4MCBvPLKK8Bvo+h9+/atT5Y2pSNj1g389t08+uijdOnShcrKSr7//vt6p9Ebg250Er0we/ZsozO4jWEM7pOqaY9LlizRTGlcunQphw8f1pXYJoEx6KZ79+7ExMRo7q8+it6eMQbdVP9unnnmGZKSkjh37hyWlpa6ErvVaC16oXY8Y2tpyjoMN2/eJDExkezsbDZs2MDEiRN56qmn8Pb2blbXwtDuEyFqTnv8wx/+UGNKY3vG0LoB2LBhA2VlZdjb23PmzJkao+i//3373bPU0Lqp/d0MHz4cUK9g9j//8z/6eAUto7HpatSa/rds2TJRXl4uQkNDxS+//CJiYmLEkiVLxL///W+xatUqoVKpxOLFi4UQQixevFjcu3dP+Pv7i4iICHHgwAFNOufOnasxFbi4uLjOdL2AgAAhRONTgoUQYu3atZppwC+99JIICQkRP/30U5307vePqrpJ9V5vKnFxceL7779vVRotBSOYxqiLo7U6qcKQuqmirelI6ka7R7Naus888wwJCQkUFhZibW1NeXk5PXv2rLFeZXVjXrvr8CB+/fVXduzYwahRo5ociRAUFERUVBSVlZXs3LmTvLw8Nm3axMqVK5sjWrOovh6uxLiQujFepG7UNMvoenh44OnpSXBwMNnZ2eTk5NCtW7caa+jo1okAACAASURBVFxaW1uzf/9+MjIy6nQdqmiou2BnZ6dxJSQnJz9wHYYPPviAW7duafxIW7ZsITs7mxdeeKGl76MG2nKZREZGoigKXbp00cxmOnPmDOfPn+eHH35g69atWFpaahbLrpr3//zzz3P8+HGtuWzaErrUDTR/cXnJb+hSNxcuXODDDz/k7t27LFq0iIsXL7JlyxaOHz9OUVERAQEB9OzZkwEDBjBlyhRtiKN1mu3TPXHihObfoaGhda6vXbsWgBdffBGgztzpplJ7bnvVmqWRkZGac9VX9wdYvnx5s/NZvnw5a9asYcOGDcyZM4d9+/aRkZHBtGnTNPdUhbYEBATUGW2t8rk+aPS79pKVHh4eeHh4EBAQwL179/jggw8YM2YMFy9epLS0lH/84x9Gv3CHrjGUbpqyuHx7H+A0lG6srKzIzc2lsrISJycnJk+ezLlz5wD1zLiCggKsrKyM+ttp92sv6NplAg1P2ti1axejRo3C3t6eCxcucPz4cU6dOsU333xDcXExiYmJNX7k2huG0s0XX3xBVlZWjcXlly1bRnx8fKtlaisYSjdXrlwhKCiIF198kTNnztS4lpWVxd/+9jc2bNjAoUOHWiaYHjCqnSMMga5dJqAOOYqOjmbQoEGaBcxVKhWffPIJnp6eFBQU1Fgse9SoUYwaNYrg4GC9TO4wVgylG7m4/IMxlG66devGli1bKC8vZ9GiRZw9e5bk5GT27t3LuHHjOH78OFevXuXJJ5/U+TtoKY1OjmiruwGb8lxyYwju1gWmrJPatDUdSd1ouQxt5WU2B1OuRMZQaXSBKeukNm1NR1I32qVduhdsbGyyFEUxyRa8jY1NlqHLoAtMWSe1aWs6krrRLu2ypdtcFEV5E/gL8LQQolIL6VkB54F3hBDbWptee0ZRlOXA08BfhRAVD7q/CelZAueAPUKI2Nam155RFGUJMAkYqSXdWABngX8IIWJam56hkEb3ASiK8gTwCTBECJGhxXQfAb4GRggh/qOtdNsTiqIMBY4Ajwkhbmkx3X6ofxT/IoRI1Va67QlFUYYAXwBDhRDpWkz3YSAReEoIcVFb6eqTdh8y1hCKogxUFOX3wH5gnjYNLoAQ4jKwAnhfURRrbabd1lEUZYCiKL1R62aBNg0ugBDiB2Apat3YPOh+SU0URbED3gde16bBBRBCXAMCUOvGVptp6wvZ0m0ARVGOAFbA/wkh/q6jPBTgY+CKECLwQfdL1CiK8glgC9wRQszWUR4KcAC1/v11kUdbRVGULUBHIcR0HaWvAPFAphDidV3koUuk0W0ARVEyARugDHATQtzRUT7/BXwHzBRCfKWLPNoaiqJkAPbAPbTsWqiVTxcgBfAVQnypizzaGoqiTAQ2AoOFEAU6zKczat28IoT4TFf56ALpXqiH+7+kTkASap+rTgwugBDiZ8AXeFdRFBdFUf6frvJqC9zXTQ/gX6h9rjoxuABCiBxgFhB3XzdP6CovU0dRlD8riuIC7ABm6NLgAggh8oCZwK77uhmuy/y0iWzpNoCiKN2EEHoJL7lvSGKBwUC+EGK8PvI1VQygmxjgcaBACGG8k/oNiKIop1G7fE4Ay/UR2HtfNxHAk0CREOKvus5TG0ijawQoivIQ6q6SBVAihHAycJEk91EUpQdwCXWvsEII0dXARTJKFEUpRO2KE8CA+70EXefpBPwHUFDbss66zlMbSPeCESCEuA24AEcBR0Vua2w0CCF+Ah5FHZrWSVEU+c3U4n5s8+9Qh1YO1IfBBRBCZAMDgU+BjoqimOsj39ZiVC1dU17roWpdh9amoyiKmTYmYGgTU9ZLdVqrI2PUjbFg6Hdj6Pybg1EZXVOe420Mc7p1hSnrpTptWUcS00F2lSQSiUSPtMsFb/SFqXbLteUqMVZMVS9V1KcfU5XpQXXNFORq7vditO6Fo0ePUlJSgrW1NSUlJaSkpODu7s7w4cNr7FGVlJTEzp072bat/nVjysvLNdtB10dZWRkvv/wy3t7eTJgwAYCPP/6Ys2fP4uLigq+vL6GhoRQWFjJkyBAee+wxPv30U86ePctrr73GyJEjq8pep+tqqt3y2rJUydGQTkaMGEFISAgWFhbMnz+f7OzsVulk9+7dHD16lAMHDlBQUFAj7aotmvbv38/PP//MsWPHOHLkCGFhYVhaWjJ27Fju3btHSEgIa9asYdCgQXXkMlW9VNGW61o9141erua6rYy2pTtu3DhWrFhBeXk5YWFhpKSkMHLkSDp06FBjjyp3d/c626ikpKRw4sQJSktL8fHx4dq1aw3uw7Rt2zYmT55c4/nf/e53dOjQQbMvVlBQEBkZGezatYsXXngBPz8/rly5wl/+8hddvgKjoyGdHDt2DG9vb5ydnYmPj2fRokWt0omvry9paWmAek++2mmDeg++s2fP0rVrV1JTU0lLS8PNzQ0rKyseffRRnn32WT29FYmkeRit0a2oqEClUjX7uaSkJEJDQ/H19WXMmDFYWlpy7dq1eu/Nzc3lxo0bXLlyBSsrK01Ld/To0YwePZro6Ghu3ryJtbU1kZGRmk03f/zxRx5++OEmbxOva+Li4sjLy8PMzEyzm3Jubm6NHsGnn37KtWvXOHjwIGfPnmXHjh1kZmbyyCOPNHn3ZH3opDkcPHiQ9evX85///AdnZ2fmz5/PsmXLiI6ObnXauqA+PTXUkjdGmlLPdu/eTX5+Pvn5+QQEBBAbG0tmZibPP/88I0aMMLAEjfPpp5+SmppKaWkpK1euBKCyspLAwEDs7Oz429/+xmOPPdbqfIzW6G7cuJG5c+eiUqnYsWOH5vyD9qhyd3fn0KFDpKamEhsby5QpUxrch8nR0ZGoqChOnTpFUVGRZh8mBwcHzp07R0ZGBj169ODJJ59k8uTJnDlzhjFjxrB3715ee+21FskVHByMg4MDly5dYujQoSQmJhIbG0tgYCCurq74+PiwevVqnJycsLW1Zc6cOQCkpqZy7NgxTTrTpk3DyUk9hyItLa3Orqm1d62dOHEi//d//0dxcTEAI0eOJCwsjCFDhjS57A3pxNPTk5CQEKysrDT7i1WnOToBOHLkCMnJyXz88cd10o6Pj2f8+PGUl5djZ2eHjY0Nbm5u7Nu3j4iICP76179y48YNvvzySy5fvszKlSuxs7NrsoxV6EtPDbXkW4uh6llSUhJbtmxhxowZVFZWEhQURGJiIv/5z3+0YnR1IVcVZ86cITw8nHXr1pGXl0fnzp1JSUnhscceY8qUKaxYsaJtG93qle/xxx8nPj6er7/+mrFjxxISEqK5lpSUhLOzc53nXVxccHFxaVJeVX5ZUBsnUG+8V8U///nPGvdXz78l+Pj4EBUVxdSpU7lz5w6KotCvXz/y8vLIysri8uXL9OvXj6ysps10bWqLe8+ePbz88ssA9O/fn3feeYeYmKavBd2YTiIjIzXXWquT8ePHM378bzOhq6ddvSUYGhoKgJmZWY17AN5///0HC/QADKUnbWGI8s+cOZONGzeSm5uLubk5V69eJSEhgTVr1rRWHA3alis2NhY3NzeNfLrWk9Ea3dpMnTq13vPu7u64u7s3K636ukk5OTm899573LhxgwULFnD58mVNV8PPz0+r3SQLCwvMzMw0f3Nzc7GwsODnn38G1MapsLAQV1dXzTONGayBAwcSFRVFr169qKysJC4ujpkzZ9boETz88MNkZ2fTo0cP7t27R0REBGVlZTzyyCMtlkObOqmiKbr59ttvSU9Pp6KigkWLFumsC6sPPXl7ezfaSzD28teuZ0IIysrKmDBhAmVlZUydOpXp06fz7bffNtizMbRcCxYsACA/P5+wsDDKysro3LkzO3bs4KWXXuL999/n8uXLeHt7a6X8CCGM5lAXpyarVq0SUVFRwtfXV2zdulXMmjVLqFQqMXfuXLFlyxZRXFwsgoKCRExMjNi+fbvmuUuXLono6GjNkZWVpbkWEBAghBBiyZIlNfI6fvy4mDx5srhx44bmntDQUJGbmyuEEOL8+fNi9+7ddcp4f3hVNEUeU6C2LA3JYSjdCCFEeXm5mDdvnuZ6Y7qpLZep6qWKtlzXah+mINeDZKh9mMTkCB8fH3r16sXUqVPp3bt3vd0JBwcH8vLympReQ92Hp556ilWrVnH16tU6XY2qbtKsWbO0I1QbwRC6KS8v54033sDfX722uNSNxJQwCaPb0u6En5+f5qjuNK/dTdq1axdpaWmsX7+ed955h4ceeogRI0ZouhpCCKZOnUrXrl1rhDnpk4CAgFanERoayrJly/jf//1fAL777rtWh70ZQjfz58/n3r17nDp1itzcXIPrpgpt6OjFF18kJiZGEzJnCLRd127evMmCBQsIDAzk66+/1kIJm482ZNq9e7dWXAxGOznC1HhQwPrWrVspLy/Hzc2NTp06cfr0aa5du0ZMTAyTJk3C09OTK1euMGDAAMzNzbG3t+fGjRtUVlbi6+vL5s2bWbt2LUFBQfTs2RNnZ2eys7M1aVYN/J0/f77B+FdAE2/82muvER8fz7Vr14iIiGhUFlPWS3UeNDnCGHTk7++Po6Mjzz//PAMGDGhUjlrnjLauvfTSSyxduhQ7OzuCgoJqDLI2dXKEscgUEBDwwO/lQZhES7e1aONXrqysjNmzZ2tCY5rL4MGDKS0tRaVSoVKpMDc3JyMjA5VKRZ8+ffDz88PS0hI/Pz/S09MBdbywl5cXJ0+eBODixYsUFRXRpUsXMjMza6TZFLKysoiMjGTJkiUcO3aMu3fvkpyczPnz51skU2vQdmuq6gN87rnnOHXqVIvSMwYdRUVFsXz58hoheaYoR/W6dvPmTebMmcPSpUtN+vvRFiYRvdDaXzmAkpKSVv3K1TdzrTnk5+djY2NDamoqNjY2ODo6UllZSUVFhWZKrJWVFfCbX/PIkSMUFxezcOFC0tLScHV1xcHBgaKiItzc3GqkWRVi1VD8qxCCiRMnauKNqyZEZGZmtmhU2Rh0ou2ZgobWEcC6detQqVStisIwtBy169of/vAHNm7cSMeOHXnuuedMUqaq9Krix1sz49Ek3Avnz5/n3LlzPPLII3Tq1ImUlBROnjzJ7t27efPNN4mJiWHhwoVs3LiRxYsX4+rqSt++fbGzs+O7774jLS2NKVOmsHPnToYNG8bdu3cZMmSIJs2qF97QB56bm8uaNWsoLS3FysqKqKio+squ1fnw7777Lu7u7jXWDtAXTXEvGFonoG5NhYWFsXbtWmxtbfnxxx/5+OOPG2xFa3vtBUPpqC3XtXqut0gufcrUZtZeqI6hf+Vqz1zTNsHBwQQEBNChQwfNudmzZzc7ncjISBRFoUuXLpqR/OvXrxMSElJjQZ8VK1bQo0cPFixYwK1bt3j++ec5fvx4jfwfhKF1Urs11dqZgvVRn16q01wdNWUa8I8//khISAiHDx+mQ4cOzJs3j//+7//G2tqahQsX6kSe5spRXz27cOECH374IXfv3mXRokX07dtXU8+mTJnC/v37SU1N5cknn2xR3X4QtWVraR71yZaens6WLVuorKxk1apV2Nvbt66wzYkv0/WBlmLy4uLixPfff6+VtJoKD4idXLZsmSgvLxehoaHil19+ETExMWLJkiXi3//+t1i1apVQqVRi8eLFQgghFi9eLO7duyf8/f1FRESEOHDggCadc+fO1YhxLS4u1lxrKMb15MmT4tNPPxVCCLFnzx5x+vRpsWnTJlFSUiLCw8M1+Tckizb0Ygid1IZ64nT1oZcq6tPPoUOHxLlz58SdO3dEVFSUEELU0MesWbPE0qVLxUcffVRHDlGPjgxVz7777jvx0ksvCR8fH1FQUFCjnlWxaNEicffu3Xp10tBhDN9QVFSUuHPnjjh37pw4dOiQqM2DZKh9tMmBtNmzZxukq9QYzzzzDAkJCRQWFmJtbU15eTk9e/bk7Nmzde4VQtRx+jeFpkxfvHDhAsePH+fUqVN88803FBcXk5iYyIkTJ5otU3MwRp2A7vXy66+/EhMTQ0pKSrOnl/76668MGTKE9evXN3lw0FD17MqVKwQFBfHiiy9y5syZGvVMCEFhYSEWFhYtWgfD0LJpG5NwL9TmQd2+pqLPbpKHhweenp4EBweTnZ1NTk4O3bp1o6KiQnOPtbU1+/fvJyMjo07Xu4rGHP3du3cnOjqaQYMGaRbvGT58OAcOHKC0tBR3d3c2bdpEeno6hw8fZtSoUYwaNYrg4GA8PT1b9hJroUvd1F7nOD4+nvT0dDp27MiECRPYsGEDdnZ2TJo0iT//+c9NykfXerGzs9O4EpKTkx84DTglJYXExES2b9/OwoULSUlJISIiosnriBiqnnXr1o0tW7ZQXl7OokWLmDBhgqaeKYrC+++/z4svvtgkGYxNNi8vL2JiYqisrNSsPtYqmtMs1vWBEXQltNlNwgSmMNZHbVlqy2Eo3Xz55Zdi5cqVYvPmzUIIIX766Sfx+uuvi23btonbt2+LF198Ubz88suaqcINyWWqeqmiLde12ocpyPUgGWofRuteaMvdJFPHULoZPXo0b731FiUlJdy8eZPu3bsTExODSqXSSiyoRKIPjNa90Ja7SaaOoXRTe53jDRs2UFZWhr29PZ07d251LKhEog9MIk7XFKgvVs8UNtWrj9ob7ZmyXqpTpSNT1UsVcmNK46LNbExpajQ3QNqUMGW9VKct60hiOhitT1cikUjaIkbl07WxsclSFMWouxINYWNj07S9QUwQU9ZLddqyjiSmg1G5F5qDoigWwGngoBCi7mIILU/3IeBfwEQhxD8fdL+kLoqimAMngcNCiA1aTLcnkAxMFkKc01a6Eok+MWX3QhDwK9D0nRWbgBDiNjAP2K8oSusi/NsvS4FyIOJBNzYHIcQd4FVgn6IoHaV+JKaISbZ0FUUZBnwEDLn/Ieoij10AQoiXdJF+W0VRlKHAYcBdCHFLR3lsBxyBgUII45tbLJE0gsm1dBVFsQf2AXN1ZXDv8zrgoSiKlrYAbfvcb3nuBxbo0OCaAd2BJ4H+iqJY6SIfiURXmExL9/4HbQFEA2VCiDl6yPNx4FPgMdC4HiS1UBTld4AVsAEwF0L46ji/0cAmYADw/6TvXWJKmJLRXQk8CvwJtVvhrp7yXQGMAfoLIZocAN2eUBQlCHBD/eP0JyGEzvc/ud/inQe8K4TQ/iLHEomOMCX3wmPAJCAPeFwfGSrqBQBGAg8BjoqidNFHvibIY8AE4Bfg/+kjQyFEpRAiVhpcialhSkZ3BFAKbAPO6CPD+9Ow/IAfAEtguD7yNUH+gjpaYQdwyrBFkUiMG1NyLzwNfC2E+NVA+Y8GvjFU/saMoih/Bc5rw+VjCnPtG6K5c/Al7ROTMbqS9oEpr/Mg13aQNAVTci9IJBKJydOstRdMrevXWHfP1GSBhuUxRVlAdscl7ZNmuRdMrevXWHfP1GSBhuUxRVmgfnmqZDl69CglJSVYW1tTUlJCSkoK7u7uDB8+nJCQEPr27cuCBQtISkpi586dbNu2rd48ysvLNVvC18ebb75J586dcXR01Ox5969//YuDBw9y9+5dQkND+d3vfqfZJ2/OnDls27aNjIwMJkyYwIgRIxqVRyKpjXQvSIyScePGkZSUxOnTp/Hy8gJg5MiRODo6ajZ6BHB3d6+zCWZKSgrR0dGEhYWRm5vL+fPniYmJ0Rz37t3T3JuTk4O/vz8HDx7UnDt58iQvv/wyf/zjHzl27Bh79+5lzJgxAFhZWTF8+HBu3bqFjY2NLl+BpI1iVEs71iYuLo68vDzMzMw0H1pBQQEhISFYWFgwf/58evfubeBSNkxTy79jxw6uXr1KREQEEydO5KmnnsLb25uHHnrIwBL8RlNkSUlJITU1ldLSUlauXMmOHTvIzMzkkUce4YUXXmhWfhUVFahUzZ9jkZSURGhoKL6+vowZMwZLS0uuXbvW4P0eHh5s2rQJBwcHzbkZM2awd+9eMjIyGD16NBcuXODHH38kLS2N+fPn4+7uztatW/nf//1fhg4d2uwySto3WjO6wcHBODg4cOnSJYYOHUpiYiKxsbEEBgbi6uqKj48Pq1evxsnJCVtbW+bMUc/iTU1N5dixY5p0pk2bhpOTEwBpaWmEh4cTGBiouX7ixAm8vb1xdnYmPj6eRYsWmXT5BwwYwNChQ7l69SoA3bp1Q6VSNdolNlZZ7ty5Q3h4OOvWrSMvL4+RI0cSFhbGkCFDmi3Dxo0bmTt3LiqVih07dmjOl5WVsXv3btLS0nj66afp379/jefc3d05dOgQqampxMbGMmXKlEb3YlMUheLiYqZPn67Zi+1Pf/oTiqLQqVMnnn76acaPH6/ZJ++nn35iz5495ObmMm7cuGbLJZFotaXr4+NDVFQUU6dO5c6dOyiKQr9+/cjLyyMrK4vLly/Tr18/srKatpZ0fTvC6hJDlP+bb77hv/7rv0hOTqagoICdO3eSl5fHpk2bWLlypUnJUnVP1d/+/fvzzjvvEBPT/NU3q/+YPv7448THx/P1118zduxYQkJCNNeSkpJwdnau87yLiwsuLi4PzGfKlCk1/j9x4kQAAgICapx3dnZmwYIFACxfvrzJckgktdGq0bWwsMDMzEzzNzc3FwsLC37++WdA/SEUFhbi6uqqeaaxj2PgwIFERUXRq1cvKisriYuLw9vbm5CQEKysrJg7d642i2+Q8le5R27fVq+ls27dOrKzs5vdHTcGWVJSUggLC6OsrAxbW1vWrFlDWVkZjzzySKtkAZg6dWq9593d3XF3d292evW5S1JSUvj888+5cuUK4eHhfPLJJ+Tn55Ofn09ISEir3CUSSRUyesGEaE/RC9XRhbtkyZIlGnfJhg01N7cIDw/Hy8uLqKgotmzZwowZM9i0aRPZ2dmEhYXh5eXFhAkTmiSPRFIbGb0gMQl8fHzo1asXU6dOpXfv3vW6SxwcHMjLy2tSeg25SxISEujatSt9+/Zl5syZbNy4kdzcXMzNzTXukir/u0TSEgwavRAQEEBEROt2dNm9ezdHjx7lwIEDWipVy9CGLPHx8aSnp9OxY0fmzZunpZK1DG3IExoaSmFhIUOGDDEJd0m/fv2Iiori2Wef5datWwghKCsrY8KECVhaWmrVXSJpv7TKvbB161bKy8txc3OjU6dOnD59mmvXrhETE8OkSZPw9PTkypUrDBgwAHNzc+zt7blx4waVlZX4+vqyefNm1q5dS1BQED179sTZ2Zns7GxNmh4eHgCcP3+eb7/9VpPvq6++WiNGsiED0Rz3gjHIkpmZyfr16xk4cCCvvPJKk+Wpr0tuDPIAZGRksGvXrnoHBZvqXjAVpHtB0hRa5V4YPHgwpaWlqFQqVCoV5ubmZGRkoFKp6NOnD35+flhaWuLn50d6ejoAo0ePxsvLi5MnTwJw8eJFioqK6NKlC5mZmTXS1CfGIEv37t2JiYnRiuzGIE9WVhaRkZEsWbKk1fJIJG2FVrkX8vPzsbGxITU1FRsbGxwdHamsrKSiokITZ2plpd7CqsqHduTIEYqLi1m4cCFpaWm4urri4OBAUVERbm5uNdIcP348QKNxlkeOHCE5OZmPP/6YZ5991qRl2bBhA2VlZdjb27dYDmORRwjBxIkTmTx5MmfOnNHM6NI32naT9O/fn7fffhs3N7caM+Mkkqai1+iFd999F3d3dwYN0s8GrrqMXtC3LKDb6AVjkccY3T7wm5vE29ubnTt30qNHD/z9/TE3N29UHomkNnqNXpg9e7ZeP2pd0pZkAeOVx9jcJI8++ihRUVH07duXb775RldiS9owWje6wcHBFBVpb9uquLg4oqKiasxqKigoYPHixSxdupSbN2+ydOlSYmJi+PLLL7WWL2hPlt27d+PtXXMn9+vXrzN79mwOHz6sObdixQpiY2MBCAsLIyoqirS0tFbnD7qV5caNG0RHRzNv3jxyc3M5evQoGzZsIDQ0lMrKSoKDg+vM8Goq1V0a//rXv+jYsWOT3CR79+5l5MiRADXcJH/84x9rpFnFsGHD8PPz0xxVrdwqN4mTkxNnzpzRTAD54osvakRKSCRNRgjR5EN9uxDLli0T5eXlIjQ0VPzyyy8iJiZGLFmyRPz73/8Wq1atEiqVSixevFgIIcTixYvFvXv3hL+/v4iIiBAHDhwQVZw7d05ER0drjuLiYlGbgIAAIYQQS5Ys0Zw7dOiQOHfunLhz546IiooSkZGRIjw8XBw+fLjGs/fLaxSyVKVRnZMnT4pPP/1UCCHEnj17xOnTp8WmTZvEpUuXxMyZM0VkZKT44YcfHiiPMchy4MAB4eXlJQoKCsTdu3dFUFCQWLVqVaPP1CdPlSwtJS4uTnz//fetSqOlNFbf5CGPqqNFLd1nnnmGhIQECgsLsba2pry8nJ49e3L27Nl6jXrt7t2D+PXXX4mJiSElJaVJc/79/f0JCAioMfvIWGRpKhcuXOD48eOcOnWK0tJSnJ2dmT9/Pps3bzYJWZ577jn+/ve/c/v2bezs7Fi7dm2dJRe1SUMt9+a6SSIjI4mKimLPnj01zn/11VdMnjwZgP379xMTE6MZPLx16xZPPPGEVnt0kvZDi6IXPDw88PT0JDg4mOzsbHJycujWrRsVFRWae6ytrdm/fz8ZGRl1RsGraGjk287OTjMynJyc/MA5//v27eP69ev07NnT6GSBmhEWTzzxBBcuXGD48OEcOHCA0tJS3N3d2bRpk2YlKzc3N/bt20dERAR//etfjV6WLl268PXXX3P9+nXeeusttm3bRn5+vmbd2u3bt5OcnExSUlKz1klYvnw5a9asYcOGDcyZM4d9+/aRkZHBtGnTNPdURScEBATUGTB77rnngAfHRtdePe369etkZ2fTp08fAF588UXOnj1L165dKS0t5R//+Adjx45tshwSSXXk2gsmRHtbe+HMmTPk5OTwz3/+gw1CVAAACN5JREFUkxUrVrB9+3bMzc0xNzcnJyeHgIAAgoODiYiIYPHixUydOpWdO3cybNgw7t69y/z584HGjW7V2gvV12DYunUrJSUlJCQksGnTJlxcXPDz82P9+vUkJiZy9uxZvvnmG+bNm8ekSZMalUciqY1RL2Iuad/oo+XevXt3oqOjGTRokGY93arV627fvo2Liwt5eXnY2dlhY2PDyJEjGTlyJMHBwXh6eur2BUjaJM1q6ZraBohyY0rjpj55TLXVDrKlK2kazTK6EomukUZX0taRSztKJBKJHpE+XYlRYWNjk6Uoism5SkBddkOXQWL8SPeCxGRRFMUG+BbYKITYpcV0/YCpwAghRJm20pVIQBpdiQmjKEoU8AfAW5uOYEVRzIDPgEQhxCptpSuRgDS6EhNFUZTRwG5gsBAiRwfp9wD+DTwnhJAr20i0hhxIk5gciqJ0BeKA2bowuABCiJ+AV4B9iqI46CIPSftEtnQlJoWiXozjI+AHIYTOt6RQFGUb8DshxAxd5yVpH0ijKzEZFEWJAzKBscATQogSPeT5O+BfwEGgoxDiNV3nKWnbyJAxiSkxEnBE3dK1AXRudAE71BESC4CbeshP0saRPl2JSaAoihXgjLqhcB3Q186l+UAGYA08ej+yQSJpMdK9IDEJFEUxB/YAi4QQPxsg/57AemCWyc5TlhgF0uhKJBKJHpFdJYlEItEjciBN0mTa0hKSVZiKTI3JIDEtpHtB0mRMddnFtrCDiFw2su0g3QsSiUSiR6R7QWJUxMXFkZeXh5mZmWZz0pSUFD7//HOuXLlCeHg4GzZsoEePHjz66KMMGzaMN954Azs7O6ZMmcLgwYMNLEFN6pOnoKCAkJAQLCwsmD9/PmfOnCE9PZ2KigpWrVrFjh07yMzM5JFHHuGFF14wsAQSbSONrqTFBAcH4+DgwKVLlxg6dCiJiYnExsYSGBiIq6srPj4+rF69GicnJ2xtbZkzZw4AqampHDt2TJPOtGnTcHJyAiAtLa3O7rxubm64ubkRHh5OXl4e3bp1o7y8nLKyMq5cucLgwYPx8PBg06ZNxMTEGL08J06cwNvbG2dnZ+Lj41m0aBEVFRUsXLgQgJEjRxIWFsaQIUNaLIvEeJHuBUmr8PHxoVevXkydOpXevXujKAr9+vUjLy+PrKwsLl++jIODA3l5eU1KT720Ql0SEhLo2rUrffv2xd/fn4CAAI4dO8aQIUMoKCjg8OHD2NnZmYw81SkvL+eNN97A398fgP79+/POO+9w9erVVskiMU5kS1fSKiwsLDAzM9P8zc3NxcLCgp9/Vs9fcHFxobCwEFdXV80zLi4uuLi41JvewIEDiYqKolevXlRWVhIXF0e/fv2Iiori2Wef5datW5w+fZrr16/Ts2dPzMzMKC8v5+7du/z97383CXm8vb0JCQnBysqKuXPnMn/+fOzs7Dh16hS9evUiIiKCsrIyHnnkkVbL8//bu5+Qpv84juPP0vJrIJk7KEJmXroYdvCgHiQGHkIM+kNIJ9khskPNtDBP86IrmJuXOiV2CCIaSOQ5ymCKEBUsMGjTaDAHC3GDnLnZIdrvNyWb/75u8XrADvt+P/t8v2823nz5vN/7fiX3qHtBspYvlf611L0guUTLCyIiJlLSlT3R09Oz7TkGBgbo7e3l6dOnvH//nrt372Kz2YhGd+W+5n+1EzE9efIEp9PJ/fv3d+CMJBdpTVe25MGDB6ysrFBXV0dpaSmvXr3i8+fPeDwezp49i9VqZWZmhhMnTlBQUEBJSQnBYJBUKoXNZgMgkUjQ19dHZWUl1dXVRCKR9JzNzc0A+Hw+pqam0se9evUqhmEA0NfXRygU4uHDh1y6dCmjw8FiseRlTKdPn8bpdHLkyJEtfCuSD3SlK1ty6tQplpeXicVixGIxCgoKCIVCxGIxampqsNvtHDhwALvdzuzsLAAtLS2cO3eOly9fAvDhwwfi8TgWi4VwOJwxZzbm5+dxuVzcuvXrARL/73DI15gqKirweDxZj5f8oytd2ZKFhQUMw8Dv92MYBmVlZaRSKZLJJIWFv35WBw8eBP5rmxofH+f79+9cv36djx8/cvLkSQ4fPkw8Hqeuri5jztbWVgAaGxtpbGxcd/zV1VXa2to4f/48r1+/pri4OKPD4ejRo3kXE8C9e/f48eMHJSUlmz5/yQ/qXpCsbafSPzo6Sn19PbW1tTt8Vn+3W90LZsak7oV/h5KuZC1f2qvWUsuY5BKt6cqOcjgcxOPxbc/jcrkYGhri0aNH6W3BYBC32821a9f49u0bTqcTl8tFV1cXc3NzOJ1Orly5siv/5NqpuEZGRrh48WLGtkAgQEdHBy9evACgs7MTj8fD5OTkto8nuUdJVzbtzp07JJNJBgcHiUajDA8Pc/v2bd69e5ce87t9qqenh0QiQXd3Ny6XC6/Xmx7j8/nweDzp19LSUnpfOBzm5s2b+P3+9Lbjx49TVVVFOBymsLCQVCrF4uIiZWVlHDt2jN7eXqxWK6FQKGfjstlsVFdXZxy3pqaGjo6O9Pvy8vKMz8i/RYU02bQzZ87w/PlzFhcXKSoqYmVlhcrKSiYmJtaNXV1dXVfRz8af7llw4cIFiouL+fr1K/v376e/vz999y6fz8eXL19ob2/P2biy4XA4ALDb7TQ0NOzYvJIblHRl05qbm7FarTgcDiKRCNFolPLycpLJZHpMUVERjx8/JhQKravo/7ZRFb+iogK3201tbS3hcJjp6WksFgtv3rwhEAjQ399PPB7H7XZz6NAhPn36xI0bN7h8+TJ+v/+P90LY67jGx8d5+/YtY2NjNDQ0MD09TVNTE8+ePWN5eZn6+nq8Xi+RSGRPio6y+1RIk6zlS9FpLRXSJJdoTVdExERaXpCsGYYxv2/fvpx/iONahmHMb7QvH2LaKAbJL1peEBExkZYXRERMpKQrImIiJV0RERMp6YqImEhJV0TEREq6IiImUtIVETGRkq6IiImUdEVETKSkKyJiIiVdERETKemKiJhISVdExEQ/ARef5TY5jLEhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
>>>>>>> 4ef4e05 (Save lambda mart)
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "source": [
    "plot_tree(ensemble[0].tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to ranklib output"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://es-learn-to-rank.labs.o19s.com/RankyMcRankFace.jar\n",
      "Running java -jar /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t DCG@10 -tree 10 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/training.txt -save data/title_model.txt \n",
      "Delete model title: 404\n",
      "Created Model title [Status: 201]\n",
      "Model saved\n",
      "Every N rounds of ranklib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.692,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.73,\n",
       " 20.7506,\n",
       " 20.6507,\n",
       " 20.7237,\n",
       " 20.7237]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 4ef4e05 (Save lambda mart)
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=ftr_logger.logged,\n",
    "                  index='tmdb',\n",
    "                  trees=10,\n",
    "                  featureSet='movies',\n",
    "                  modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine queries we learned\n",
    "\n",
    "Try out some queries, look at the final model prediction `last_prediction` compare to the correct ordering `grade`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pure Pandas Implementation?\n",
    "\n",
    "Can we make it faster by vectorizing with pandas?\n",
    "\n",
    "Turns out Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "lambdas_per_query['last_prediction'] = 0.0\n",
    "lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "#TBD - How do generalize this?\n",
    "lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1) # * lambdas_per_query['discount']\n",
    "\n",
    "lambdas_per_query[['join_key', 'qid', 'display_rank', 'discount', 'grade', 'gain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise deltas\n",
    "\n",
    "Delta captures pair-wise difference of the ranking metric (ie DCG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each group paired with each other group\n",
    "swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "# changes[j][i] = changes[i][j] = (discount(i) - discount(j)) * (gain(rel[i]) - gain(rel[j]));\n",
    "swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise rhos\n",
    "\n",
    "Rho captures pair-wise difference of the current model's prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute lambdas\n",
    "\n",
    "For every row where grade_x > grade_y,  compute `delta*rho`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps['lambda'] = 0\n",
    "slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "swaps[['qid', 'display_rank_x', 'display_rank_y', 'delta', 'last_prediction_x', 'last_prediction_y', 'rho', 'lambda']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get per-key lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid  display_rank_x\n",
       "1    0                 211.781688\n",
       "     1                  46.938369\n",
       "     2                  30.637615\n",
       "     3                   5.888732\n",
       "     4                  43.177578\n",
       "                          ...    \n",
       "40   25                 -9.853045\n",
       "     26                 -9.911575\n",
       "     27                 -9.966853\n",
       "     28                -10.019174\n",
       "     29                -10.068796\n",
       "Name: lambda, Length: 1390, dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better minus worse\n",
    "lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "lambdas = lambdas_x - lambdas_y\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
=======
   "execution_count": 399,
>>>>>>> 4ef4e05 (Save lambda mart)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
<<<<<<< HEAD
       "      <th>qid</th>\n",
       "      <th>uid</th>\n",
=======
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "      <th>features</th>\n",
       "      <th>last_prediction</th>\n",
       "      <th>display_rank</th>\n",
       "      <th>discount</th>\n",
       "      <th>gain</th>\n",
       "      <th>train_dcg</th>\n",
       "      <th>dcg</th>\n",
       "      <th>lambda</th>\n",
       "      <th>weight</th>\n",
       "      <th>path</th>\n",
<<<<<<< HEAD
       "      <th>lambdas</th>\n",
=======
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
>>>>>>> 4ef4e05 (Save lambda mart)
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7555</td>\n",
       "      <td>rambo</td>\n",
       "      <td>7555</td>\n",
       "      <td>4</td>\n",
       "      <td>[11.657399, 10.083591]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>213.776822</td>\n",
       "      <td>106.888411</td>\n",
       "      <td>1010010</td>\n",
       "      <td>211.781688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1370</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1370</td>\n",
       "      <td>3</td>\n",
       "      <td>[9.456276, 13.265001]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>4.416508</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>48.010828</td>\n",
       "      <td>26.458003</td>\n",
       "      <td>1100100</td>\n",
       "      <td>49.390958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1369</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1369</td>\n",
       "      <td>3</td>\n",
       "      <td>[6.036743, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>31.382749</td>\n",
       "      <td>18.143963</td>\n",
       "      <td>1101000</td>\n",
       "      <td>33.090203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_13258</td>\n",
       "      <td>rambo</td>\n",
       "      <td>13258</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 6.869545]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430677</td>\n",
       "      <td>1.292030</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>6.460558</td>\n",
       "      <td>7.448315</td>\n",
       "      <td>1101000</td>\n",
       "      <td>10.106768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_1368</td>\n",
       "      <td>rambo</td>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 11.113943]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>5.802792</td>\n",
       "      <td>30.700871</td>\n",
       "      <td>30.552986</td>\n",
       "      <td>43.639845</td>\n",
       "      <td>21.819923</td>\n",
       "      <td>1101000</td>\n",
       "      <td>43.177578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>40</td>\n",
       "      <td>40_37079</td>\n",
       "      <td>star wars</td>\n",
       "      <td>37079</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.846078</td>\n",
       "      <td>4.923039</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>40</td>\n",
       "      <td>40_126757</td>\n",
       "      <td>star wars</td>\n",
       "      <td>126757</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.903461</td>\n",
       "      <td>4.951730</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>40</td>\n",
       "      <td>40_39797</td>\n",
       "      <td>star wars</td>\n",
       "      <td>39797</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.205847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-9.957655</td>\n",
       "      <td>4.978827</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>40</td>\n",
       "      <td>40_18112</td>\n",
       "      <td>star wars</td>\n",
       "      <td>18112</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.008949</td>\n",
       "      <td>5.004475</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>40</td>\n",
       "      <td>40_43052</td>\n",
       "      <td>star wars</td>\n",
       "      <td>43052</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.207651</td>\n",
       "      <td>30.120435</td>\n",
       "      <td>-10.057598</td>\n",
       "      <td>5.028799</td>\n",
       "      <td>1101000</td>\n",
       "      <td>0.568410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid        uid   keywords   docId  grade                features  \\\n",
       "0       1     1_7555      rambo    7555      4  [11.657399, 10.083591]   \n",
       "1       1     1_1370      rambo    1370      3   [9.456276, 13.265001]   \n",
       "2       1     1_1369      rambo    1369      3   [6.036743, 11.113943]   \n",
       "3       1    1_13258      rambo   13258      2         [0.0, 6.869545]   \n",
       "4       1     1_1368      rambo    1368      4        [0.0, 11.113943]   \n",
       "...   ...        ...        ...     ...    ...                     ...   \n",
       "1385   40   40_37079  star wars   37079      0              [0.0, 0.0]   \n",
       "1386   40  40_126757  star wars  126757      0              [0.0, 0.0]   \n",
       "1387   40   40_39797  star wars   39797      0              [0.0, 0.0]   \n",
       "1388   40   40_18112  star wars   18112      0              [0.0, 0.0]   \n",
       "1389   40   40_43052  star wars   43052      0              [0.0, 0.0]   \n",
       "\n",
       "      last_prediction  display_rank  discount       gain  train_dcg  \\\n",
       "0                   0             0  1.000000  15.000000  30.700871   \n",
       "1                   0             1  0.630930   4.416508  30.700871   \n",
       "2                   0             2  0.500000   3.500000  30.700871   \n",
       "3                   0             3  0.430677   1.292030  30.700871   \n",
       "4                   0             4  0.386853   5.802792  30.700871   \n",
       "...               ...           ...       ...        ...        ...   \n",
       "1385                0            25  0.210310   0.000000  30.207651   \n",
       "1386                0            26  0.208015   0.000000  30.207651   \n",
       "1387                0            27  0.205847   0.000000  30.207651   \n",
       "1388                0            28  0.203795   0.000000  30.207651   \n",
       "1389                0             9  0.289065   0.000000  30.207651   \n",
       "\n",
       "            dcg      lambda      weight     path     lambdas  \n",
       "0     30.552986  213.776822  106.888411  1010010  211.781688  \n",
       "1     30.552986   48.010828   26.458003  1100100   49.390958  \n",
       "2     30.552986   31.382749   18.143963  1101000   33.090203  \n",
       "3     30.552986    6.460558    7.448315  1101000   10.106768  \n",
       "4     30.552986   43.639845   21.819923  1101000   43.177578  \n",
       "...         ...         ...         ...      ...         ...  \n",
       "1385  30.120435   -9.846078    4.923039  1101000    0.000000  \n",
       "1386  30.120435   -9.903461    4.951730  1101000    0.000000  \n",
       "1387  30.120435   -9.957655    4.978827  1101000    0.000000  \n",
       "1388  30.120435  -10.008949    5.004475  1101000    0.000000  \n",
       "1389  30.120435  -10.057598    5.028799  1101000    0.568410  \n",
       "\n",
       "[1390 rows x 16 columns]"
      ]
     },
     "execution_count": 106,
=======
       "      <th rowspan=\"41\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>2_1366</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1366</td>\n",
       "      <td>4</td>\n",
       "      <td>[10.00459, 8.694555]</td>\n",
       "      <td>3.798236</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>2.485134</td>\n",
       "      <td>2.452373</td>\n",
       "      <td>1010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>2_152601</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>152601</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>0.386853</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.022374</td>\n",
       "      <td>0.022079</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>2_198001</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>198001</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.032957</td>\n",
       "      <td>0.032522</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>2_170430</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>170430</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.039345</td>\n",
       "      <td>0.038826</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>2_18206</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>18206</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>4</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>0.278943</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.043712</td>\n",
       "      <td>0.043136</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67</td>\n",
       "      <td>2_47059</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>47059</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.262650</td>\n",
       "      <td>0.525299</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>1.592257</td>\n",
       "      <td>0.861259</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68</td>\n",
       "      <td>2_18215</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>18215</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.055760</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>69</td>\n",
       "      <td>2_70808</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>70808</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>7</td>\n",
       "      <td>0.239812</td>\n",
       "      <td>0.479625</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>1.147813</td>\n",
       "      <td>0.645303</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>2_351862</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>351862</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>8</td>\n",
       "      <td>0.231378</td>\n",
       "      <td>0.462756</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>0.986005</td>\n",
       "      <td>0.566714</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71</td>\n",
       "      <td>2_154019</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>154019</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>9</td>\n",
       "      <td>0.224244</td>\n",
       "      <td>0.448488</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>0.849135</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>72</td>\n",
       "      <td>2_70</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>40</td>\n",
       "      <td>0.156438</td>\n",
       "      <td>0.312876</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.059459</td>\n",
       "      <td>0.058675</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>73</td>\n",
       "      <td>2_22777</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>22777</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>11</td>\n",
       "      <td>0.212746</td>\n",
       "      <td>0.212746</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.167689</td>\n",
       "      <td>0.113340</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74</td>\n",
       "      <td>2_43189</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>43189</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>12</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>0.208015</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.187613</td>\n",
       "      <td>0.123788</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>75</td>\n",
       "      <td>2_318</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>13</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.205381</td>\n",
       "      <td>0.133105</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>76</td>\n",
       "      <td>2_39829</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>39829</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>14</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.221361</td>\n",
       "      <td>0.141485</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>77</td>\n",
       "      <td>2_1578</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1578</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>15</td>\n",
       "      <td>0.196562</td>\n",
       "      <td>0.393123</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.064003</td>\n",
       "      <td>0.063159</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>2_25855</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>25855</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>16</td>\n",
       "      <td>0.193426</td>\n",
       "      <td>0.193426</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.249042</td>\n",
       "      <td>0.156001</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>79</td>\n",
       "      <td>2_42012</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>42012</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>17</td>\n",
       "      <td>0.190551</td>\n",
       "      <td>0.381103</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.065270</td>\n",
       "      <td>0.064410</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>80</td>\n",
       "      <td>2_32276</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>32276</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>18</td>\n",
       "      <td>0.187902</td>\n",
       "      <td>0.187902</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.272306</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>63</td>\n",
       "      <td>2_17819</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>17819</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>19</td>\n",
       "      <td>0.185449</td>\n",
       "      <td>0.185449</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.282634</td>\n",
       "      <td>0.173616</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>61</td>\n",
       "      <td>2_330459</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>330459</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>20</td>\n",
       "      <td>0.183169</td>\n",
       "      <td>0.183169</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.292234</td>\n",
       "      <td>0.178650</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>42</td>\n",
       "      <td>2_1374</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1374</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 9.808358]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>21</td>\n",
       "      <td>0.181043</td>\n",
       "      <td>1.448341</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.067276</td>\n",
       "      <td>0.066389</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>60</td>\n",
       "      <td>2_1893</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1893</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>22</td>\n",
       "      <td>0.179052</td>\n",
       "      <td>0.179052</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.309571</td>\n",
       "      <td>0.187741</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>43</td>\n",
       "      <td>2_1371</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1371</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 5.557296]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>23</td>\n",
       "      <td>0.177184</td>\n",
       "      <td>1.417471</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.068090</td>\n",
       "      <td>0.067192</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44</td>\n",
       "      <td>2_1246</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1246</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 9.540649]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>24</td>\n",
       "      <td>0.175425</td>\n",
       "      <td>1.403401</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.068461</td>\n",
       "      <td>0.067558</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45</td>\n",
       "      <td>2_1375</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1375</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 9.472043]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>25</td>\n",
       "      <td>0.173765</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.068811</td>\n",
       "      <td>0.067904</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46</td>\n",
       "      <td>2_1367</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>1367</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 8.262948]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>26</td>\n",
       "      <td>0.172195</td>\n",
       "      <td>1.377563</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.069142</td>\n",
       "      <td>0.068231</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>47</td>\n",
       "      <td>2_60375</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>60375</td>\n",
       "      <td>3</td>\n",
       "      <td>[8.115546, 7.819334]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170707</td>\n",
       "      <td>1.365658</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.069456</td>\n",
       "      <td>0.068540</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>48</td>\n",
       "      <td>2_110123</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>110123</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.8909245, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>28</td>\n",
       "      <td>0.169294</td>\n",
       "      <td>0.338588</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.069754</td>\n",
       "      <td>0.068834</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>49</td>\n",
       "      <td>2_36685</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>36685</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.8909245, 5.777752]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>29</td>\n",
       "      <td>0.167949</td>\n",
       "      <td>0.335898</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.070038</td>\n",
       "      <td>0.069114</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>50</td>\n",
       "      <td>2_17711</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>17711</td>\n",
       "      <td>1</td>\n",
       "      <td>[6.8265696, 8.381828]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>30</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.070308</td>\n",
       "      <td>0.069381</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>51</td>\n",
       "      <td>2_2153</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>2153</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>31</td>\n",
       "      <td>0.165443</td>\n",
       "      <td>0.165443</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.366880</td>\n",
       "      <td>0.217793</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>52</td>\n",
       "      <td>2_339403</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>339403</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>32</td>\n",
       "      <td>0.164272</td>\n",
       "      <td>0.164272</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.371809</td>\n",
       "      <td>0.220378</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>53</td>\n",
       "      <td>2_21501</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>21501</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>33</td>\n",
       "      <td>0.163151</td>\n",
       "      <td>0.163151</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.376529</td>\n",
       "      <td>0.222853</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>54</td>\n",
       "      <td>2_81182</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>81182</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>34</td>\n",
       "      <td>0.162077</td>\n",
       "      <td>0.162077</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.381054</td>\n",
       "      <td>0.225226</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>55</td>\n",
       "      <td>2_62414</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>62414</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>35</td>\n",
       "      <td>0.161045</td>\n",
       "      <td>0.161045</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.385399</td>\n",
       "      <td>0.227504</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>56</td>\n",
       "      <td>2_21989</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>21989</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>36</td>\n",
       "      <td>0.160053</td>\n",
       "      <td>0.160053</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.389575</td>\n",
       "      <td>0.229694</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>57</td>\n",
       "      <td>2_11</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>37</td>\n",
       "      <td>0.159099</td>\n",
       "      <td>0.159099</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.393593</td>\n",
       "      <td>0.231801</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>58</td>\n",
       "      <td>2_12180</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>12180</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>38</td>\n",
       "      <td>0.158180</td>\n",
       "      <td>0.158180</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.397464</td>\n",
       "      <td>0.233831</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>59</td>\n",
       "      <td>2_140607</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>140607</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>39</td>\n",
       "      <td>0.157293</td>\n",
       "      <td>0.157293</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.401196</td>\n",
       "      <td>0.235788</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>81</td>\n",
       "      <td>2_105536</td>\n",
       "      <td>2</td>\n",
       "      <td>rocky</td>\n",
       "      <td>105536</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>-0.517338</td>\n",
       "      <td>10</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>11.466327</td>\n",
       "      <td>-0.404799</td>\n",
       "      <td>0.237677</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       uid  qid keywords   docId  grade               features  \\\n",
       "qid                                                                           \n",
       "2   0      41    2_1366    2    rocky    1366      4   [10.00459, 8.694555]   \n",
       "    1      62  2_152601    2    rocky  152601      0             [0.0, 0.0]   \n",
       "    2      64  2_198001    2    rocky  198001      0             [0.0, 0.0]   \n",
       "    3      65  2_170430    2    rocky  170430      0             [0.0, 0.0]   \n",
       "    4      66   2_18206    2    rocky   18206      0             [0.0, 0.0]   \n",
       "    5      67   2_47059    2    rocky   47059      1             [0.0, 0.0]   \n",
       "    6      68   2_18215    2    rocky   18215      0             [0.0, 0.0]   \n",
       "    7      69   2_70808    2    rocky   70808      1             [0.0, 0.0]   \n",
       "    8      70  2_351862    2    rocky  351862      1             [0.0, 0.0]   \n",
       "    9      71  2_154019    2    rocky  154019      1             [0.0, 0.0]   \n",
       "    10     72      2_70    2    rocky      70      1             [0.0, 0.0]   \n",
       "    11     73   2_22777    2    rocky   22777      0             [0.0, 0.0]   \n",
       "    12     74   2_43189    2    rocky   43189      0             [0.0, 0.0]   \n",
       "    13     75     2_318    2    rocky     318      0             [0.0, 0.0]   \n",
       "    14     76   2_39829    2    rocky   39829      0             [0.0, 0.0]   \n",
       "    15     77    2_1578    2    rocky    1578      1             [0.0, 0.0]   \n",
       "    16     78   2_25855    2    rocky   25855      0             [0.0, 0.0]   \n",
       "    17     79   2_42012    2    rocky   42012      1             [0.0, 0.0]   \n",
       "    18     80   2_32276    2    rocky   32276      0             [0.0, 0.0]   \n",
       "    19     63   2_17819    2    rocky   17819      0             [0.0, 0.0]   \n",
       "    20     61  2_330459    2    rocky  330459      0             [0.0, 0.0]   \n",
       "    21     42    2_1374    2    rocky    1374      3   [8.115546, 9.808358]   \n",
       "    22     60    2_1893    2    rocky    1893      0             [0.0, 0.0]   \n",
       "    23     43    2_1371    2    rocky    1371      3   [8.115546, 5.557296]   \n",
       "    24     44    2_1246    2    rocky    1246      3   [8.115546, 9.540649]   \n",
       "    25     45    2_1375    2    rocky    1375      3   [8.115546, 9.472043]   \n",
       "    26     46    2_1367    2    rocky    1367      3   [8.115546, 8.262948]   \n",
       "    27     47   2_60375    2    rocky   60375      3   [8.115546, 7.819334]   \n",
       "    28     48  2_110123    2    rocky  110123      1       [5.8909245, 0.0]   \n",
       "    29     49   2_36685    2    rocky   36685      1  [5.8909245, 5.777752]   \n",
       "    30     50   2_17711    2    rocky   17711      1  [6.8265696, 8.381828]   \n",
       "    31     51    2_2153    2    rocky    2153      0             [0.0, 0.0]   \n",
       "    32     52  2_339403    2    rocky  339403      0             [0.0, 0.0]   \n",
       "    33     53   2_21501    2    rocky   21501      0             [0.0, 0.0]   \n",
       "    34     54   2_81182    2    rocky   81182      0             [0.0, 0.0]   \n",
       "    35     55   2_62414    2    rocky   62414      0             [0.0, 0.0]   \n",
       "    36     56   2_21989    2    rocky   21989      0             [0.0, 0.0]   \n",
       "    37     57      2_11    2    rocky      11      0             [0.0, 0.0]   \n",
       "    38     58   2_12180    2    rocky   12180      0             [0.0, 0.0]   \n",
       "    39     59  2_140607    2    rocky  140607      0             [0.0, 0.0]   \n",
       "    40     81  2_105536    2    rocky  105536      0             [0.0, 0.0]   \n",
       "\n",
       "        last_prediction  display_rank  discount      gain  train_dcg  \\\n",
       "qid                                                                    \n",
       "2   0          3.798236             0  0.500000  8.000000  11.466327   \n",
       "    1         -0.517338             1  0.386853  0.386853  11.466327   \n",
       "    2         -0.517338             2  0.333333  0.333333  11.466327   \n",
       "    3         -0.517338             3  0.301030  0.301030  11.466327   \n",
       "    4         -0.517338             4  0.278943  0.278943  11.466327   \n",
       "    5         -0.517338             5  0.262650  0.525299  11.466327   \n",
       "    6         -0.517338             6  0.250000  0.250000  11.466327   \n",
       "    7         -0.517338             7  0.239812  0.479625  11.466327   \n",
       "    8         -0.517338             8  0.231378  0.462756  11.466327   \n",
       "    9         -0.517338             9  0.224244  0.448488  11.466327   \n",
       "    10        -0.517338            40  0.156438  0.312876  11.466327   \n",
       "    11        -0.517338            11  0.212746  0.212746  11.466327   \n",
       "    12        -0.517338            12  0.208015  0.208015  11.466327   \n",
       "    13        -0.517338            13  0.203795  0.203795  11.466327   \n",
       "    14        -0.517338            14  0.200000  0.200000  11.466327   \n",
       "    15        -0.517338            15  0.196562  0.393123  11.466327   \n",
       "    16        -0.517338            16  0.193426  0.193426  11.466327   \n",
       "    17        -0.517338            17  0.190551  0.381103  11.466327   \n",
       "    18        -0.517338            18  0.187902  0.187902  11.466327   \n",
       "    19        -0.517338            19  0.185449  0.185449  11.466327   \n",
       "    20        -0.517338            20  0.183169  0.183169  11.466327   \n",
       "    21        -0.517338            21  0.181043  1.448341  11.466327   \n",
       "    22        -0.517338            22  0.179052  0.179052  11.466327   \n",
       "    23        -0.517338            23  0.177184  1.417471  11.466327   \n",
       "    24        -0.517338            24  0.175425  1.403401  11.466327   \n",
       "    25        -0.517338            25  0.173765  1.390123  11.466327   \n",
       "    26        -0.517338            26  0.172195  1.377563  11.466327   \n",
       "    27        -0.517338            27  0.170707  1.365658  11.466327   \n",
       "    28        -0.517338            28  0.169294  0.338588  11.466327   \n",
       "    29        -0.517338            29  0.167949  0.335898  11.466327   \n",
       "    30        -0.517338            30  0.166667  0.333333  11.466327   \n",
       "    31        -0.517338            31  0.165443  0.165443  11.466327   \n",
       "    32        -0.517338            32  0.164272  0.164272  11.466327   \n",
       "    33        -0.517338            33  0.163151  0.163151  11.466327   \n",
       "    34        -0.517338            34  0.162077  0.162077  11.466327   \n",
       "    35        -0.517338            35  0.161045  0.161045  11.466327   \n",
       "    36        -0.517338            36  0.160053  0.160053  11.466327   \n",
       "    37        -0.517338            37  0.159099  0.159099  11.466327   \n",
       "    38        -0.517338            38  0.158180  0.158180  11.466327   \n",
       "    39        -0.517338            39  0.157293  0.157293  11.466327   \n",
       "    40        -0.517338            10  0.218104  0.218104  11.466327   \n",
       "\n",
       "              dcg    lambda    weight     path  \n",
       "qid                                             \n",
       "2   0   11.466327  2.485134  2.452373  1010101  \n",
       "    1   11.466327 -0.022374  0.022079  1100000  \n",
       "    2   11.466327 -0.032957  0.032522  1100000  \n",
       "    3   11.466327 -0.039345  0.038826  1100000  \n",
       "    4   11.466327 -0.043712  0.043136  1100000  \n",
       "    5   11.466327  1.592257  0.861259  1100000  \n",
       "    6   11.466327 -0.055760  0.051946  1100000  \n",
       "    7   11.466327  1.147813  0.645303  1100000  \n",
       "    8   11.466327  0.986005  0.566714  1100000  \n",
       "    9   11.466327  0.849135  0.500237  1100000  \n",
       "    10  11.466327 -0.059459  0.058675  1100000  \n",
       "    11  11.466327 -0.167689  0.113340  1100000  \n",
       "    12  11.466327 -0.187613  0.123788  1100000  \n",
       "    13  11.466327 -0.205381  0.133105  1100000  \n",
       "    14  11.466327 -0.221361  0.141485  1100000  \n",
       "    15  11.466327 -0.064003  0.063159  1100000  \n",
       "    16  11.466327 -0.249042  0.156001  1100000  \n",
       "    17  11.466327 -0.065270  0.064410  1100000  \n",
       "    18  11.466327 -0.272306  0.168200  1100000  \n",
       "    19  11.466327 -0.282634  0.173616  1100000  \n",
       "    20  11.466327 -0.292234  0.178650  1100000  \n",
       "    21  11.466327 -0.067276  0.066389  1100000  \n",
       "    22  11.466327 -0.309571  0.187741  1100000  \n",
       "    23  11.466327 -0.068090  0.067192  1100000  \n",
       "    24  11.466327 -0.068461  0.067558  1100000  \n",
       "    25  11.466327 -0.068811  0.067904  1100000  \n",
       "    26  11.466327 -0.069142  0.068231  1100000  \n",
       "    27  11.466327 -0.069456  0.068540  1100000  \n",
       "    28  11.466327 -0.069754  0.068834  1100000  \n",
       "    29  11.466327 -0.070038  0.069114  1100000  \n",
       "    30  11.466327 -0.070308  0.069381  1100000  \n",
       "    31  11.466327 -0.366880  0.217793  1100000  \n",
       "    32  11.466327 -0.371809  0.220378  1100000  \n",
       "    33  11.466327 -0.376529  0.222853  1100000  \n",
       "    34  11.466327 -0.381054  0.225226  1100000  \n",
       "    35  11.466327 -0.385399  0.227504  1100000  \n",
       "    36  11.466327 -0.389575  0.229694  1100000  \n",
       "    37  11.466327 -0.393593  0.231801  1100000  \n",
       "    38  11.466327 -0.397464  0.233831  1100000  \n",
       "    39  11.466327 -0.401196  0.235788  1100000  \n",
       "    40  11.466327 -0.404799  0.237677  1100000  "
      ]
     },
     "execution_count": 399,
>>>>>>> 4ef4e05 (Save lambda mart)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
   "source": [
<<<<<<< HEAD
    "lambdas_per_query.merge(lambdas, left_on=['qid', 'display_rank'], right_on=['qid', 'display_rank_x'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambdas(lambdas_per_query):\n",
    "    lambdas_per_query = lambdas_per_query.sort_values(['qid', 'last_prediction'], ascending=[True, False], kind='stable')\n",
    "    lambdas_per_query['display_rank'] = lambdas_per_query.groupby('qid').cumcount()\n",
    "\n",
    "    #TBD - How do generalize this to any metric?\n",
    "    lambdas_per_query['discount'] = 1 / np.log2(2 + lambdas_per_query['display_rank'])\n",
    "    lambdas_per_query['gain'] = (2**lambdas_per_query['grade'] - 1)\n",
    "\n",
    "    # swaps dataframe holds each pair-wise swap computed (shrink columns for memory?)   \n",
    "    # Optimization of swaps = lambdas_per_query.merge(lambdas_per_query, on='qid', how='outer')\n",
    "    # to limit to just needed columns\n",
    "    to_swap = lambdas_per_query[['qid', 'display_rank', 'grade', 'last_prediction', 'discount', 'gain']]\n",
    "    #to_swap = lambdas_per_query\n",
    "    swaps = to_swap.merge(to_swap, on='qid', how='outer')\n",
    "\n",
    "    # delta - delta in DCG due to swap\n",
    "    swaps['delta'] = np.abs((swaps['discount_x'] - swaps['discount_y']) * (swaps['gain_x'] - swaps['gain_y']))\n",
    "    \n",
    "    # rho - based on current model prediction delta\n",
    "    swaps['rho'] = 1 / (1 + np.exp(swaps['last_prediction_x'] - swaps['last_prediction_y']))\n",
    "    swaps['weight'] = swaps['rho'] * (1.0 - swaps['rho']) * swaps['delta']\n",
    "\n",
    "    # Compute lambdas (the next model in ensemble's predictors) when grade_x > grade_y\n",
    "    swaps['lambda'] = 0\n",
    "    slice_x_better =swaps[swaps['grade_x'] > swaps['grade_y']]\n",
    "    swaps.loc[swaps['grade_x'] > swaps['grade_y'], 'lambda'] = slice_x_better['delta'] * slice_x_better['rho']\n",
    "    \n",
    "    # accumulate lambdas and add back to model\n",
    "    lambdas_x = swaps.groupby(['qid', 'display_rank_x'])['lambda'].sum().rename('lambda')\n",
    "    lambdas_y = swaps.groupby(['qid', 'display_rank_y'])['lambda'].sum().rename('lambda')\n",
    "\n",
    "    weights_x = swaps.groupby(['qid', 'display_rank_x'])['weight'].sum().rename('weight')\n",
    "    weights_y = swaps.groupby(['qid', 'display_rank_y'])['weight'].sum().rename('weight')\n",
    "    \n",
    "    weights = weights_x + weights_y\n",
    "    lambdas = lambdas_x - lambdas_y\n",
    "\n",
    "    lambdas_per_query = lambdas_per_query.merge(lambdas, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "    lambdas_per_query = lambdas_per_query.merge(weights, \n",
    "                                                left_on=['qid', 'display_rank'], \n",
    "                                                right_on=['qid', 'display_rank_x'], \n",
    "                                                how='left')\n",
    "\n",
    "    return lambdas_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "grade                 4\n",
      "last_prediction    0.01\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.193092700772873\n",
      "----------\n",
      "round 1\n",
      "grade                     4\n",
      "last_prediction    0.019915\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 2\n",
      "grade                     4\n",
      "last_prediction    0.029745\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 3\n",
      "grade                     4\n",
      "last_prediction    0.039492\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 4\n",
      "grade                     4\n",
      "last_prediction    0.049159\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 5\n",
      "grade                     4\n",
      "last_prediction    0.058748\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 6\n",
      "grade                    4\n",
      "last_prediction    0.06826\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 7\n",
      "grade                     4\n",
      "last_prediction    0.077698\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 8\n",
      "grade                     4\n",
      "last_prediction    0.087063\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 9\n",
      "grade                     4\n",
      "last_prediction    0.096357\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 10\n",
      "grade                     4\n",
      "last_prediction    0.105582\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 11\n",
      "grade                     4\n",
      "last_prediction    0.114739\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 12\n",
      "grade                    4\n",
      "last_prediction    0.12383\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 13\n",
      "grade                     4\n",
      "last_prediction    0.132857\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 14\n",
      "grade                    4\n",
      "last_prediction    0.14182\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 15\n",
      "grade                     4\n",
      "last_prediction    0.150722\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 16\n",
      "grade                     4\n",
      "last_prediction    0.159564\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 17\n",
      "grade                     4\n",
      "last_prediction    0.168347\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 18\n",
      "grade                     4\n",
      "last_prediction    0.177072\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 19\n",
      "grade                     4\n",
      "last_prediction    0.185742\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 20\n",
      "grade                     4\n",
      "last_prediction    0.194356\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 21\n",
      "grade                     4\n",
      "last_prediction    0.202916\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 22\n",
      "grade                     4\n",
      "last_prediction    0.211424\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 23\n",
      "grade                    4\n",
      "last_prediction    0.21988\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 24\n",
      "grade                     4\n",
      "last_prediction    0.228285\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 25\n",
      "grade                     4\n",
      "last_prediction    0.236641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 26\n",
      "grade                     4\n",
      "last_prediction    0.244949\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 27\n",
      "grade                     4\n",
      "last_prediction    0.253209\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 28\n",
      "grade                     4\n",
      "last_prediction    0.261423\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 29\n",
      "grade                     4\n",
      "last_prediction    0.269591\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 30\n",
      "grade                     4\n",
      "last_prediction    0.277715\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 31\n",
      "grade                     4\n",
      "last_prediction    0.285795\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 32\n",
      "grade                     4\n",
      "last_prediction    0.293832\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 33\n",
      "grade                     4\n",
      "last_prediction    0.301828\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 34\n",
      "grade                     4\n",
      "last_prediction    0.309782\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 35\n",
      "grade                     4\n",
      "last_prediction    0.317695\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 36\n",
      "grade                     4\n",
      "last_prediction    0.325569\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 37\n",
      "grade                     4\n",
      "last_prediction    0.333405\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 38\n",
      "grade                     4\n",
      "last_prediction    0.341202\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 39\n",
      "grade                     4\n",
      "last_prediction    0.348962\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 40\n",
      "grade                     4\n",
      "last_prediction    0.356685\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 41\n",
      "grade                     4\n",
      "last_prediction    0.364372\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 42\n",
      "grade                     4\n",
      "last_prediction    0.372024\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 43\n",
      "grade                     4\n",
      "last_prediction    0.379641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 44\n",
      "grade                     4\n",
      "last_prediction    0.387224\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 45\n",
      "grade                     4\n",
      "last_prediction    0.394774\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 46\n",
      "grade                     4\n",
      "last_prediction    0.402291\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 47\n",
      "grade                     4\n",
      "last_prediction    0.409776\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 48\n",
      "grade                     4\n",
      "last_prediction    0.417229\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 49\n",
      "grade                     4\n",
      "last_prediction    0.424651\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ensemble=[]\n",
    "def lambda_mart_pure(judgments, rounds=20, learning_rate=0.1, max_leaf_nodes=8, metric=dcg):\n",
    "\n",
    "    print(judgments.columns)\n",
    "    # Convert to Pandas Dataframe\n",
    "    lambdas_per_query = judgments.copy()\n",
    "\n",
    "\n",
    "    lambdas_per_query['last_prediction'] = 0.0\n",
    "\n",
    "    for i in range(0, rounds):\n",
    "        print(f\"round {i}\")\n",
    "\n",
    "        # ------------------\n",
    "        #1. Build pair-wise predictors for this round\n",
    "        lambdas_per_query = compute_lambdas(lambdas_per_query)\n",
    "\n",
    "        # ------------------\n",
    "        #2. Train a regression tree on this round's lambdas\n",
    "        features = lambdas_per_query['features'].tolist()\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "        tree.fit(features, lambdas_per_query['lambda'])    \n",
    "\n",
    "        # ------------------\n",
    "        #3. Reweight based on LambdaMART's weighted average\n",
    "        # Add each tree's paths\n",
    "        lambdas_per_query['path'] = tree_paths(tree, features)\n",
    "        predictions = lambdas_per_query.groupby('path')['lambda'].sum() / lambdas_per_query.groupby('path')['weight'].sum()\n",
    "        predictions = predictions.fillna(0.0) # for divide by 0\n",
    "\n",
    "        # -------------------\n",
    "        #4. Add to ensemble, recreate last prediction\n",
    "        new_tree = OverridenRegressionTree(predictions=predictions, tree=tree)\n",
    "        ensemble.append(new_tree)\n",
    "        next_predictions = new_tree.predict(features)\n",
    "        lambdas_per_query['last_prediction'] += (next_predictions * learning_rate) \n",
    "        \n",
    "        print(lambdas_per_query.loc[0, ['grade', 'last_prediction']])\n",
    "        \n",
    "        print(\"Train DCGs\")\n",
    "        lambdas_per_query['discounted_gain'] = lambdas_per_query['gain'] * lambdas_per_query['discount'] \n",
    "        dcg = lambdas_per_query[lambdas_per_query['display_rank'] < 10].groupby('qid')['discounted_gain'].sum().mean()\n",
    "        print(\"mean   \", dcg)\n",
    "        print(\"----------\")\n",
    "        \n",
    "        lambdas_per_query = lambdas_per_query.drop(['lambda', 'weight'], axis=1)\n",
    "    return lambdas_per_query\n",
    "\n",
    "\n",
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'qid', 'keywords', 'docId', 'grade', 'features'], dtype='object')\n",
      "round 0\n",
      "grade                 4\n",
      "last_prediction    0.01\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.193092700772873\n",
      "----------\n",
      "round 1\n",
      "grade                     4\n",
      "last_prediction    0.019915\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 2\n",
      "grade                     4\n",
      "last_prediction    0.029745\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 3\n",
      "grade                     4\n",
      "last_prediction    0.039492\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 4\n",
      "grade                     4\n",
      "last_prediction    0.049159\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 5\n",
      "grade                     4\n",
      "last_prediction    0.058748\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 6\n",
      "grade                    4\n",
      "last_prediction    0.06826\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 7\n",
      "grade                     4\n",
      "last_prediction    0.077698\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.709013438589473\n",
      "----------\n",
      "round 8\n",
      "grade                     4\n",
      "last_prediction    0.087063\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 9\n",
      "grade                     4\n",
      "last_prediction    0.096357\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 10\n",
      "grade                     4\n",
      "last_prediction    0.105582\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 11\n",
      "grade                     4\n",
      "last_prediction    0.114739\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 12\n",
      "grade                    4\n",
      "last_prediction    0.12383\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 13\n",
      "grade                     4\n",
      "last_prediction    0.132857\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 14\n",
      "grade                    4\n",
      "last_prediction    0.14182\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 15\n",
      "grade                     4\n",
      "last_prediction    0.150722\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 16\n",
      "grade                     4\n",
      "last_prediction    0.159564\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 17\n",
      "grade                     4\n",
      "last_prediction    0.168347\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 18\n",
      "grade                     4\n",
      "last_prediction    0.177072\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 19\n",
      "grade                     4\n",
      "last_prediction    0.185742\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 20\n",
      "grade                     4\n",
      "last_prediction    0.194356\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 21\n",
      "grade                     4\n",
      "last_prediction    0.202916\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 22\n",
      "grade                     4\n",
      "last_prediction    0.211424\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 23\n",
      "grade                    4\n",
      "last_prediction    0.21988\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 24\n",
      "grade                     4\n",
      "last_prediction    0.228285\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 25\n",
      "grade                     4\n",
      "last_prediction    0.236641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 26\n",
      "grade                     4\n",
      "last_prediction    0.244949\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 27\n",
      "grade                     4\n",
      "last_prediction    0.253209\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 28\n",
      "grade                     4\n",
      "last_prediction    0.261423\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 29\n",
      "grade                     4\n",
      "last_prediction    0.269591\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 30\n",
      "grade                     4\n",
      "last_prediction    0.277715\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 31\n",
      "grade                     4\n",
      "last_prediction    0.285795\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 32\n",
      "grade                     4\n",
      "last_prediction    0.293832\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 33\n",
      "grade                     4\n",
      "last_prediction    0.301828\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 34\n",
      "grade                     4\n",
      "last_prediction    0.309782\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 35\n",
      "grade                     4\n",
      "last_prediction    0.317695\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 36\n",
      "grade                     4\n",
      "last_prediction    0.325569\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 37\n",
      "grade                     4\n",
      "last_prediction    0.333405\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 38\n",
      "grade                     4\n",
      "last_prediction    0.341202\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 39\n",
      "grade                     4\n",
      "last_prediction    0.348962\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 40\n",
      "grade                     4\n",
      "last_prediction    0.356685\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 41\n",
      "grade                     4\n",
      "last_prediction    0.364372\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 42\n",
      "grade                     4\n",
      "last_prediction    0.372024\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 43\n",
      "grade                     4\n",
      "last_prediction    0.379641\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 44\n",
      "grade                     4\n",
      "last_prediction    0.387224\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 45\n",
      "grade                     4\n",
      "last_prediction    0.394774\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 46\n",
      "grade                     4\n",
      "last_prediction    0.402291\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 47\n",
      "grade                     4\n",
      "last_prediction    0.409776\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 48\n",
      "grade                     4\n",
      "last_prediction    0.417229\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      "round 49\n",
      "grade                     4\n",
      "last_prediction    0.424651\n",
      "Name: 0, dtype: object\n",
      "Train DCGs\n",
      "mean    20.692041359674654\n",
      "----------\n",
      " "
     ]
    }
   ],
   "source": [
    "judgments = to_dataframe(ftr_logger.logged)\n",
    "%prun -s cumtime lambdas_per_query = lambda_mart_pure(judgments=judgments, rounds=50, max_leaf_nodes=10, learning_rate=0.01, metric=dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/RankyMcRankFace.jar already exists\n",
      "Running java -jar /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t DCG@10 -tree 10 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/bz/schh49y93yg6323ynrz7j8j00000gn/T/training.txt -save data/title_model.txt \n",
      "Delete model title: 200\n",
      "Created Model title [Status: 201]\n",
      "Model saved\n",
      " Every N rounds of ranklib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.692,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.669,\n",
       " 20.73,\n",
       " 20.7506,\n",
       " 20.6507,\n",
       " 20.7237,\n",
       " 20.7237]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltr.ranklib import train\n",
    "%prun -s cumtime trainLog  = train(client, training_set=ftr_logger.logged, index='tmdb', trees=10, featureSet='movies', modelName='title')\n",
    "\n",
    "print(\"Every N rounds of ranklib\")\n",
    "trainLog.trainingLogs[0].rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "lambdas_per_query[lambdas_per_query['qid'] == 2]"
   ]
<<<<<<< HEAD
>>>>>>> 4ef4e05 (Save lambda mart)
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 12bbee8 (Fix a few minor bugs / add back deleted cells)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
