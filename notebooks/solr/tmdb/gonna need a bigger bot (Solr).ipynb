{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're Gonna Need a Bigger Bot\n",
    "\n",
    "Let's map all the bits and pieces to a meatier example, to see how hello-ltr's abstractions make it easier to play with LTR via ipynbs\n",
    "\n",
    "Genome-tags is a crowdsourced movie tagging resource project. Each movie is assigned from 0-1 how close a movie matches a tag. Luckily the tags look remarkably like search queries (ie `star trek` or `berlin`). We derrived judgments from the genome-tags data, and use them to experiment with search.\n",
    "\n",
    "BUT\n",
    "\n",
    "While some tags are straight forward (`Star Trek`) others are much tougher (`boxing` or `french art movie`) where its unlikely any text has a match. \n",
    "\n",
    "We're not going to solve those problems here, but we give this to you as a sandbox to apply your skills after the class to see how close you can get to approximating the genome tags data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clients \n",
    "\n",
    "While syntaxes differ, the LTR process is nearly identical between Solr and Elastic. So you can repeat a lot of the labs with Solr (some have been already translated)\n",
    "\n",
    "But we'll stick with Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import SolrClient\n",
    "client = SolrClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import download\n",
    "\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/genome_judgments.txt'\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "download([corpus, judgments]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindex if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Sets in ipynb\n",
    "\n",
    "You played with creating a feature set in the last lab, see the same process repeated here.\n",
    "\n",
    "Learning to rank requires creating feature set. Each feature has a name like `title_bm25` and as part of a list an ordinal `title_bm25` is the 0th item. Confusingly, Ranklib uses 1-based feature numbering, so feature 0 in this list corresponds to feature 1 in Ranklib training file, that we'll see soon.\n",
    "\n",
    "Notice also:\n",
    "\n",
    "- Each feature is a templated query with `{{keywords}}` parameter, that is passed at query time\n",
    "- We've added a `validation` block, which will run these queries with the specified parameters and index and return any query errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = [\n",
    "    {\n",
    "        \"store\": \"genome\",\n",
    "        \"name\" : \"title_bm25\",\n",
    "        \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "        \"params\" : {\n",
    "          \"q\" : \"title:(${keywords})\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"store\": \"genome\",\n",
    "        \"name\" : \"overview_bm25\",\n",
    "        \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "        \"params\" : {\n",
    "          \"q\" : \"overview:(${keywords})\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "client.create_featureset(index='tmdb', name='genome', ftr_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Queries\n",
    "\n",
    "Logging is one of the more complex operations from an engineering perspective. \n",
    "\n",
    "The same query you ran manually when reviewing the slides is rerun here for every query in the source judgment list `judgmentInFile` with some batching when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.judgments import judgments_open\n",
    "from ltr.log import FeatureLogger\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "\n",
    "ftr_logger=FeatureLogger(client, index='tmdb', feature_set='genome')\n",
    "with judgments_open('data/genome_judgments.txt') as judg_list:\n",
    "    # For each query's judgments log features, and add to our training set...\n",
    "    for qid, query_judgments in groupby(judg_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments,\n",
    "                               qid=qid, \n",
    "                               keywords=judg_list.keywords(qid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here's where we train the model, under the hood this executes Ranklib just as you ran during the training exercises.\n",
    "\n",
    "Notice here we're optimizing for NDCG@10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainResponse = train(client,\n",
    "                 training_set=ftr_logger.logged,\n",
    "                 metric2t='NDCG@10',\n",
    "                 featureSet='genome',\n",
    "                 index='tmdb',\n",
    "                 modelName='genome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is done, we can output some statistics about the model, including the training metrics. In future units we'll get more into what this looks like.\n",
    "\n",
    "Notice the training NDCG isn't that great. When originally run, it was only 0.5885. So pretty far off of the genome data. One challenge of Learning to Rank (and Relevance in general) is trying to figure out the features that can close the gap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Impact of each feature on the model\")\n",
    "trainLog = trainResponse.trainingLogs[0]\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(client.get_feature_name(config, ftrId), impact))\n",
    "    \n",
    "print(\"trainLog Metric %s\" % trainLog.metric())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with our model\n",
    "\n",
    "Here we're going to search using the `genome` model. You can see the LTR query being output (sent to Elasticsearch). You're encouraged to run that directly against Elasticsearch if you like.\n",
    "\n",
    "Please note, this isn't rescoring. And that's fine for our purposes of directly evaluating the model, in real life you really should run a rescore query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import search\n",
    "search(client, \"brad pitt\", modelName='genome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
