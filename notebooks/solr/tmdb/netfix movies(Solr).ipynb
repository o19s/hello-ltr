{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solr Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import SolrClient\n",
    "client = SolrClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download & Build Index (run once)\n",
    "\n",
    "If you don't already have the downloaded dependencies; if you don't have TheMovieDB data indexed run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import download\n",
    "\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/title_judgments.txt'\n",
    "download([corpus, judgments], dest='data/');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for movie titles\n",
    "\n",
    "We'll be searching movie titles (think searching for a specific movie on Netflix). And we have a set of judgments around the appropriatte movie to return. IE search for \"Star Wars\" return good star wars matches, in quality order...\n",
    "\n",
    "These cover various aspects of the problem (searching title by phrase, title bm25 score, release date, etc). We'll use this to explore and analyze a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = [\n",
    "    #1\n",
    "    {\n",
    "      \"name\" : \"title_has_phrase\",\n",
    "      \"store\": \"title\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"title:\\\"${keywords})\\\"^=1\"\n",
    "      }\n",
    "    },\n",
    "    #2\n",
    "    {\n",
    "      \"name\" : \"title_has_terms\",\n",
    "      \"store\": \"title\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"title:(${keywords})^=1\"\n",
    "      }\n",
    "    },\n",
    "    #3\n",
    "    {\n",
    "      \"name\" : \"title_bm25\",\n",
    "      \"store\": \"title\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"title:(${keywords})\"\n",
    "      }\n",
    "    },\n",
    "    #4\n",
    "    {\n",
    "      \"name\" : \"overview_bm25\",\n",
    "      \"store\": \"title\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"overview:(${keywords})\"\n",
    "      }\n",
    "    },\n",
    "    #5\n",
    "    {\n",
    "      \"name\" : \"overview_phrase_bm25\",\n",
    "      \"store\": \"title\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"overview:\\\"${keywords}\\\"\"\n",
    "      }\n",
    "    },\n",
    "    #6\n",
    "    {\n",
    "      \"name\" : \"title_fuzzy\",\n",
    "      \"store\": \"title\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"{!lucene df=title}${keywords}~\"\n",
    "      }\n",
    "    },\n",
    "    #7\n",
    "    {\n",
    "      \"name\" : \"release_year\",\n",
    "      \"store\": \"title\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"{!func}def(release_year,2000)\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "client.create_featureset(index='tmdb', name='title', ftr_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Generation\n",
    "\n",
    "Log out features for each of the above queries out to a training set file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.judgments import judgments_from_file\n",
    "from ltr.log import log_query\n",
    "from itertools import groupby\n",
    "\n",
    "training_set=[]\n",
    "with open('data/title_judgments.txt') as judgF:\n",
    "    judgments=judgments_from_file(judgF)\n",
    "    # For each query's judgments log features, and add to our training set...\n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        query_training_set, discarded = log_query(client, index='tmdb', \n",
    "                                                  judgments=query_judgments, feature_set='title')\n",
    "        training_set.extend(query_training_set)\n",
    "        print(\"Got %s Training Samples\" % len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Search: which features work best?\n",
    "\n",
    "What combination of these features work best? Train a model with every combination, and use k-fold cross valudation (see `kcv=15` below). The combination with the best NDCG is output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import feature_search\n",
    "rankLibResult, ndcgPerFeature = feature_search(client,\n",
    "                                               training_set=training_set,\n",
    "                                               metric2t='NDCG@10',\n",
    "                                               leafs=20,\n",
    "                                               trees=20,\n",
    "                                               kcv=15,\n",
    "                                               features=[1,2,7],\n",
    "                                               featureSet='title')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "trainLogs = rankLibResult.trainingLogs\n",
    "for ftrId, impact in trainLogs[-1].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLogs[-1].rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Avg NDCG@10 when feature included:\")\n",
    "for ftrId, ndcg in ndcgPerFeature.items():\n",
    "    print(\"%s => %s\" % (ftrId, ndcg))\n",
    "    \n",
    "print(\"Avg K-Fold NDCG@10 %s\" % rankLibResult.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to model w/ all features\n",
    "\n",
    "Compare the features output above (something like...)\n",
    "\n",
    "```\n",
    "Impact of each feature on the model\n",
    "7 - 17618.35445148437\n",
    "4 - 16165.586045512271\n",
    "3 - 10958.610341321868\n",
    "5 - 9256.821192289186\n",
    "1 - 1436.0640878600943\n",
    "```\n",
    "\n",
    "to one trained with the full model. Notice how features have different impacts. This is due to feature dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=training_set,\n",
    "                  metric2t='NDCG@10',\n",
    "                  leafs=20,\n",
    "                  trees=20,\n",
    "                  features=[1,2,3,4,5,6,7],\n",
    "                  featureSet='title',\n",
    "                  index='tmdb',\n",
    "                  modelName='title')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLog.rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Train NDCG@10 %s\" % trainLog.rounds[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias towards fewer features\n",
    "\n",
    "By adding a 'cost', to feature search, we add a multiplier that punishes models with more features slightly. This results in a tiny bias towards simpler models all things being equal. As we'd prefer one that doesn't need to execute more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import feature_search\n",
    "rankLibResult, ndcgPerFeature = feature_search(client,\n",
    "                                               training_set=training_set,\n",
    "                                               metric2t='NDCG@10',\n",
    "                                               leafs=20,\n",
    "                                               trees=20,\n",
    "                                               kcv=15,\n",
    "                                               featureCost=0.1,# adjustedNDCG = NDCG * ( (1.0-cost) ^ num_features)\n",
    "                                               features=[1,2,3,4,5,6,7],\n",
    "                                               featureSet='title')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "trainLogs = rankLibResult.trainingLogs\n",
    "for ftrId, impact in trainLogs[-1].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLogs[-1].rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Avg NDCG@10 when feature included:\")\n",
    "for ftrId, ndcg in ndcgPerFeature.items():\n",
    "    print(\"%s => %s\" % (ftrId, ndcg))\n",
    "    \n",
    "print(\"Avg K-Fold NDCG@10 %s\" % rankLibResult.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "It's interesting to see what features our model makes use of, but we need guidance on adding additional features to the model. We know our model is an ensemble of decision trees. Wouldn't it be cool if we could trace where documents end up on that decision tree?\n",
    "\n",
    "Specifically, we care about problems. Or what we will call affectionately *whoopsies*. \n",
    "\n",
    "As a 'whoopsie' example, consider the query \"Rambo\". if a '0' document like 'First Daughter' ranked the same or higher than a '4' document (\"Rambo\")., that's a problem. It's also an opportunity for improvement. We'd want to isolate that, see if it's indicative of a broader trend, and thus worth adding a feature for.\n",
    "\n",
    "Let's see a concrete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.MART_model import eval_model\n",
    "from ltr.judgments import judgments_from_file\n",
    "from itertools import groupby\n",
    "\n",
    "features, _ = client.feature_set(index='tmdb', name='title')\n",
    "\n",
    "for qid, query_judgments in groupby(training_set, key=lambda j: j.qid):\n",
    "    print(qid)\n",
    "    if qid == 1: # Rambo\n",
    "        model = eval_model(modelName='title',\n",
    "                               features=features,\n",
    "                               judgments=query_judgments)\n",
    "\n",
    "        print()\n",
    "        print(\"## Evaluating graded docs for search keywords 'Rambo'\")\n",
    "        print()\n",
    "        print(model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining our evaluation for whoopsies\n",
    "\n",
    "Let's looks at one tree in our ensemble, te see how it was evaluated.\n",
    "\n",
    "```\n",
    "if title_bm25 > 10.664251:\n",
    "  if title_phrase > 0.0:\n",
    "    if title_bm25 > 13.815164:\n",
    "      if release_year > 2000.0:\n",
    "        <= 0.1215(0/0/)\n",
    "      else:\n",
    "        <= 0.1240(0/0/)\n",
    "    else:\n",
    "      if title_bm25 > 10.667803:\n",
    "        if overview_bm25 > 0.0:\n",
    "          <= 0.1194(0/0/)\n",
    "        else:\n",
    "          <= 0.1161(1/0/)\n",
    "      else:\n",
    "        <= 0.1264(0/0/)\n",
    "  else:\n",
    "    <= 0.0800(0/0/)\n",
    "else:\n",
    "  if title_phrase > 0.0:\n",
    "    if title_bm25 > 8.115499:\n",
    "      if title_bm25 > 8.217656:\n",
    "        <= 0.1097(2/1/qid:40:2(12180)-3(140607))\n",
    "      else:\n",
    "        <= 0.1559(0/0/)\n",
    "    else:\n",
    "      <= -0.0021(2/1/qid:40:2(1895)-3(330459))\n",
    "  else:\n",
    "    <= -0.1093(25/1/qid:40:0(85783)-3(1892))\n",
    "```\n",
    "\n",
    "You'll notice here this tree is represented by a series of if statements, where the feature's name is used. This is handy as it lets us take apart the structure of the tree.\n",
    "\n",
    "You'll also notice the leaf nodes starting with \n",
    "\n",
    "```\n",
    "<=\n",
    "```\n",
    "\n",
    "These leaf nodes have a floating point value, corresponding to the relevance score that documents ending up here will have. Each leaf also has three items in paranthesis, such as `(2/1/qid:40:2(1895)-3(330459))`. This is a report summarizing the result of evaluating the tree on the provided judgment list. Indicating:\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "   +--- 2 Documents evaluated to this leaf node                   +-- max grade doc eval'd to this leaf\n",
    "   |                                                              |\n",
    "   | +----- 1 'whoopsie' occured                                  |  +-- corresp. doc id of max doc\n",
    "   | |                                                            |  |\n",
    "   | |   +--- details on each whoopsie ----------- qid:40:2(1985)-3(330459)\n",
    "   | |   |                                              | |  |\n",
    "  (2/1/qid:40:2(1895)-3(330459))                        | |  |\n",
    "                                                        | |  + doc id of min graded doc\n",
    "                                                        | |\n",
    "                                                        | + min grade of docs eval'd to this leaf\n",
    "                                                        |\n",
    "                                                        + query id of whoopsie from judgments\n",
    "```\n",
    "\n",
    "\n",
    "Looking at Star Wars, our biggest issues in this tree are with the bottom-most leaf. Here\n",
    "\n",
    "```\n",
    "if title_bm25 > 10.664251:\n",
    "  ...\n",
    "else:\n",
    "  if title_phrase > 0.0:\n",
    "    ...\n",
    "  else:\n",
    "    <= -0.1093(25/1/qid:40:0(85783)-3(1892))\n",
    "```\n",
    "\n",
    "\n",
    "Document 85783 (a '0') and doc 1892 are given the same grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whoopsie, from the query perspective\n",
    "\n",
    "Whoopsies can also be examined at the \"query\" level to see for a query id, how many whoopsies existed, and what was the evaluation for that query at each tree. This can help see if an error was fixed later in the ensemble of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whoopsies = model.whoopsies()\n",
    "for qid, whoopsie in whoopsies.items():\n",
    "    print(\"== QID %s ==\" % qid)\n",
    "    print(\"%s - %s\" % (whoopsie.count, whoopsie.totalMagnitude))\n",
    "    print(whoopsie.perTreeReport())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine problem doc 319074\n",
    "\n",
    "(notice nothing mentions 'star wars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client.get_doc(index='tmdb', doc_id=1368)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a feature: collection name\n",
    "\n",
    "We have an intuition about our data, there is a field for the movies \"collection name\". See it here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.helpers.movies import get_movie\n",
    "get_movie(1368)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now reindex with collection name...\n",
    "\n",
    "We'll add collection name, and reindex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "def add_collection_and_char_name(src_movie, base_doc):\n",
    "    if 'belongs_to_collection' in src_movie and src_movie['belongs_to_collection'] is not None:\n",
    "        if 'name' in src_movie['belongs_to_collection']:\n",
    "            base_doc['collection_name_en'] = src_movie['belongs_to_collection']['name']\n",
    "            \n",
    "    if 'cast' in src_movie and src_movie['cast'] is not None:\n",
    "        characters = [cast['character'] for cast in get_movie(1368)['cast']][:5]\n",
    "        base_doc['top_cast_en'] = characters\n",
    "    return base_doc\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json', enrich=add_collection_and_char_name)\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm it's in our doc now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_doc(index='tmdb', doc_id=319074)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add it to the features, and regenerate training data...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    #1\n",
    "    {\n",
    "      \"name\" : \"title_has_phrase\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"title:\\\"${keywords})\\\"^=1\"\n",
    "      }\n",
    "    },\n",
    "    #2\n",
    "    {\n",
    "      \"name\" : \"title_has_terms\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"title:(${keywords})^=1\"\n",
    "      }\n",
    "    },\n",
    "    #3\n",
    "    {\n",
    "      \"name\" : \"title_bm25\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"title:(${keywords})\"\n",
    "      }\n",
    "    },\n",
    "    #4\n",
    "    {\n",
    "      \"name\" : \"overview_bm25\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"overview:(${keywords})\"\n",
    "      }\n",
    "    },\n",
    "    #5\n",
    "    {\n",
    "      \"name\" : \"overview_phrase_bm25\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"overview:\\\"${keywords}\\\"\"\n",
    "      }\n",
    "    },\n",
    "    #6\n",
    "    {\n",
    "      \"name\" : \"title_fuzzy\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"{!lucene df=title}${keywords}~\"\n",
    "      }\n",
    "    },\n",
    "    #7\n",
    "    {\n",
    "      \"name\" : \"release_year\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"{!func}def(release_year,2000)\"\n",
    "      }\n",
    "    },\n",
    "    #8 Collection Name BM25 Score\n",
    "    {\n",
    "      \"name\" : \"coll_name_bm25\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"collection_name_en:(${keywords})\"\n",
    "      }\n",
    "    },\n",
    "    #9 Collection Name Phrase BM25 Score\n",
    "    {\n",
    "      \"name\" : \"coll_name_phrase_bm25\",\n",
    "      \"store\": \"title2\",\n",
    "      \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\" : {\n",
    "        \"q\" : \"collection_name_en:\\\"${keywords}\\\"\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "client.create_featureset(index='tmdb', name='title2', ftr_config=config)\n",
    "\n",
    "from ltr.judgments import judgments_from_file\n",
    "from ltr.log import log_query\n",
    "from itertools import groupby\n",
    "\n",
    "training_set=[]\n",
    "with open('data/title_judgments.txt') as judgF:\n",
    "    judgments=judgments_from_file(judgF)\n",
    "    # For each query's judgments log features, and add to our training set...\n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        query_training_set, discarded = log_query(client, index='tmdb', \n",
    "                                                  judgments=query_judgments, feature_set='title2')\n",
    "        training_set.extend(query_training_set)\n",
    "        print(\"Got %s Training Samples\" % len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now a feature search\n",
    "\n",
    "And do a feature search over these new features (go get some coffee).\n",
    "\n",
    "We also up the number of trees & leafs to see if it has an impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import feature_search\n",
    "rankLibResult, ndcgPerFeature = feature_search(client,\n",
    "                                               training_set=training_set,\n",
    "                                               metric2t='NDCG@10',\n",
    "                                               leafs=20,\n",
    "                                               trees=20,\n",
    "                                               kcv=15,\n",
    "                                               features=[1,2,3,4,5,6,7,8,9],\n",
    "                                               featureSet='title2')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "trainLogs = rankLibResult.trainingLogs\n",
    "for ftrId, impact in trainLogs[-1].impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLogs[-1].rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Avg NDCG@10 when feature included:\")\n",
    "for ftrId, ndcg in ndcgPerFeature.items():\n",
    "    print(\"%s => %s\" % (ftrId, ndcg))\n",
    "    \n",
    "print(\"Avg K-Fold NDCG@10 %s\" % rankLibResult.kcvTestAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review new feature impacts\n",
    "\n",
    "Impact of each feature on the model... this is the best mix. Feature 8 helps, but not feature 9 as much. Interesting\n",
    "\n",
    "```\n",
    "4 - 18032.527656827504\n",
    "3 - 9801.409052757816\n",
    "5 - 8051.741259194476\n",
    "7 - 5711.964176322393\n",
    "8 - 3798.6132329430748\n",
    "1 - 1439.2180228991883\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now save away this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog  = train(client,\n",
    "                  training_set=training_set,\n",
    "                  metric2t='NDCG@10',\n",
    "                  leafs=20,\n",
    "                  trees=20,\n",
    "                  features=[1,3,4,5,7,8],\n",
    "                  featureSet='title2',\n",
    "                  index='tmdb',\n",
    "                  modelName='title2')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "for roundDcg in trainLog.rounds:\n",
    "    print(roundDcg)\n",
    "    \n",
    "print(\"Train NDCG@10 %s\" % trainLog.rounds[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import search\n",
    "search(client, \"rambo\", modelName='title2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.MART_model import eval_model\n",
    "from ltr.judgments import judgments_from_file\n",
    "from itertools import groupby\n",
    "\n",
    "features, _ = client.feature_set(index='tmdb', name='title2')\n",
    "\n",
    "for qid, query_judgments in groupby(training_set, key=lambda j: j.qid):\n",
    "    if qid == 1:\n",
    "        model = eval_model(modelName='title',\n",
    "                           features=features,\n",
    "                           judgments=query_judgments)\n",
    "\n",
    "        print()\n",
    "        print(\"## Evaluating graded docs for search keywords 'Rambo'\")\n",
    "        print()\n",
    "        print(model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whoopsies = model.whoopsies()\n",
    "for qid, whoopsie in whoopsies.items():\n",
    "    print(\"== QID %s ==\" % qid)\n",
    "    print(\"%s - %s\" % (whoopsie.count, whoopsie.totalMagnitude))\n",
    "    print(whoopsie.perTreeReport())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "== QID 1 ==\n",
    "10 - 40\n",
    "tree:0=>0(319074)-4(1368);tree:1=>0(319074)-4(1368);tree:2=>0(319074)-4(1368);tree:3=>0(319074)-4(1368);tree:4=>0(319074)-4(1368);tree:5=>0(319074)-4(1368);tree:6=>0(319074)-4(1368);tree:7=>0(319074)-4(1368);tree:8=>0(319074)-4(1368);tree:9=>0(319074)-4(1368)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.helpers.movies import get_movie\n",
    "[cast['character'] for cast in get_movie(1368)['cast']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "def add_collection_and_char_name(src_movie, base_doc):\n",
    "    if 'belongs_to_collection' in src_movie and src_movie['belongs_to_collection'] is not None:\n",
    "        if 'name' in src_movie['belongs_to_collection']:\n",
    "            base_doc['collection_name_en'] = src_movie['belongs_to_collection']['name']\n",
    "            \n",
    "    if 'cast' in src_movie and src_movie['cast'] is not None:\n",
    "        characters = [cast['character'] for cast in get_movie(1368)['cast']][:5]\n",
    "        base_doc['top_chars_en'] = characters\n",
    "    return base_doc\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json', enrich=add_collection_and_char_name)\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_doc(index='tmdb', doc_id=1892)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
