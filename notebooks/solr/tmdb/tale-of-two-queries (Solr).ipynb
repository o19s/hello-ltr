{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics & Prereqs (run once)\n",
    "\n",
    "If you don't already have the downloaded dependencies; if you don't have TheMovieDB data indexed run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client.solr_client import SolrClient\n",
    "client = SolrClient()\n",
    "\n",
    "from ltr import download\n",
    "from ltr.index import rebuild\n",
    "from ltr.helpers.movies import indexable_movies\n",
    "\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/tmdb.json'\n",
    "download([corpus], dest='data/');\n",
    "\n",
    "movies=indexable_movies(movies='data/tmdb.json')\n",
    "rebuild(client, index='tmdb', doc_src=movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Solr Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client.solr_client import SolrClient\n",
    "client = SolrClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Task: Optimizing \"Drama\" and \"Science Fiction\" queries\n",
    "\n",
    "In this example we have two user queries\n",
    "\n",
    "- Drama\n",
    "- Science Fiction\n",
    "\n",
    "And we want to train a model to return the best movies for these movies when a user types them into our search bar.\n",
    "\n",
    "We learn through analysis that searchers prefer newer science fiction, but older drama. Like a lot of search relevance problems, two queries need to be optimized in *different* directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Judgment List Generation\n",
    "\n",
    "To setup this example, we'll generate a judgment list that rewards new science fiction movies as more relevant; and old drama movies as relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.date_genre_judgments import synthesize\n",
    "judgments = synthesize(client, judgmentsOutFile='data/genre_by_date_judgments.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection should be *easy!*\n",
    "\n",
    "Notice we have 4 proposed features, that seem like they should work! This should be a piece of cake...\n",
    "\n",
    "1. Release Year of a movie `release_year` - feature ID 1\n",
    "2. Is the movie Science Fiction `is_scifi` - feature ID 2\n",
    "3. Is the movie Drama `is_drama` - feature ID 3\n",
    "4. Does the search term match the genre field `is_genre_match` - feature ID 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = [\n",
    "            {\n",
    "                \"store\": \"genre\", # Note: This overrides the _DEFAULT_ feature store location\n",
    "                \"name\" : \"release_year\",\n",
    "                \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "                \"params\" : {\n",
    "                  \"q\" : \"{!func}def(release_year,2000)\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"store\": \"genre\",\n",
    "                \"name\" : \"is_sci_fi\",\n",
    "                \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "                \"params\" : {\n",
    "                  \"q\" : \"genres:\\\"Science Fiction\\\"^=10.0\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"store\": \"genre\",\n",
    "                \"name\" : \"is_drama\",\n",
    "                \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "                \"params\" : {\n",
    "                  \"q\" : \"genres:\\\"Drama\\\"^=4.0\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"store\": \"genre\",\n",
    "                \"name\" : \"is_genre_match\",\n",
    "                \"class\" : \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "                \"params\" : {\n",
    "                  \"q\" : \"genres:\\\"${keywords}\\\"^=100.0\"\n",
    "                }\n",
    "            }\n",
    "]\n",
    "\n",
    "client.create_featureset(index='tmdb', name='genre', ftr_config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log from search engine -> to training set\n",
    "\n",
    "Each feature is a query to be scored against the judgment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.log import judgments_to_training_set\n",
    "trainingSet = judgments_to_training_set(client,\n",
    "                                        judgmentInFile='data/genre_by_date_judgments.txt', \n",
    "                                        trainingOutFile='data/genre_by_date_judgments_train.txt', \n",
    "                                        featureSet='genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Guaraneed Perfect Search Results!\n",
    "\n",
    "We'll train a LambdaMART model against this training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.train import train\n",
    "trainLog = train(client, \n",
    "                 trainingInFile='data/genre_by_date_judgments_train.txt',\n",
    "                 metric2t='NDCG@10',\n",
    "                 featureSet='genre',\n",
    "                 index='tmdb',\n",
    "                 modelName='genre')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "print(\"Perfect NDCG! {}\".format(trainLog.rounds[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But this search sucks!\n",
    "Try searches for \"Science Fiction\" and \"Drama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.search import search\n",
    "search(client, keywords=\"drama\", modelName=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why didn't it work!?!? Training data\n",
    "\n",
    "1. Examine the training data, do we cover every example of a BAD result\n",
    "2. Examine the feature impacts, do any of the features the model uses even USE the keywords?\n",
    "\n",
    "### Ranklib only sees the data you give it, we don't have good enough coverage\n",
    "\n",
    "You need to have feature coverage, especially over negative examples. Most documents in the index are negative! \n",
    "\n",
    "One trick commonly used is to treat other queries positive results as this queries negative results. Indeed what we're missing here are negative examples for \"Science Fiction\" that are not science fiction movies. A glaring omission, we'll handle now... With the `autoNegate` flag, we'll add additional negative examples to the judgment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr import date_genre_judgments\n",
    "from ltr.log import judgments_to_training_set\n",
    "\n",
    "date_genre_judgments.synthesize(client,\n",
    "                                judgmentsOutFile='data/genre_by_date_judgments.txt',\n",
    "                                autoNegate=True)\n",
    "\n",
    "judgments_to_training_set(client,\n",
    "                          judgmentInFile='data/genre_by_date_judgments.txt', \n",
    "                          trainingOutFile='data/genre_by_date_judgments_train.txt', \n",
    "                          featureSet='genre')\n",
    "\n",
    "from ltr.train import train\n",
    "trainLog = train(client,\n",
    "                 trainingInFile='data/genre_by_date_judgments_train.txt',\n",
    "                 metric2t='NDCG@10',\n",
    "                 featureSet='genre',\n",
    "                 index='tmdb',\n",
    "                 modelName='genre')\n",
    "\n",
    "print()\n",
    "print(\"Impact of each feature on the model\")\n",
    "for ftrId, impact in trainLog.impacts.items():\n",
    "    print(\"{} - {}\".format(ftrId, impact))\n",
    "    \n",
    "print(\"Perfect NDCG! {}\".format(trainLog.rounds[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try those queries...\n",
    "\n",
    "Replace keywords below with 'science fiction' or 'drama' and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.search import search\n",
    "search(client, keywords=\"science fiction\", modelName=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.search import search\n",
    "search(client, keywords=\"drama\", modelName=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next problem\n",
    "\n",
    "- Overfit to these two examples\n",
    "- We need many more queries, covering more use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
